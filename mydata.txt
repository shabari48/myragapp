EBOOK

A Compact Guide to
Large Language Models
A Compact Guide to Large Language Models                                                                                                                                2

S ECT I O N 1

Introduction
                                                                                    Translate English to Spanish:          Que hora es?
Definition of large language models (LLMs)                                          What's the time?
                                                                                                                           In December; Puerto Rico
Large language models are AI systems that are designed to process and analyze                                              experiences pleasant
vast amounts of natural language data and then use that information to generate     What is the expected                   tropical weather with an
responses to user prompts. These systems are trained on massive data sets           weather in Puerto Rico in              84 (29C) and an average
                                                                                                                                                  average high temperature of
                                                                                    December?
using advanced machine learning algorithms to learn the patterns and structures                                            low temperature of around
                                                                                                                           72"F (22C).
of human language, and are capable of generating natural language responses to
a wide range of written inputs. Large language models are becoming increasingly     Summarize in 15 words
important in a variety of applications such as natural language processing,         'During extra time; Messi
machine translation, code and text generation, and more.                            then scored again to give
                                                                                    Argentina a 3-2 lead:             LLM
                                                                                    However; Mbappe scored
While this guide will focus on language models, it’s important to understand that   another penalty to tie the             Mbappe scores       hat-trick
they are only one aspect under a larger generative AI umbrella. Other noteworthy    game 3-3 with only minutes             but Argentina wins World
                                                                                   remaining; becoming the                 Cup in a penalty shoot-out:
generative AI implementations include projects such as art generation from text,    second man to score       hat -
audio and video generation, and certainly more to come in the near future.          trick in  World Cup final.
                                                                                   Argentina then won the
                                                                                    ensuing penalty shoot-out
                                                                                    4-2 to win their third World
                                                                                   Cup:
                                                                                    Generate     tagline for a fruit       Purely natural, sustainably
                                                                                   juice brand that focused on             sourced: Taste the goodness
                                                                                   farming:sustainable and organic            organicfruits with every
                                                                                                                           sip.
A Compact Guide to Large Language Models                                                                                                            3

Extremely brief historical background and development of LLMs

        1950s–1990s                                                           2018
        Initial attempts are made to map hard rules around languages and      Google introduces BERT (Bidirectional Encoder Representations
        follow logical steps to accomplish tasks like translating a sentence  from Transformers), which is a big leap in architecture and paves
        from one language to another.                                         the way for future large language models.

        While this works sometimes, strictly defined rules only work for
        concrete, well-defined tasks that the system has knowledge about.     2020
                                                                              OpenAI releases GPT-3, which becomes the largest model at
                                                                              175B parameters and sets a new performance benchmark for
        1990s                                                                 language-related tasks.
        Language models begin evolving into statistical models and
        language patterns start being analyzed, but larger-scale projects
        are limited by computing power.                                       2022
                                                                              ChatGPT is launched, which turns GPT-3 and similar models into
                                                                              a service that is widely accessible to users through a web interface
        2000s                                                                 and kicks off a huge increase in public awareness of LLMs and
        Advancements in machine learning increase the complexity of           generative AI.
        language models, and the wide adoption of the internet sees an
        enormous increase in available training data.
                                                                              2023
                                                                              Open source LLMs begin showing increasingly impressive results
        2012                                                                  with releases such as Dolly 2.0, LLaMA, Alpaca and Vicuna.
        Advancements in deep learning architectures and larger data sets      GPT-4 is also released, setting a new benchmark for both parameter
        lead to the development of GPT (Generative Pre-trained Transformer).  size and performance.
        A Compact Guide to Large Language Models                                    4

        S ECT I O N 2

        Understanding Large Language Models

What are language models and how do they work?

Large language models are advanced artificial intelligence systems that take
some input and generate humanlike text as a response. They work by first
analyzing vast amounts of data and creating an internal structure that models
the natural language data sets that they’re trained on. Once this internal
structure has been developed, the models can then take input in the form of
natural language and approximate a good response.


If they’ve been around for so many years, why are they just now
making headlines?

A few recent advancements have really brought the spotlight to generative AI
and large language models:

     A D V A N C E M E N T S  I N  T E C H N I Q U E S
     Over the past few years, there have been significant advancements in the
     techniques used to train these models, resulting in big leaps in performance.
     Notably, one of the largest jumps in performance has come from integrating
     human feedback directly into the training process.
I N C R E A S E D A C C E S S I B I L I T Y
The release of ChatGPT opened the door for anyone with internet access
to interact with one of the most advanced LLMs through a simple web
interface. This brought the impressive advancements of LLMs into the
spotlight, since previously these more powerful LLMs were only available
to researchers with large amounts of resources and those with very deep
technical knowledge.

G R O W I N G  C O M P U T A T I O N A L P O W E R
The availability of more powerful computing resources, such as graphics
processing units (GPUs), and better data processing techniques allowed
researchers to train much larger models, improving the performance of
these language models.

I M P R O V E D T R A I N I N G D A T A
As we get better at collecting and analyzing large amounts of data, the
model performance has improved dramatically. In fact, Databricks showed
that you can get amazing results training a relatively small model with a
high-quality data set with Dolly 2.0 (and we released the data set as well
with the databricks-dolly-15k data set).
        A Compact Guide to Large Language Models                                                                                                                         5

So what are organizations using large language models for?

Here are just a few examples of common use cases for large language models:

       C H A T B O T S  A N D  V I R T U A L  A S S I S T A N T S
       One of the most common implementations, LLMs can be used by
       organizations to provide help with things like customer support,
       troubleshooting, or even having open-ended conversations with user-
       provided prompts.

       C O D E  G E N E R A T I O N  A N D   D E B U G G I N G
       LLMs can be trained on large amounts of code examples and give
       useful code snippets as a response to a request written in natural language.
       With the proper techniques, LLMs can also be built in a way to reference
       other relevant data that it may not have been trained with, such as a
       company’s documentation, to help provide more accurate responses.
L A N G U A G E  T R A N S L A T I O N
Globalize all your content without hours of painstaking work by simply
feeding your web pages through the proper LLMs and translating them to
different languages. As more LLMs are trained in other languages, quality
and availability will continue to improve.

S U M M A R I Z A T I O N  A N D  P A R A P H R A S I N G
Entire customer calls or meetings could be efficiently summarized so that
others can more easily digest the content. LLMs can take large amounts of
text and boil it down to just the most important bytes.

C O N T E N T  G E N E R A T I O N
Start with a detailed prompt and have an LLM develop an outline for you.
Then continue on with those prompts and LLMs can generate a good first
draft for you to build off. Use them to brainstorm ideas, and ask the LLM
questions to help you draw inspiration from.

               S E N T I M E N T A N A LY S I S                                             Note: Most LLMs are not trained to be fact machines. They know how to use
               Often a hard task to quantify, LLMs can help take a piece of text and gauge  language, but they might not know who won the big sporting event last year.
               emotion and opinions. This can help organizations gather the data and        It’s always important to fact check and understand the responses before
               feedback needed to improve customer satisfaction.                            using them as a reference.

               T E X T C L A S S I F I C A T I O N A N D  C L U S T E R I N G
               The ability to categorize and sort large volumes of data enables the
               identification of common themes and trends, supporting informed
               decision-making and more targeted strategies.
        A Compact Guide to Large Language Models                                    6

        S ECT I O N 3

        Applying Large Language Models

There are a few paths that one can take when looking to apply large language
models for their given use case. Generally speaking, you can break them down
into two categories, but there’s some crossover between each. We’ll briefly cover
the pros and cons of each and what scenarios fit best for each.

and require you to send your data to their servers in order to interact with their
language models. This raises privacy and security concerns, and also subjects
users to “black box” models, whose training and guardrails they have no control
over. Also, due to the compute required, these services are not free beyond a
very limited use, so cost becomes a factor in applying these at scale.

Proprietary services

As the first widely available LLM powered service, OpenAI’s ChatGPT was the
explosive charge that brought LLMs into the mainstream. ChatGPT provides
a nice user interface (or API) where users can feed prompts to one of many
models (GPT-3.5, GPT-4, and more) and typically get a fast response. These are
among the highest-performing models, trained on enormous data sets, and are
capable of extremely complex tasks both from a technical standpoint, such as
code generation, as well as from a creative perspective like writing poetry in a
specific style.

The downside of these services is the absolutely enormous amount of compute
required not only to train them (OpenAI has said GPT-4 cost them over $100
million to develop) but also to serve the responses. For this reason, these
extremely large models will likely always be under the control of organizations,
In summary: Proprietary services are great to use if you have very complex tasks,
are okay with sharing your data with a third party, and are prepared to incur
costs if operating at any significant scale.


Open source models

The other avenue for language models is to go to the open source community,
where there has been similarly explosive growth over the past few years.
Communities like Hugging Face gather hundreds of thousands of models
from contributors that can help solve tons of specific use cases such as text
generation, summarization and classification. The open source community has
been quickly catching up to the performance of the proprietary models, but
ultimately still hasn’t matched the performance of something like GPT-4.
        A Compact Guide to Large Language Models                                           7

It does currently take a little bit more work to grab an open source model and
start using it, but progress is moving very quickly to make them more accessible
to users. On Databricks, for example, we’ve made improvements to open source
frameworks like MLflow to make it very easy for someone with a bit of Python
experience to pull any Hugging Face transformer model and use it as a Python
object. Oftentimes, you can find an open source model that solves your specific
problem that is orders of magnitude smaller than ChatGPT, allowing you to bring
the model into your environment and host it yourself. This means that you can
keep the data in your control for privacy and governance concerns as well as
manage your costs.
Conclusion and general guidelines

Ultimately, every organization is going to have unique challenges to overcome,
and there isn’t a one-size-fits-all approach when it comes to LLMs. As the world
becomes more data driven, everything, including LLMs, will be reliant on having
a strong foundation of data. LLMs are incredible tools, but they have to be used
and implemented on top of this strong data foundation. Databricks brings both
that strong data foundation as well as the integrated tools to let you use and
fine-tune LLMs in your domain.


        Another huge upside to using open source models is the ability to fine-tune
        them to your own data. Since you’re not dealing with a black box of a proprietary
        service, there are techniques that let you take open source models and train
        them to your specific data, greatly improving their performance on your
        specific domain. We believe the future of language models is going to move
        in this direction, as more and more organizations will want full control and
        understanding of their LLMs.
       A Compact Guide to Large Language Models                                                                                     8

       S ECT I O N 4

       So What Do I Do Next If I Want to Start Using LLMs?

That depends where you are on your journey! Fortunately, we have a few paths
for you.
If you want to go a little deeper into LLMs but aren’t quite ready to do it yourself,
you can watch one of Databricks’ most talented developers and speakers go
over these concepts in more detail during the on-demand talk “How to Build
Your Own Large Language Model Like Dolly.”

If you’re ready to dive a little deeper and expand your education and
understanding of LLM foundations, we’d recommend checking out our
course on LLMs. You’ll learn how to develop production-ready LLM applications
and dive into the theory behind foundation models.

Getting started with NLP using Hugging Face
transformers pipelines



Fine-Tuning Large Language Models with
Hugging Face and DeepSpeed




       If your hands are already shaking with excitement and you already have some     Introducing AI Functions: Integrating Large
       working knowledge of Python and Databricks, we’ll provide some great examples   Language Models with Databricks SQL
       with sample code that can get you up and running with LLMs right away!
About Databricks
Databricks is the data and AI company. More than 9,000
organizations worldwide — including Comcast, Condé Nast and
over 50% of the Fortune 500 — rely on the Databricks Lakehouse
Platform to unify their data, analytics and AI. Databricks is
headquartered in San Francisco, with offices around the globe.
Founded by the original creators of Apache Spark™, Delta Lake
and MLflow, Databricks is on a mission to help data teams solve
the world’s toughest problems. To learn more, follow Databricks on
Twitter, LinkedIn and Facebook.

      S TA R T YO U R F R E E T R I A L

Contact us for a personalized demo:
databricks.com/contact

© Databricks 2023. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation. Privacy Policy | Terms of Use
                                        Introduction
                                         Malay Agarwal

Contents
Generative AI                                                                                1

LLMs                                                                                         1
    Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .            1
    Examples (Foundation/Base Models) . . . . . . . . . . . . . . . . . . .                   2
    Use Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .             2
    Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .             2
           Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .             2
           Context Window . . . . . . . . . . . . . . . . . . . . . . . . . . .               2
           Completion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .             3

Generative AI Project Lifecycle                                                              3
    Overall Lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .             3
    Steps in the Lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . .            3
           Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .            3
           Select . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           4
           Adapt and Align Model . . . . . . . . . . . . . . . . . . . . . . .                4
           Application Integration            . . . . . . . . . . . . . . . . . . . . . . .   4
    Cheat Sheet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .             5

Generative AI
It is a subset of traditional ML. The ML algorithms that work behind generative
AI do so by exploiting the statistical patterns present in the massive datasets of
content that was originally generated by humans.

LLMs
Definition
LLMs (Large Language Models) are generative AI models specifically designed
to understand text. All LLMs are powered by the Transformer (Google, 2017)
architecture. They are designed to take in input text and repeatedly generate the

                                                    1
next token or word that appropriately “completes” the input text. For example,
an LLM can be given the input:
        Where is Ganymede located in the solar system?
In response, the LLM might generate the following output:
        Ganymede is a moon of Jupiter and is located in the solar system
        within Jupiter’s orbit.
Here, the model essentially completed the given input by repeatedly generating
the next word or token that fits appropriately.
These models have abilities beyond just language and are capable of breaking
down complex tasks, reasoning and problem solving.
It is commonly accepted that as the size (in terms of number of parameters) of
an LLM increases, so does its understanding of language. At the same time, it is
also true the smaller models can be fine-tuned to perform well on specific tasks.

Examples (Foundation/Base Models)
    • BERT (Google, 2018)
    • GPT (OpenAI, 2018)
    • BLOOM (BigScience Warehouse, 2022)
    • FLAN-T5 (Google, 2023)
    • PaLM (Google, 2022)
    • LLaMA (Facebook, 2023)

Use Cases
    • Chatbots.
    • Text summary (articles, files).
    • Translation (traditional human language to human language, or from
        natural language to code).
    • Named Entity Recognition.
    • More complex tasks which require invoking external tools (answering
        fact-based queries using Google’s Search API).

Terminology
Prompt
The input given to an LLM is called the prompt.

Context Window
The space/memory that is available to the prompt is called the context window.
It is essentially the maximum size of the prompt that the model can handle

                                                 2
before it performs poorly. It is limited to a few thousand of words but also varies
model to model.

Completion
The output of the an LLM when given a prompt is called the completion.
Generally, the completion consists of the prompt and the text generated by
the model by repeatedly generating the next token or word, though almost all
applications omit the prompt from the model’s output when showing it to users.

Generative AI Project Lifecycle
Overall Lifecycle
    1. Scope - Define the use case of your application.
    2. Select - Choose an existing model or pretrain your own.
    3. Adapt and align model - Prompt engineering, fine-tuning, align with
        human feedback and evaluate.
    4. Application integration - Optimize and deploy model for inference, and
        augment model and build LLM-powered applications.
      Scope            Select          Adapt and align model           Application integration
                                   Prompt
                                   engineering
                   Choose an                                         Optimize        Augment
   Define the      existing         Fine-tuning                      and deploy      model and
   use case        model or                         Evaluate         model for       build LLM-
                   pretrain                                          inference       powered
                   your own        Align with                                        applications
                                    human
                                   feedback
Steps in the Lifecycle
Scope
We need to define the scope of the project as accurately and as narrowly as
we can. LLMs are capable of carrying out various tasks but their ability is
dependent by their size and architecture. We need to think about the function(s)
that the LLM will have in our application.
For example, do we need the model to be able to carry out many different tasks
such as long-form text generation for different prompts; or do we need the model
to be good at a single task such as NER.

                                                  3
Being specific about what the application needs to do can save us time and more
importantly, help us save on compute cost.

Select
We need to decide whether we want to train our own model from scratch or work
with an existing base model.
In general, we start with a base model except in a few cases where it might be
necessary to train our own model.

Adapt and Align Model
We need to evaluate the performance of our model and carry out additional
training as needed.
Sometimes, prompt engineering could be enough to get our model to perform
well. Thus, in general, we’d start with trying , providing examples suitable for
our task(s) in the prompt.
There are cases where the model may not perform as well as we need even with
ICL. In those cases, we can try fine-tuning the model on some dataset specific
to our task, using a supervised learning.
Additionally, to ensure the model responds naturally and safely (no racism, no
harmful advice, etc), we might also need to align the model with human feedback.
This is achieved through Reinforcement Learning With Human Feedback (RLHF).
All these techniques work on basis of good evaluation. If we are unable to
evaluate the performance gains/losses due to these techniques, they are not
useful.
For example, we can start with prompt engineering and then evaluating the
model’s performance. We can then use fine-tuning to improve the model’s
performance, and then revisit and evaluate prompt engineering one more time
to get the performance we need.

Application Integration
Once we have a performant model which satisfies our requirements, we integrate
it with the application.
An important step is to optimize the model for deployment. This will ensure
that we are making the best use of our compute resources and provide the best
experience for our users.
Finally, we need to consider any additional infrastructure that might be required
by our LLM. For example, LLMs have a tendency to invent information when
they don’t know an answer, or are incapable of carrying out complex reasoning
and doing mathematics. There are some powerful techniques available that can
help us overcome or at least reduce the impact of these limitations.

                                                  4
Cheat Sheet
The below diagram details the lifecycle in terms of time and effort required.
                        Pre-training                 Prompt                  Prompt tuning and        Reinforcement                 Compression/
                                                     engineering                  ~tuning             learning/human feedback      @otimization
                                                                                                                                   deplovment
   Training             Davs t0 weekst0              Notrequired             Minutes     hours        Minutes to hours similar t0  Minutes to hours
   duration             months                                                                        fine-tuning
   Customization        Determine model              No model weights        Tune for specific tasks  Nced scparate reward model   Redicemodel size
                        arcnitecture size and                                                         t0 Jlienith humangoals       through model
                         tokeniter                   Only prompt             Add domain-specific      (helpful honest harmless)    pruning Vicight
                                                     customizaton            data                                                  quantizaton
                        Choose vocabulary size                                                        Update LLM model =           distillation
                                 okensi0r                                    Update LLM model or      dopterwreichts
                        inputcontext                                         adapter weights                                       Smaller size
                                                                                                                                   Inicrcnce
                               Amountal
                        domaintraining data
   Objective            Next-token prediction        Increase task           Increase task            Increase alignment with      Increase inference
                                                     perionnanct              penonnance              humn pretererces             pertormlonce
   Expertise            High                                                 Medium                   Medium High                  Medium
                                                                                  5
A Survey on Large Language Models: Applications,
            Challenges, Limitations, and Practical Usage
  Muhammad Usman Hadi5, Muhammad Bilal Shaikh6, Naveed Akhtar7, Jia Wu23, Amgad Muneer2, MuhammadIrfan4, Anas Zafar1,*, Qasem Al-Tashi2,*, Rizwan Qureshi2,*, Abbas Shah, and Seyedali Mirjalili8,9
         1School of Engineering, Ulster University, Belfast, BT15 1AP, United Kingdom (m.hadi@ulster.ac.uk)
2Department of Imaging Physics, The University of Texas MD Anderson Cancer Center, Houston, TX 77030, USA
 (qaal@mdanderson.org; frizwan@mdanderson.org; amabdulraheem@mdanderson.org; JWu11@mdanderson.org)
    3Department of Electronics Engineering, Mehran University of Engineering and Technology, Jamshoro,76062
                                                      Pakistan (abbasshah.syed@gmail.com)
 4Faculty of Electrical Engineering, Ghulam Ishaq Khan Institute (GIKI) of Engineering Sciences and Technology,
                                                 Swabi, 23460 Pakistan (mirfan@giki.edu.pk)
  5Department of Computer Science, National University of Computer and Emerging Sciences, Karachi, Pakistan
                                                              (anaszafar98@gmail.com)
     6Center for Artificial Intelligence and Machine Learning (CAIML), Edith Cowan University, 270 Joondalup
                             Drive, Joondalup, WA 6027, Perth, Australia (mbshaikh@our.ecu.edu.au)
                         7University of Western Australia, 35 Stirling Hwy, Crawley WA 6009, Australia
     8Centre for Artificial Intelligence Research and Optimization, Torrens University Australia, Fortitude Valley,
                                      Brisbane, QLD 4006, Australia (ali.mirjalili@torrens.edu.au)
                  9University Research and Innovation Center, Obuda University, 1034 Budapest, Hungary
                                                                            Abstract
           Within the vast expanse of computerized language processing, a revolutionary entity known as Large Language Models
       (LLMs) has emerged, wielding immense power in its capacity to comprehend intricate linguistic patterns and conjure coherent
       and contextually fitting responses. Large language models (LLMs) are a type of artificial intelligence (AI) that have emerged as
       powerful tools for a wide range of tasks, including natural language processing (NLP), machine translation, and question-answering.
       This survey paper provides a comprehensive overview of LLMs, including their history, architecture, training methods, applications,
       and challenges. The paper begins by discussing the fundamental concepts of generative AI and the architecture of generative pre-
       trained transformers (GPT). It then provides an overview of the history of LLMs, their evolution over time, and the different training
       methods that have been used to train them. The paper then discusses the wide range of applications of LLMs, including medical,
       education, finance, and engineering. It also discusses how LLMs are shaping the future of AI and how they can be used to solve
       real-world problems. The paper then discusses the challenges associated with deploying LLMs in real-world scenarios, including
       ethical considerations, model biases, interpretability, and computational resource requirements. It also highlights techniques for
       enhancing the robustness and controllability of LLMs and addressing bias, fairness, and generation quality issues. Finally, the
       paper concludes by highlighting the future of LLM research and the challenges that need to be addressed in order to make LLMs
       more reliable and useful. This survey paper is intended to provide researchers, practitioners, and enthusiasts with a comprehensive
       understanding of LLMs, their evolution, applications, and challenges. By consolidating the state-of-the-art knowledge in the field,
       this survey serves as a valuable resource for further advancements in the development and utilization of LLMs for a wide range
       of real-world applications. The GitHub repo for this project is available at https://github.com/anas-zafar/LLM-Survey

                                                                         Index Terms
           Large Language Models, Generative AI, Conversational AI, Co-pilots, Natural language processing, GPT, ChatGPT, Bing,
       Bard, AI-enabled Tools, AI chatbots.
                                                                         1

              A Survey on Large Language Models: Applications,
                          Challenges, Limitations, and Practical Usage

                           I. INTRODUCTION

    Human beings possess the remarkable ability to express and
communicate through language, which starts developing dur-
ing early childhood and continues to evolve throughout their

lifetime [1], [2]. However, machines lack the inherent capacity
to comprehend and communicate in human language unless
they are equipped with powerful AI algorithms [3]. The goal

of achieving human-like reading, writing, and communication
skills in machines has been a long-standing research challenge
and desire [4] [5].

    The advent of large language models (LLMs) can be
attributed to advancements in deep learning (DL) methods,

the availability of massive computational resources, and the
availability of vast amounts of training data. These models,
often pre-trained on extensive corpora from the web, possess

the ability to learn complex patterns, linguistic nuances, and
semantic relationships. Fine-tuning these models on specific
downstream tasks has shown promising results, achieving

state-of-the-art performance in various benchmarks [6].
    Language modeling (LM) is a crucial approach to enhancing
the language intelligence of machines. In general, LM involves

modeling the probability of word sequences to predict the
likelihood of the future. As can be seen from Fig 1, LM
research has received widespread attention and has undergone

four significant development stages, as follows: The first major
development in LM was the Statistical Language Models
(SLMs) [7], such as n-gram models [8]. These models estimate
the likelihood of the next word in a sequence based on the
frequency of occurrence of previous n-grams of words [9],
[10]. For instance, a bigram model would use the frequency
of occurrence of pairs of words to estimate the probability
of the next word. The second stage of LM development
involved the introduction of neural network-based LM, named
Neural Language Models (NLMs) [11], which is also known
as neural language modeling. This approach utilizes neural
networks to predict the probability distribution of the next
word in a sequence given the previous words in the sequence.
Recurrent neural networks (RNNs) and their variants such
as Long Short-Term Memory (LSTM) and Gated Recurrent
Units (GRUs) are often used in this approach [12], [13].
The third stage of LM development involves the creation of
contextualized word embeddings, which aim to capture the
meaning and context of words in a sentence or text, called
Pre-trained Language Models (PLMs). These models utilize
neural networks to learn a vector representation of words that
takes into account the context in which the word appears [14].
Examples of contextualized word embeddings include ELMo
[15] and BERT [16]. In Table II, we present sources of data
for pre-training LLaMA.
  TABLE I: List of Acronyms and corresponding definitions.
  Acronym       Definition
  AI            Artificial Intelligence
  AGI           Artificial General Intelligence
  BERT          Bidirectional Encoder Representations from Transformers
  CV            Computer Vision
  CTRL          Conditional Transformer Language Model
  FFF           Fused Filament Fabrication
  GANs          Generative Adversarial Networks
  GNMT          Google Neural Machine Translation
  GPT           Generative Pre-Trained transformers
  GPT-3         Generative Pre-trained Transformer 3
  GPT-4         Generative Pre-trained Transformer 4
  GPUs          Graphical Processing Units
  GRUs          Gated Recurrent Units
  LLaMA         Large Language Model Meta AI
  LLM           Large Language Models
  LM            Language Model
  LSTM          Long Short-Term Memory
  ML            Machine Learning
  MLM           Masked Language Modeling
  NSP           next-sentence prediction
  NLP           Natural Language Processing
  NLTK          Natural Language Toolkit
  PLMs          Pre-trained Language Models
  RNN           Recurrent neural networks
  RNNLM         Recurrent neural network language model
  SLMs          Statistical Language Models
  T5            Text-to-Text Transfer Transformer
  TPUs          Tensor Processing Units
  USMLE         United States Medical Licensing Exam
  VL-PTMs       Vision-Language Pre-trained Models
  XLNet         eXtreme Language Understanding Network



   The fourth stage of LM development involved the creation
of large-scale pretraining language models called Large Lan-
guage Models (LLMs) [17], which have the capability of
performing various natural language processing (NLP) tasks
with excellent performance [18]. These models, such as GPT-
3 and GPT-4, are trained on massive amounts of text data
and can be fine-tuned for specific downstream tasks, such as
language translation or question answering [19].
   In summary, these four stages of LM development represent
major advancements in the field, with each stage building upon
the previous one and pushing the boundaries of what machines
can achieve in NLP and Computer Vision. In this research, we
mainly focus on LLMs. We have hierarchically divided the
LLM types into four major categories (see Fig. 1).
   Modern language model called ChatGPT was created by
OpenAI. It is based on the GPT-3.5 architecture and was
trained using a sizable amount of internet-sourced text data,
including books, articles, wikis and websites II. ChatGPT is
exceptional at producing human-like responses and having
conversations with users. In computer vision (CV), researchers
are actively engaged in the development of vision-language
models inspired by the capabilities of ChatGPT. These models
                                                                                                                                                                             2
                                                                                       Linguistically motivated
                                                                             Adaptive
                                                                                                              Exponential
                                                                   Decision Tree           Statistical
                                                                                           LanguageModels
                                                                                                                Continuous Space
                                                                     N-Gram
                                  Galactica       PaLM                                                                            Speech Recognition
                           AlexaTM                                 Minerva
                                                                                                               Parsing Tools                        Machine Translation
                     LLaMA                                              GLaM
                  PanGu-2
                                         LanguageLarge                               Language
                                                                                     Modeling                                      LanguageNeural        Sentiment Analysis
                Cerebras-GPT              Models                                                                                     Models
                                                                 GPT3
                  BloombergGPT
                                 Falcon                  ChatGPt                                                                   Text Suggestions
                                                                             Switch
                                           GPT-4 API                     Transformers                        biLSTM
                                                                                            Pre-trained
                                                                               T5           Language             BERT
                                                                                              Models
                                                                                           DeBERTa
                                                                        Fig. 1: Types of language modeling.

are specifically designed to enhance multimodal dialogues,
where both visual and textual information are important [20].
Moreover, the advancements in the field have led to the
introduction of GPT-4 [21], which has further expanded the
capabilities of language models by seamlessly integrating
visual information as part of the input. This integration of
visual data empowers the model to effectively understand and
generate responses that incorporate both textual and visual
cues, enabling more contextually rich and nuanced conver-
sations in multimodal settings.

A. Survey Motivation

   The revolutionary ChatGPT has captivated the attention
of the community, sparking a wealth of fascinating reviews
and discussions on the advancements of LLMs and artificial
intelligence [3], [22], [23], [24]. For example, the role of
ChatGPT in education is evaluated in [25], healthcare in [26],
finance in [27], on code writing capabilities in [28], impact
on labour market in [29], legal aspects in [30], and an
opinion paper in [5]. A comprehensive survey on LLMs
presents advancements in pre-training, fine-tuning, utilization
and capability evaluation of LLMs [3]. The recent progress
in visio-language pre-trained models is discussed in [22].
The paper presents an overview of various techniques for
encoding raw images and texts into single-modal embeddings
as a fundamental aspect, and prevalent architectures of
Vision-Language Pre-trained Models (VL-PTMs), focusing
on their ability to effectively model the interaction between
text and image representations.
Despite the growing number of studies on LLMs, there
remains a scarcity of research focusing on their technical
intricacies and effective utilization. In this review and tutorial
article, our primary objective is to explore, learn, and evaluate
language models across various domains. We delve into the
working principles of language models, analyze different
architectures of the GPT family, and discuss strategies for their
optimal utilization. Furthermore, we provide detailed insights
into writing prompts, and visual prompting techniques,
leveraging GPT-plug-ins, and harnessing other AI/LLM
tools. Our comprehensive examination also encompasses a
discussion on the limitations associated with LLMs, including
considerations related to security, ethics, economy, and the
environment. In addition, we present a set of guidelines to
steer future research and development in the effective use of
LLMs. We hope that this research will contribute to a better
understanding and utilization of LLMs. A list of commonly
used acronyms in this article with definitions is given in
Table I).
                                                                                                                                                                                           3
                                                                                      Schematic of Sections
                     section |               Section               Section                Section
                 Generative Al/ Alin                              Application;        Al-Enabled Tools; The  Section           section VII      Section VIII         Section IX
                     Crcativity           Oveniew   LLMS                                      Everyining    GPT Plugins        Guideline s      Limitations       Expert Point of View
                        benerative Pre         History                   Mediaal         Chalbots _ chnlGpt   ChatGPT prompts   Step{  exmples    Environmental                  Al-Human
                      trained Trnstormcrs                                                                       guidellnes                                                      coextstence
                      Data, Creativity and                                                Al tools for = image                                   Sustainability and             Should Alhr
                          Vanancr             Training of LLM            Education       Eeneration; history;                      Prompting
                                                                                          me dicalindustny
                                              How LLMs work               clnance          Atools forntert                                        Hallucinations              Open Questions
                                                                                            classification                                             LLMs
                                                                     Engineering related Al tools for Literature                                  Drawbacks of LLMs           Recommendations
                                                                        applications       revieu Research
                                                                                             CodeGPT                                                 Singularity
                                                                                                                                                     CompetitiveMarect
                                                                                                                                                     Prolitabillty
                                                              Fig. 2: Schematic of the overview of the survey at a glance.

B. Contributions

   The main contributions of this article include:
   1) Providing a comprehensive overview of LLMs, includ-

         ing their technical details, advancements, challenges,
         capabilities and limitations.
   2) Presenting a state-of-the-art analysis and comparison of

         different LLMs.
   3) Addressing ethical concerns about LLMs, including their
         computational requirements and potential for perpetuat-
         ing biases.
   4) Offering insights into the future potential of LLMs and
         their impact on society and demonstrating the applica-
         tions of LLM through four practical use cases in the
         fields of medicine, education, finance, and engineering.
   5) The paper is written in a unique way to promote
         practical usage of LLMs. Most of the content, including,
         images, figures and tables is generated using LLMs.
   The survey paper is organized into the following sections.
Section II provides an introduction to the role of AI in creativ-
ity, specifically focusing on generative pre-trained transformers
and their significance. Section III presents an overview of
LLMs, summarizing a brief history of LLMs and discussing
their training and functionality. Section IV demonstrates the
applications of LLM through four use cases in the fields
of medicine, education, finance, and engineering. Section V
explores AI-enabled tools that are expected to shape the future.
Section VI discusses the practical use case of GPT plugins
and their potential to enhance user productivity and efficiency.
Section VII presents guidelines and working examples using
prompting techniques. Section VIII proposes the limitations
and drawbacks of the current state-of-the-art LLM. Section
IX-D presents the impact of LLM on humans and society.
TABLE II: Pre-training data. Mixtures of data used for pre-
training LLaMA [31].

               Dataset          Sampling prop.       Epochs      Disk size
           CommonCrawl               67.0%             1.10        3.3TB
                  C4                 15.0%             1.06        783GB
                Github                4.5%             0.64        328GB
              Wikipedia               4.5%             2.45         83GB
                Books                 4.5%             2.23         85GB
                ArXiv                 2.5%             1.06         92GB
           StackExchange              2.0%             1.03         78GB


Section XI presents expert opinions on the subject matter
and the author’s perspective on open unanswered avenues.
Section XI concludes the survey paper. The overall structure
and sections for better visibility are shown in the form of a
schematic in Fig. 2.

              II. GENERATIVE AI / AI IN CREATIVITY
    Generative AI refers to AI systems primarily designed to
generate content (text, images, audio and videos). It sets apart
from AI systems with different functions, like classifying data
(e.g., labeling images), grouping data (e.g., identifying cus-
tomer segments with similar purchasing behavior), or making
decisions (e.g., guiding an autonomous vehicle). Some com-
mon examples of generative AI systems are image generators
(Midjourney or stable diffusion), Chatbots (ChatGPT, Bard,
Palm), code generators (CodeX, Co-Pilot [40]) and audio
generators (VALL-E).
    Generative AI works by leveraging complex algorithms and
statistical models to generate new content that mimics the
patterns and characteristics of the training data. Generative
AI systems can employ different techniques like Variational
Autoencoders (VAEs) [41], Generative Adversarial Networks
                                                                                                                                                                      4

TABLE III: Comparison of LLMs’ Reasoning Performance. Notations: MMLU [32]: high school and college knowledge,
GSM8K: elementary school math, MATH: very hard math and natural science. All current models struggle, BBH [33]: a
collection of 27 hard reasoning problems, HumanEval [34]: a classical dataset for evaluating coding capability, C-Eval [35]:
a collection of 52 disciplines of knowledge test in Chinese, TheoremQA [36]: a question-answering dataset driven by STEM
theorems. [37], [32], [31], [38], [39], [35]
                   Model                 Param.      Type     GSM8K        MATH       MMLU        BBH       HumanEval       C-Eval     TheoremQA
                   GPT-4                    -       RLHF         92.0        42.5       86.4         -          67.0         68.7*          43.4
                   claude-v1.3              -       RLHF        81.8*         -        74.8*      67.3*           -          54.2*          24.9
                   PaLM-2                   -        Base        80.7        34.3       78.3       78.1           -            -            31.8
                   GPT-3.5-turbo            -       RLHF        74.9*         -        67.3*      70.1*         48.1         54.4*          30.2
                   claude-instant           -       RLHF        70.8*         -           -       66.9*           -          45.9*          23.6
                   text-davinci-003         -       RLHF          -           -         64.6       70.7           -            -            22.8
                   code-davinci-002         -        Base        66.6        19.1       64.5       73.7         47.0           -             -
                   text-davinci-002         -        SIFT        55.4         -         60.0       67.2           -            -            16.6
                   Minerva                540B       SIFT        58.8        33.6         -          -            --           --            --
                   Flan-PaLM              540B       SIFT         -           -         70.9       66.3
                   Flan-U-PaLM            540B       SIFT         -           -         69.8       64.9           -            -             -
                   PaLM                   540B       Base        56.9        8.8        62.9       62.0         26.2           -             -
                   LLaMA                  65B        Base        50.9        10.6       63.4         -          23.7         38.8*           -
                   PaLM                   64B        Base        52.4        4.4        49.0       42.3           -            -             -
                   LLaMA                  33B        Base        35.6        7.1        57.8         -          21.7           -             -
                   InstructCodeT5+        16B        SIFT         -           -           -          -          35.0           -            11.6
                   StarCoder              15B        Base        8.4         15.1       33.9         -          33.6           -            12.2
                   Vicuna                 13B        SIFT         -           -           -          -            -            -            12.9
                   LLaMA                  13B        Base        17.8        3.9        46.9         -          15.8           -             -
                   Flan-T5                11B        SIFT       16.1*         -         48.6       41.4           -            -             -
                   Alpaca                  7B        SIFT         -           -           -          -            -            -            13.5
                   LLaMA                   7B        Base        11.0        2.9        35.1         -          10.5           -             -
                   Flan-T5                 3B        SIFT       13.5*         -         45.5       35.2           -            -             -
(GANs) [42], or autoregressive models [43] to achieve the                                            Generative Al: Real vs_           Generated Data
desired generation capabilities. These techniques allow the
model to capture the underlying data distribution and generate
new content that is similar to the training data, as shown in
Fig. 3.
   LLMs, such as ChatGPT, are a type of generative AI that
is specifically designed to generate human-like language in
response to a given prompt. These models are trained on mas-
sive amounts of textual data (see Table II), using techniques
such as unsupervised learning to learn the statistical patterns
of language. However, many people accord the capabilities
provided by GPT models to “more data and computing power”
instead of “better ML research”.
A. Generative Pre-trained Transformers - GPT 3.5                                                          Data Type        Real Data      Cenerated Data
                                                                                                                                     Real Data (Red) and Generated Data(Blue)
   The transformer-based architecture is a type of neural
network that is well-suited for NLP tasks. It uses a stack                           Fig. 3: A multivariate normal distribution applied to estimate
of self-attention layers to learn long-range dependencies in                         the mean vector and covariance matrix of the underlying
text. Self-attention [44] is a mechanism that allows the model                       distribution using maximum likelihood estimation. Finally,
to learn the importance of each word in the input sequence,                          we generate new synthetic data from the learned probability
regardless of its position. This is important for NLP tasks, such                    distribution. This demonstrates the concept of generative AI
as translation and question answering, where the meaning of                          using probability distributions, where the AI learns to generate
a sentence can depend on the words that are far apart.                               new data that is similar to the original data by modeling the
   The architecture of GPT 3.5 is shown in Fig. 4, which                             underlying probability distribution
consists of six major steps.
   1) Input Encoding: The input to the GPT model is a
        sequence of tokens representing the data. Each token is
        converted into a high-dimensional vector representation                               networks. Self-attention allows the model to capture
        through an embedding layer.                                                           dependencies between different words in the input se-
   2) Transformer Encoder: The GPT model consists of mul-                                     quence, while the feed-forward networks process and
        tiple layers of transformer encoders. Each encoder layer                              transform the representations.
        has a self-attention mechanism and feed-forward neural                           3) Contextual Embeddings: As the input sequence passes
                                                                                                                                                                     5

                                                                                                 news articles etc. This diversity in the training mix allows the
                                                      EncodingInput                              model to provide output text that is coherent just as a human
                                                                                                 written text may read. However, it should be noted that the
                                                                                                 “creativity” exhibited in LLMs goes beyond the regurgitation
                                                                                                 of data that it may have seen during the training process. To
                              OutputLayer                                    TransformerEncoder  produce text creatively, the deep learning model of the LLM
                                                                                                 needs to form an understanding of the text used in its training
                                                                                                 in aspects related to language, tone and writing patterns etc.
                                                    GPT                                          This way the LLM is able to generate responses to user queries
                                                                                                 that are creative and genuine in terms of language writing by
                                                     billion of                                  combining the different types of input information it ingested
                                                   parameters                                    during training together to generate meaningful results for the
                            Training with                                      Contextual        provided query.
                              Masked                                          Embeddings            A fundamental criterion for gaining the capability of this
                             Lang= uage                                                          creativity is to have sufficient variance, which indicates to
                             Modeling
                                                                                                 the model’s ability to produce an unexpected output. In short,
                                                    Decoding and                                 variance ensures that there is randomness in a model’s output,
                                                      Language
                                                      Generation                                 and it is introduced to enable it to generate sufficiently good
                                                                                                 results over a range of output results. By introducing variance
               Fig. 4: The architecture of Generative Pre-trained Transform-                     in the model, one can increase the diversity of output content
               ers                                                                               generated which goes beyond the scope of what the training
                                                                                                 data consisted of.
                                                                                                    It is acknowledged that since the release and mainstreaming
                       through the transformer encoders, each token’s represen-                  of LLMs by users of all walks of life, some have complained of
                       tation is updated in a contextualized manner. This means                  LLM getting stuck in a cycle of similar answers, especially if
                       that the representation of each token is influenced by the                a complex query has been asked of it multiple times. For e.g.,
                       surrounding tokens and their contextual information.                      Microsoft found that its Bing AI powered by ChatGPT tends
                  4) Decoding and Language Generation: Once the input                            to get repetitive in its responses after 15 consecutive chats 1.
                       sequence has been encoded through the transformer                         While this problem has mitigated since then by taking mea-
                       encoders, the GPT model can generate new text by                          sures such as refreshing context and/or introducing limits to
                       predicting the probability distribution of the next token                 the questions asked per session, this does question the variance
                       given the context. This process is typically done using a                 capability of LLMs. There is a philosophical consideration
                       softmax activation function, which produces a probabil-                   here, LLMs have been associated with being “God-like AI”
                       ity distribution over all possible tokens in the vocabulary.              due to their exhibited creativity in different fields. From a
                  5) Training with Masked Language Modeling: During the                          human scenario, an example could be considered fingerprints,
                       pre-training phase, GPT models often employ a tech-                       each one of the more than eight billion people in the world
                       nique called Masked Language Modeling (MLM) [45].                         has unique fingerprints which can be used to identify them.
                       MLM involves randomly masking certain tokens in the                       However, from a philosophical perspective, it could be of
                       input sequence and training the model to predict those                    interest how the size of the human population affects the
                       masked tokens based on the context. This helps the                        variance of this space of potential of different fingerprints
                       model learn contextual relationships and improves its                     for humans? Therefore, it is important to appreciate the total
                       ability to generate coherent text.                                        “space” of creativity that LLMs have by design and consider
                  6) The 6th layer represents the output of the model.                           creativity in that respect.
                  LLMs have captured significant interest in recent years due
               to their remarkable performance across an extensive array of                                             III. OVERVIEW OF LLMS
               NLP tasks, including text generation, translation, summariza-                        LLMs have revolutionized the field of artifical intelligence
               tion, question-answering, and sentiment analysis [46]. Con-                       and have found applications in various domains, including
               structed upon the foundation of the transformer architecture                      communication, content generation, and knowledge dissemi-
               [14], these models exhibit an extraordinary capacity to process                   nation. In this Section, we briefly discuss the history, training,
               and generate human-like text by leveraging massive volumes                        and working of LLMs.
               of training data.

B. Data, Creativity, and Variance
   As discussed previously, LLMs are developed by training
large deep neural networks using text data made up of multiple
different sources including but not limited to books, social
media and also text from the internet such as poetry, songs,
A. History of LLM
   LLMs are a type of AI model that can process and generate
natural language text. These models are typically trained on
  1https://www.zdnet.com/article/long-chats-confuse-bing-chat-so-microsoft
s-chatgpt-powered-bot-is-getting-new-limits/
                                                                  6

massive amounts of text data and use deep learning techniques
to learn the patterns and structures of language [47]. The
history of LLMs can be traced back to the early days of NLP
research [6].
   The first language models were developed in the 1950s and
1960s. These models were rule-based and relied on hand-
crafted linguistic rules and features to process language. They
were limited in their capabilities and were not able to handle
the complexity of NLP [48].
   In the 1980s and 1990s, statistical language models were
developed. These models used probabilistic methods to esti-
mate the likelihood of a sequence of words in a given context.
They were able to handle larger amounts of data and were
more accurate than rule-based models [49]. However, they still
had limitations in their ability to understand the semantics and
context of language [50].
   The next major breakthrough in language modeling came
in the mid-2010s with the development of neural language
models [51]. These models used deep learning techniques
to learn the patterns and structures of language from large
amounts of text data. The first neural language model was the
recurrent neural network language model (RNNLM), which
was developed in 2010. RNNLM was able to model the
context of words and produce more natural-sounding text than
previous models [52].
   In 2015, Google introduced the first large-scale neural
language model called the Google Neural Machine Translation
(GNMT) system [53]. This model was trained on massive
amounts of bilingual text data and was able to achieve state-
of-the-art performance on machine translation tasks. Figure 5
shows the evolution of LLMs.
   The development of LLMs continued with the introduction
of the Transformer model in 2017 [14]. The Transformer was
able to learn the longer-term dependencies in language and
allowed for parallel training on multiple Graphical Processing
Units (GPUs), making it possible to train much larger models
[54].
   The release of OpenAI’s GPT-1 [55] in 2018, marked a
significant advance in NLP with its transformer-based archi-
tecture. With 117 million parameters, GPT-1 could generate
contextually relevant sentences, demonstrating the potential
of transformers in revolutionizing NLP tasks [56]. It was
trained using a two-step process of unsupervised pre-training
and supervised fine-tuning, a methodology that generated
considerable interest in the academic and research commu-
nity. Although GPT-1 had its limitations, it set the stage for
subsequent, more powerful models, propelling a new era of AI
research and highly-competitive research in LLMs (see Fig. 5).
   In 2020, OpenAI released the largest language model to
date, GPT-3, which was trained on a massive amount of text
data and was able to generate highly coherent and natural-
sounding text [57]. GPT-3 demonstrated the potential of LLMs
for a wide range of NLP tasks [58].
   Inspired by the success of GPT-3, OpenAI recently an-
nounced and began working on the development of the next
iteration of their language model, GPT-4 [59]. GPT-4 is
expected to be even larger and more powerful than GPT-3,
with the ability to generate even more coherent and natural-
sounding text.
   While LLMs have found use in a number of different
applications, they can also be biased and produce inaccurate
or harmful outputs. There are a number of factors that can
contribute to the weakness of an LLM. One factor is the size
of the training dataset. If the dataset is too small, the model
may not be able to learn to generalize to new situations.
Another factor is the quality of the training dataset. If the
dataset contains biased or inaccurate information, the model
will learn to reflect that bias or inaccuracy.
   There are also a number of factors that can contribute
to the strength of an LLM. One factor is the size of the
model. Larger models have more parameters, which allows
them to learn more complex relationships between words and
concepts. Another factor is the architecture of the model. Some
architectures are better suited for certain tasks than others.
   In order to evaluate the performance of an LLM, it is
important to consider the following factors:
   • The size of the training dataset.
   • The quality of the training dataset.
   • Parametric size of the model
   • Complexity of the architecture of the model.
   • The task that the model is being evaluated on.
   It is also important to note that LLMs are still under
development, and their performance can vary depending on
the specific task and the environment in which they are used.
The following subsection discusses the most common LLMs,
their training and working principles.

B. Training of LLMs
   Training large language models involves several key steps
that are fundamental to their successful development. The
process typically begins with the collection and preprocessing
of a massive amount of text data from diverse sources, such
as books, articles, websites, and other textual corpora (see
Table. IV). The curated dataset [61] serves as the foundation
for training the LLMs. The training process itself revolves
around a technique known as unsupervised learning, where
the model learns to predict the next word in a sequence given
the preceding context. This task is commonly referred to as
language modeling. LLMs utilize sophisticated neural network
architectures, such as Transformers, which enable them to
capture complex patterns and dependencies in language. The
training objective is to optimize the model’s parameters to
maximize the likelihood of generating the correct next word
in a given context [3]. This optimization is typically achieved
through an algorithm called stochastic gradient descent (SGD)
or its variants, combined with backpropagation, which com-
putes gradients to update the model’s parameters iteratively.
   • Generative Pre-trained Transformer 3 (GPT-3): GPT-3 is
      one of the most advanced and largest language models
      developed by OpenAI [57]. It represents a significant
      breakthrough in NLP and has garnered considerable
      attention due to its impressive capabilities [3]. GPT-3
      follows a transformer-based architecture, which allows it
      to capture complex linguistic patterns and dependencies
                                                                                                                                                                            7
                                GPT-4                        Bard                     LaMA          Falcon     Guerilla     Jurassic-2            Claude    Red Pajama
                12023        ChatGPT             Minerva Flan PaLM Gallactica              OPT-IML                  Sparrow                     Anthropic      BLOOMZ
                                                              PaLM                          GPT-NeoX                                           LM_v4-s3         BLOOM
                         InstructGPT              LaMDA                     OPT                                    Chinchilla      YaLM
                2022                                                                                         GoPher                                           Google
                                                  GLaM          MT-NLG                      GPT-J                                            AnthropicLM      Meta
                            CodeX                                             GPT-Neo                                      Jurassic-1                         OpenAI
                2021                                                                                                                                          ElutherAI
                                                                                                                                                              Anthropic
                          GPT-3                                                                                                                               A121
                                                                                                                                                              DeepMind
                (2020                                                                                                                                         BigScience
                         GPT-2                    XLNet                                                                                                       Microsoft-
                                                                                                                                                              NVIDIA
                2019                                                                                                                                          YCombinator
                         GPT-1
                 2018                                                                                                                                         Startups
                                                                                                                                                              Universitles
                                           Fig. 5: Evolution of LLMs across different research and commercial organizations.

               TABLE IV: State-of-the-art for LLM training pipeline [60]. Notations: RM: Reward Modeling, RL: Reinforcement Learning,
               SFT: Supervised Fine-tuned.
                                       Stage       Pretraining                Supervised-                Reward Modeling            Reinforcement Learning
                                                                              Finetuning
                                      Dataset      Raw Internet II            Demonstration              Comparisons                Prompts
                                     Algorithm     Language Modeling          Language Modeling          Binary Classification      Reinforcement Learning
                                       Model       Base Model                 SFT Model                  RM Model                   RL Model
                                     Resources     100s of GPUs               1-100 of GPUs              1-100 of GPUs              1-100 of GPUs
                                                   months of training         days of training           days of training           days of training
                                                   deployable                 deployable                 not deployable             deployable

   in text [62]. The model consists of a stack of transformer
   layers, enabling it to process and generate text at various
   levels of abstraction. With a staggering number of approx-
   imately 175 billion parameters, GPT-3 is currently one
   of the largest language models ever created. The training
   process of GPT-3 involves unsupervised learning on a
   massive corpus of publicly available text data from the
   internet. By leveraging its enormous size and extensive
   training data, GPT-3 has acquired a broad understanding
   of language and can generate human-like text across
   various topics [63].
• Bidirectional Encoder Representations from Transformer
   (BERT): BERT is a prominent language model with
   significantly advanced NLP tasks. Its training process
   comprises pretraining and fine-tuning stages [64]. During
   pretraining, BERT learns a general language represen-
   tation from large-scale unlabeled text data. It employs
   masked language modeling (MLM) and next-sentence
   prediction (NSP) tasks. MLM involves masking a portion
   of input tokens and training the model to predict the
   original masked tokens, fostering bidirectional context
   understanding [65]. NSP trains BERT to predict whether
    a second sentence follows the first, enhancing coherence
    comprehension. After pretraining, BERT undergoes fine-
    tuning on specific tasks with labeled data. Fine-tuning tai-
    lors BERT’s learned representations to target tasks, such
    as sentiment analysis or named entity recognition. It em-
    ploys backpropagation and gradient descent optimization
    to update model parameters. Training BERT demands
    significant computational resources [56], utilizing high-
    performance hardware like GPUs or Tensor Processing
    Units (TPUs) or field programmable gate arrays (FPGAs)
    [66], [67], [68]. Techniques such as layer normalization,
    residual connections, and attention mechanisms inherent
    in the transformer architecture further enhance BERT’s
    capacity to capture intricate dependencies and long-range
    contextual relationships.
• eXtreme Language understanding Network (XLNet): XL-
    Net is a generalized autoregressive pre-training method
    that surpasses the limitations of traditional left-to-right
    or right-to-left language modeling. XLNet is trained
    using a permutation-based approach that differs from
    traditional autoregressive models [69]. In the training
    process, rather than predicting the next word given the
                                                                     8

   previous words in a fixed order, XLNet considers all
   possible permutations of the input sequence and models
   the probability of each permutation. This allows XLNet to
   capture dependencies in both directions, thus addressing
   the limitations of sequential left-to-right or right-to-left
   modeling [70]. The training of XLNet involves two
   key steps: unsupervised pretraining and supervised fine-
   tuning. During unsupervised pretraining, XLNet learns to
   predict words conditioned on the entire input context by
   maximizing the expected log-likelihood over all possible
   permutations. This is achieved using a variant of the
   transformer architecture, similar to models like BERT.
   The permutation-based objective function used in XLNet
   training presents unique challenges. Unlike traditional
   autoregressive models that can rely on the causal order
   of words for prediction, XLNet needs to consider all
   possible permutations, resulting in an exponentially large
   number of training instances. This makes the training
   process computationally intensive and requires efficient
   strategies, such as ”factorized sampling,” to sample a
   subset of permutations during each training iteration.
   Another difficulty in training XLNet is the need for
   large-scale computing resources [3], [71], [72]. The vast
   number of possible permutations and the large model
   size contribute to increased memory and computation re-
   quirements. Training XLNet often necessitates distributed
   training on multiple GPUs or TPUs and can take signif-
   icant time [3].
• Text-to-Text Transfer Transformer (T5): T5, developed
   by Google, is a versatile language model that is trained
   in a ”text-to-text” framework. The training process of
   T5 involves two main steps: pretraining and fine-tuning.
   During pretraining, T5 is trained on a massive corpus of
   publicly available text from the internet. The objective is
   to learn a generalized representation of language that can
   be applied to a wide range of tasks.The key innovation
   of T5 is the formulation of all tasks as text generation
   problems. This means that every task, including text
   classification, summarization, translation, and question
   answering, is cast into a text-to-text format. For example,
   instead of training T5 to answer questions directly, it
   is trained to generate the complete answer given the
   question and relevant context. In the pretraining phase, T5
   is trained using a variant of the transformer architecture.
   The transformer model allows T5 to capture long-range
   dependencies and effectively model the contextual rela-
   tionships in the input text [73]. The pretraining objective
   is typically based on maximum likelihood estimation,
   where T5 is trained to predict the target text given the
   source text. Once pretraining is complete, T5 undergoes
   fine-tuning on specific downstream tasks [73].
   One of the challenges in training T5 is the availability
   of large-scale labeled datasets for various tasks. Fine-
   tuning requires task-specific labeled data, and the quality
   and quantity of the data play a crucial role in the
   model’s performance [3]. Additionally, the computational
   resources required to train T5 can be substantial, as the
   model is computationally intensive due to its transformer
      architecture and the size of the pre-trained parameters.
   • Conditional Transformer Language Model (CTRL):
      CTRL is a language model designed to generate text
      based on specific control codes or prompts. It is trained
      using a two-step process: pretraining and fine-tuning.
      During pretraining, CTRL is trained on a large corpus
      of publicly available text data [74]. The objective of pre-
      training is to teach the model to understand and generate
      coherent text based on different control codes or prompts
      [75]. The training data includes diverse sources such
      as books, articles, websites, and other text documents.
      The training process involves utilizing the transformer
      architecture, similar to models like BERT and GPT.
      The model is trained to predict the next word or phrase
      in a given context, learning the statistical patterns and
      linguistic structures of the language. One of the unique
      aspects of CTRL is its conditioning of control codes
      or prompts. These control codes guide the model’s text
      generation process, allowing users to specify the desired
      style, topic, or other characteristics of the generated text.
      The control codes act as explicit instructions to guide the
      model’s behavior during both training and inference. The
      fine-tuning phase of CTRL is crucial for adapting the
      model to specific tasks or domains. Fine-tuning involves
      training the pre-trained CTRL model on task-specific
      datasets with control codes. The model is exposed to task-
      specific prompts and is trained to generate text that aligns
      with the desired output or behavior for the given task.

C. How LLMs work

   At their core, LLMs, are a type of AI that can mimic
human intelligence. They function by employing advanced
statistical models and deep learning techniques to process and
understand extensive amounts of text data [76]. These models
learn the intricate patterns and relationships present in the data,
enabling them to generate new content that closely resembles
the style and characteristics of a specific author or genre [77].
   The process begins with pre-training, during which the LLM
is exposed to a massive corpus of text from various sources
such as books, articles, and websites. Through unsupervised
learning, the model learns to predict the next word in a
sentence based on the context of the preceding words. This
allows the model to develop an understanding of grammar,
syntax, and semantic relationships [74]. As shown in the
LLMs pre-training pipeline in Figure 6, the first step is pre-
training corpus sources which can be roughly divided into
two categories: general data and specialized data. Following
the collection of a huge amount of text data, it is critical
to preprocess the data in order to generate the pre-training
corpus, particularly by removing noisy, redundant, unneces-
sary, and potentially poisonous material [78] [79].The second
stage involves quality filtering to remove the low quality and
unwanted data from the training corpus using some techniques
such as the language filtering, statistic filtering and keyword
filtering [79]. Third stage is deduplication, where previous
research [80] discovered that duplicate data in a corpus reduces
the diversity of LMs, causing the training process to become
                                                                                                                                                                               9
                           Raw'Corpus                  Quality Filtering            Deduplication           Privacy Reduction          Tokenization          Ready to pre-train!
                                                        Metric Filtering              Sentence-level            Detect Personality      Reuse Existing
                                                        Language Filtering            Document-level            Identifiable              Tokenizer
                                                        Keyword Filtering             Set-level                 Information (PII)       Sentence Piece
                                                                                                                 Remove PII             Byte-level BPE
                                                         Statistic Filtering
                                                      Authors are crafting        Authors are crafting
                                                        an article about            an article about
                                                             LLMs                        LLMs.               Replace('Authors'      Encode( (Somebody]
                                                       #S^& Authors are                                        are crafting an      is crafting an articleabout LLMs:)
                                                                                   #S4& Authors are          article about LLMs
                                                       crafting an article         crafting an article                                                          32, 145,66
                                                          about LLMs                  about LLMs                                                                79,12, 56,
                                                Fig. 6: The illustration of a data processing pipeline for pre-training LLMs.

unstable and thereby affecting model performance [80]. The
fourth stage privacy reduction, it is crucial to address privacy
concerns related to the use of web-based data for pre-training
language models. This data often includes user-generated
content containing sensitive or personal information, thereby
posing a potential risk of privacy breaches [81]. Therefore, it
is essential to undertake privacy redaction measures to remove
personally identifiable information (PII) from the pre-training
corpus. The last step is tokenization. It is an important step in
data preprocessing as well. It seeks to segment raw text into
sequences of individual tokens, which are then fed into LLMs.
   Following pre-training, the LLM undergoes fine-tuning,
which involves training the model on a specific task or domain.
During this phase, the model is provided with labeled exam-
ples and guided to generate more accurate and contextually
appropriate responses for the target task [74]. Fine-tuning
allows the LLM to specialize in various applications, such as
language translation, question-answering, or text generation.
   The success of LLMs lies in their ability to capture the
statistical patterns and linguistic nuances present in the training
data [75]. By processing and analyzing vast amounts of text,
LLMs gain a comprehensive understanding of language and
are able to generate coherent and contextually relevant re-
sponses. During the inference stage, when interacting with an
LLM, a user inputs a prompt or query. The model processes the
input and generates a response based on its learned knowledge
and context. This response is generated using probabilistic
methods that consider the likelihood of various words or
phrases given the input context. In Figure 6, we show the
data pre-processing pipelines for pre-training LLMs.

     IV. APPLICATIONS OF LARGE LANGUAGE MODELS
   Given LLMs wide range of applications, in this section, we
provide a discussion of their use in the fields of medicine,
education, finance, and engineering. The selection of medical,
education, finance, and engineering as the applications for
LLM is based on their significance, relevance, and potential
impact within their respective domains. These applications
demonstrate the versatility and potential of LLMs in address-
ing complex challenges and supporting human endeavors.

A. Medical

   LLMs like ChatGPT have exhibited remarkable potential
in diverse healthcare applications, particularly in the field of
medicine. They have been successfully employed in medical
education, radiologic decision-making, clinical genetics, and
patient care, as supported by several studies [82],[83]. In
medical education, ChatGPT has emerged as an interactive tool
that aids learning and problem-solving [84]. Notably, Chat-
GPT’s performance in the United States Medical Licensing
Exam (USMLE) was comparable to or exceeded the passing
threshold, indicating its proficiency in medical knowledge
without requiring specialized training or reinforcement [84].
Moreover, ChatGPT’s explanations displayed a high level of
concordance and insightful understanding [82].
   According to a study conducted by Rao et al. [85], it is
anticipated that specialized AI-based clinical decision-making
tools will emerge in the future. This study emphasizes the
potential of ChatGPT in radiologic decision-making, high-
lighting its feasibility and potential benefits in improving
clinical workflow and ensuring responsible use of radiology
services. Similarly, Kung et al. [82] concluded in their research
that LLMs, including ChatGPT, have the capacity to enhance
the delivery of individualized, compassionate, and scalable
healthcare. These models can assist in medical education and
potentially aid in clinical decision-making.In the domain of clinical genetics, a paper by Duong and
Solomon [86] found that ChatGPT’s performance did not
significantly differ from humans when answering genetics-
related questions. However, the model demonstrated better ac-
                                                                      10

curacy on memorization-type questions compared to questions
requiring critical thinking. Notably, this study also highlighted
that ChatGPT provided varying answers when asked the same
question multiple times, providing plausible explanations for
both correct and incorrect responses. Furthermore, Fijacko
[87] conducted a study to evaluate ChatGPT’s accuracy in
answering questions related to life support and resuscitation.
The findings revealed that ChatGPT demonstrated the ability
to provide accurate answers to a majority of the questions
on the American Heart Association’s Basic Life Support and
Advanced Cardiovascular Life Support exams.
   In the field of neurosurgical research and patient care,
ChatGPT has been investigated for its potential role in var-
ious aspects, including gathering patient data, administering
surveys or questionnaires, and providing information about
care and treatment [88]. Nevertheless, the implementation of
such technologies necessitates careful consideration to ensure
their effectiveness and safety. The integration of biotechnology
and AI to address global challenges and advance sustainable
development goals is examined in a research paper that
encompasses a wide range of AI applications in the life
sciences. These applications encompass decision support, NLP,
data mining, and machine learning. The authors underscore
the significance of reproducibility in the development of AI
models and highlight ongoing research issues and challenges
in these domains [89]. Furthermore, AI-powered chatbots like
ChatGPT hold the potential to enhance patient outcomes by
facilitating communication between patients and healthcare
professionals. Leveraging NLP, these chatbots can provide
patients with information about their care and treatment in
a more accessible manner [90]. A database for Covid-19
drug repurposing using NLP is proposed in [91]. There have
several tools already in use that allows the system to interact
with patients such as Ada Health, Babylon Health and Buoy
Health. The recent popularity of LLMs can potentially not only
improve patient confidence in interacting with such chatbots
but also improve upon the services provided. In fact, there are
tools developed to assist medical practitioners. One such tool
is XrayGPT [92], it can be used for automated analysis of X-
ray images and have the user/patient ask questions about the
analysis. Through the chats, the user can get insight into their
condition through an interactive chat dialogue.

B. Education
   The impact of AI on education has been a topic of much
discussion in recent years. One area where AI is having a
significant impact is in the realm of student assignments and
exams. Since the advent of ChatGPT developed by OpenAI,
the way students interact with educational materials, assign-
ments and coursework has become different [93] [94] [95].
One of the main advantages of using ChatGPT and AI bots
in education is that they can help students complete their
assignments more efficiently             [96]. ChatGPT is capable of
generating high-quality responses to a wide range of prompts,
which can save students time and effort when they are working
on assignments. Additionally, AI bots can help to automate the
grading process, which can reduce the workload for teachers
and enable them to provide more detailed feedback to students.
   Another advantage of using ChatGPT and AI bots in educa-
tion is that they can provide personalized learning experiences
for students. AI bots can analyze a student’s performance
on previous assignments and exams and use this data to
generate personalized recommendations for future work. This
can help students to identify their strengths and weaknesses
and focus their efforts on areas where they need to improve.
Khan Academy, a nonprofit educational organization, has
shown interest in utilizing ChatGPT for its business. They
have developed an AI chatbot called Khanmigo, which serves
as a virtual tutor and classroom assistant [97]. The goal
of incorporating ChatGPT into their platform is to enhance
tutoring and coaching experiences by providing one-on-one
interactions with students. This is optimism about the potential
of AI, including ChatGPT, in the field of education. The use
of AI in assisting with tutoring and teaching also dispels the
notion that its primary use is for cheating. Undoubtedly, AI
technology is still in its early stages, but it holds promise for
helping students and addressing their diverse needs [98].
   However, there are also some potential drawbacks to using
ChatGPT and AI bots in education. One concern is that these
technologies may lead to a loss of creativity and critical
thinking skills among students. If students rely too heavily on
AI bots to complete their assignments and exams, they may
not be developing the skills necessary to think critically and
solve problems on their own [96].
   1) Learning in the age of AI: Another major assistance
that these bots such as ChatGPT can offer is the provision
of assistance in designing a course in an academic setting.
AI chatbots can serve as a valuable tool to aid in various
aspects of syllabus preparation. Course objectives can be gen-
erated, relevant topics identified, curricula structured, learning
resources gathered and reviewed, assessment methods defined,
engaging learning activities established, and a well-balanced
course schedule created. The iterative process of interacting
with ChatGPT enables refinement and enhancement of the
syllabus based on the model’s suggestions and insights. It is
important to note that ChatGPT acts as a supportive tool, aug-
menting the expertise and input of experienced educators. The
collaboration between human and AI in the course syllabus
design process facilitates the development of comprehensive
and effective learning plans that align with desired learning
outcomes.
   2) Major issues for AI in Education: One of the major con-
cerns is the utilization of these tools without proper training.
Even if LLMs can provide answers to a plethora of questions
and assist the user in providing answers to questions on their
fingers, it is important for pupils in educational institutions
to be trained sufficiently to make the best use of LLMs
capabilities.
   Another concern is that the use of AI bots in education
could lead to increased inequality [99]. Students who have
access to these technologies may have an unfair advantage over
those who do not, which could exacerbate existing inequalities
in education. Additionally, the use of AI bots could lead to
a decrease in the number of teaching jobs available, which
could further widen the gap between those who have access
to education and those who do not. In conclusion, the use
                                                                                                                                                             11
                       45      Distribution of Citations with Fake Citation Probability  identify potential opportunities in the trading market by using
                                   Real citations                                        its predictive and analyzing capabilities.
                                    Overall distrbulion                                     However, due to the sensitivity of the financial information
                                   Fake citations
                       3.5                                                               and privacy concerns, techniques like data encryption, redac-
                                                                                         tion and data protection policies should be implemented so
                    8                                                                    that these LLMs can be used efficiently in accordance with
                                                                                         data protection policies. In this regard, a recent proposition
                    L                                                                    suggested is FinGPT [100] which is an open-source LLM
                                                                                         tailored for finace. It is expected that more work will be carried
                                                                                         out in this space.




                                  Probability of citation

Fig. 7: The overall citation distribution is represented by a
gray histogram with 50% opacity. The blue line represents the
probability density function of the overall citation distribution,
estimated using a Gaussian kernel density estimator. The
red line represents the probability density function of fake
citations, estimated using a Gaussian kernel density estimator.
Prompt ”SUPPOSE you have the probability distribution of
a universe, and when you sample from them there is a high
probability of fake citation, I want to show this concept on a
single probability distribution” Please provide MATLAB code.


of ChatGPT and AI bots in education has both pros and
cons. While these technologies can help students complete
assignments more efficiently and provide personalized learning
experiences, they may also lead to a loss of critical thinking
skills and increased inequality. As AI continues to transform
the field of education, it will be important to carefully consider
these potential benefits and drawbacks and work to minimize
the discussed negative consequences that may arise.


C. Finance

   LLMs are making significant advancements in the finance
industry with applications ranging from financial NLP tasks,
risk assessment, algorithmic trading, market prediction and
financial reporting. LLM’s such as BloombergGPT[27], a 50
billion parameter large language model trained on large diver-
sified financial corpus, has revolutionized financial NLP tasks
such as news classification, entity recognition and question
answering. By utilizing the huge amount of financial data
available, it is able to enhance customer services drastically
by efficiently handling customer queries and providing them
with excellent financial advisory.
   In addition, LLMs are being used for risk assessment and
management, by analyzing past market trends and data, it is
able to identify potential risks and provide mitigation steps
through different financial algorithms. Financial institutions
can use it for better decision making such as credit risk
assessment, loan approvals and investments. Algorithmic Trad-
ing is another application that can leverage LLM models to
D. Engineering related applications
   LLMs have gained substantial attention across various
fields, and their potential applications in engineering domains
are increasingly being explored. For instance, ChatGPT has
diverse applications in software engineering, including code
generation, debugging, software testing, NLP, documentation
generation, and collaboration. It enables developers to generate
code snippets, identify and fix errors, generate test cases,
analyze user requirements, create user interfaces, generate
software documentation, and facilitate collaboration within de-
velopment teams. ChatGPT’s language understanding and gen-
eration capabilities enhance efficiency, streamline workflows,
and foster effective communication in software engineering.
   In software engineering, ChatGPT can be employed to
generate code snippets based on natural language descriptions
of desired functionality. This feature saves developers time and
improves overall efficiency, allowing them to focus on higher-
level design aspects [101]. Additionally, ChatGPT can assist in
debugging code by leveraging its language understanding ca-
pabilities to identify errors and suggest potential fixes, thereby
streamlining the debugging process and reducing development
time. The use of ChatGPT extends to software testing, where
it can generate test cases and test data based on natural
language descriptions of desired test scenarios. This approach
enhances the efficiency and effectiveness of software testing,
ensuring comprehensive coverage and accurate validation of
the software’s functionality.
   In mechanical engineering, Tiro [102] has study the pos-
sibility of ChatGPT utilization to various calculations in me-
chanical engineering, Tiro encountered instances where incor-
rect procedures, formulas, or results were provided. None of
the tasks yielded an exact solution, leading them to discontinue
further research. Based on Tiro findings, it can be concluded
that, at the current stage of AI development, ChatGPT should
not be relied upon for solving engineering practice problems.
Furthermore, caution should be exercised in using ChatGPT
for such applications, as incorrect results can have potential
consequences.
   In Mathematics, some attempts have been done such as
Wardat et al [103] found that ChatGPT holds potential for
assisting in teaching mathematics by providing interactive
and dynamic learning experiences. It can generate customized
examples and problem-solving strategies tailored to individual
student needs, fostering personalized learning. Moreover, it
can serve as a virtual tutor, offering real-time feedback and
guidance, identifying areas of difficulty, and suggesting al-
ternative approaches. As an AI language model, ChatGPT is
capable of performing mathematical calculations and solving
math equations. However, the accuracy and effectiveness of
ChatGPT solutions may depend on various factors such as the

complexity of the equation, the accuracy of the input data,
and the instructions given to ChatGPT. Frieder et al., [104]
has investigated the mathematical capabilities of ChatGPT by
testing it on publicly available datasets, as well as hand-crafted
ones, and measuring its performance against other models
trained on a mathematical corpus, such as Minerva. They also
test whether ChatGPT can be a useful assistant to professional
mathematicians by emulating various use cases that come up
in the daily professional activities of mathematicians (question
answering, theorem searching).
    However, it is essential to acknowledge the limitations of
ChatGPT, including the possibility of generating incorrect
responses or failing to address complex mathematical concepts
adequately. Therefore, it should be utilized as a supplemental
tool alongside traditional teaching methods and human super-
vision to ensure accuracy and quality in teaching mathematics.
    In manufacturing, Wang et al. [105] conducted an evaluation
of ChatGPT’s capabilities in supporting design, manufactur-
ing, and engineering education tasks. The results indicate that
ChatGPT is impressive in providing information, generating
coherent and structured content, and proposing initial solu-
tions. The authors recommended a technology development
roadmap to successfully integrate ChatGPT into the man-
ufacturing industry. Therefore, in manufacturing, ChatGPT
struggles to understand questions and lacks the ability to
properly use knowledge to generate correct solutions and it
can even fabricate non-existing rules or equations in order to
generate solutions. Similarly, Badini et al. [106], performed
a study in additive manufacturing troubleshooting and evalu-
ated ChatGPT’s expertise in technical matters, focusing on
the evaluation of printing parameters and bed detachment,
warping, and stringing issues for Fused Filament Fabrication
(FFF) methods using thermoplastic polyurethane polymer as
feedstock material. It was found that ChatGPT provided re-
markable accuracy, correctness, and organization in its re-
sponses and its approach to problem-solving offered valuable
insights in addressing hurdles. In particular, for the specific
technical issues of warping, bed detachment, and stringing,
ChatGPT demonstrated its ability to provide hierarchical and
logically organized responses while taking into account given
information and constraints. Furthermore, it was also able
to fine-tune printing parameters for different types of TPU
filaments, showing its ability to relate the mechanical prop-
erties of the filament material to the printing parameters.
Finally, the authors recommended integrating ChatGPT into an
Additive Manufacturing software platform to provide real-time
suggestions and optimization for users, which can enhance the
efficiency and quality of the Additive Manufacturing process.

   V. AI-ENABLED TOOLS: THE FUTURE OF EVERYTHING

    AI tools are becoming increasingly powerful and versatile.
They can be used to generate text, translate languages, write
different kinds of creative content, and answer your questions
in an informative way. These powerful tools are designed to
                                                                                                   12
              explain the coneeptol chalupt

                                                                       GpT {scncudiyc Ac        0 Q
              trained Transtormer) tamily. whichis basedon deep learning techniques  specitically
              transtormcr ncuralnctworks ChalGPT is dcsigned to cage in convcrsationalintcractions
                       providing human-bke responsesto promptsquestions
                                                                              During [e
              previous Motds This pocess helps the modellearn ptammar,facts reasoning abilities and


    Fig. 8: A simple educational conversation with ChatGPT


understand and generate human-like text, offering a wide range
of applications and benefits. AI tools built upon LLMs provide
developers and researchers with accessible APIs and libraries
to leverage the capabilities of these models. They offer a user-
friendly interface for tasks like text generation, image gen-
eration, coding, sentiment analysis, language understanding,
and content recommendation [3]. In this Section, we discuss
various AI-enabled tools based on LLMs.

A. Chatbots / ChatGPT
    Chatbots are frequently used in customer service applica-
tions where they can respond to queries, offer assistance,
and fix problems [107]. They can also be utilised for other
purposes, including entertainment, healthcare, and education.
Chatbots and LLMs are often used together to create more
sophisticated and engaging conversational experiences. For
example, a chatbot might use an LLM to generate text for
its responses. Some of the popular chatbots include ChatGPT,
Google Bard, and Microsft Bing. In Fig. 8, we show a simple
educational conversation with ChatGPT.
    Another
    1) Comparison between Chatbots: ChatGPT and Google
Bard are two of the most popular LLMs available today [108].
Both models are capable of generating text, translating lan-
guages, writing different kinds of creative content, and an-
swering your questions in an informative way. However, there
are some key differences between the two models, such
as ChatGPT is more creative, while Google Bard is more
authentic. Table VII presents a comparison between ChatGPT,
Google Bard, and Microsoft Bing Chatbots.


B. AI tools for image generation, history, medical, industry
    Table V showcases the output of image generation using
various prompts. In total, nine different prompts were used,
these required the AI model to generate humans and natural
scenery. The first four prompts tended to depiction of famous
personalities (sportsmen and politicians in this case), Muham-
mad Salah, Lionel Messi, Mike Tyson and Imran Khan. The
prompts used were ”Mo Salah playing cricket”, ”Lionel Messi
playing tennis”, ”Mike Tyson playing football” and ”Imran
Khan as a hero”. The second prompt used was regarding
the famous painting Monalisa. The prompt was ”Generate an
image of Monalisa showing her teeth in a wedding ceremony”.
The third prompt related to natural scenery and was written
as ”Area of rocks, deep inside the forest, divine domain”.
                                                                                                                                                       13
     sharly                AHenilorWlyou netd pcl                                  information. Using fake citations and references can hide the
                                                                                   true sources of information used in the research, making it
                                                                                   difficult for others to replicate or verify the findings. To avoid
                                                                                   these complications, it is important to ensure that any citations
                                                                                   and references used are accurate and reliable and that they
                                                                                   have been properly vetted and sourced. It is also important
                                                                                   to be transparent about the sources of information used in
                                                                                   research so that others can verify and build upon the work.
                                                                                   Finally, developers of AI tools should implement rigorous
Fig. 9: An example of PdfGPT. Upload any PDF document and                          quality control measures to ensure that their tools generate
start chatting. It helps in summarizing, highlighting, critiquing,                 accurate and reliable citations and references.
and simplifying the content.                                                       Recently, WebChatGPT 2 is an impressive extension that has
                                                                                   the potential to address the pervasive issue of fake citations.
                                                                                   With the installation of this extension, WebChatGPT becomes
Lastly, the fourth prompt also centered around the generation                      equipped with robust capabilities to detect and eliminate fake
of humans. In this case, three prompts were given, ”A man                          citations. This advanced tool uses sophisticated algorithms
kissing a girl”, ”Generate an image of a guy” and Generate                         to analyze the authenticity and reliability of citations, ensur-
and image of a woman”.                                                             ing that only accurate and legitimate sources are included.
                                                                                   By incorporating WebChatGPT into the research process,
C. AI tools for text classification                                                researchers and writers can confidently rely on its ability to
    AI tools are increasingly being used for text classification.                  verify citations, resulting in improved academic integrity and
Text classification is the process of assigning a category to a                    the mitigation of misleading information.
piece of text [109]. For example, a text classification tool could
be used to classify emails as spam or not spam or to classify                      E. AI tools for coding / CodeGPT
news articles as business, sports, or entertainment. Some of                           AI tools are increasingly being used to help programmers
the popular libraries include Scikit-learn, NLTK, and Spacy.                       write code. These tools can be used to automate tasks such
                                                                                   as code completion, refactoring, linting, and testing [115].
D. AI tools for Literature review Research                                         GitHub Copilot [116] is an AI-powered code completion tool
    AI tools are increasingly being used to assist with literature                 developed by GitHub in collaboration with OpenAI. It utilizes
review research. These tools can be used to automate tasks                         OpenAI’s GPT-3 language model to assist developers in writ-
such as: Identifying relevant literature, extracting informa-                      ing code more efficiently LLMs have been used to develop
tion, and summarizing the content [110]. One such tool is                          applications in three primary categories which include: (a)
PDFGPT [111], which uses the GPT-3 model to generate                               Question Answering, (b) Creativity (c) Multi-step planning.
responses to user queries. PDFGPT can be used to extract                           These template categories are illustrated in Fig. 10.
information from PDF files, answer questions about the con-                                                          VI. GPT-PLUG-INS
tent of PDF files, and generate summaries of PDF files. An
example of PDFChat is shown in Fig. 9.                                                 GPT-Plugins are a new way to extend the functionality
    Another interesting AI tool is elicit.org, which helps au-                     of ChatGPT. They allow developers to create custom apps
tomate literature reviews. The website offers a variety of                         that can be integrated into ChatGPT, providing users with
features, including, finding relevant literature, summarizing                      new features and capabilities. GPT-Plugins can be used to
and visualizing literature, and extracting relevant information.                   do things, such as access to external data sources, automate
    1) Fake references: One of the major drawbacks of using                        tasks, and enhance user experience [118]. In this Section, we
AI tools such as ChatGPT in research is the creation of                            demonstrate several GPT-Plug-ins.
fake citations and references using AI tools can have serious
complications, particularly in academic or professional settings                   A. ChatGPT prompts guidelines
where accuracy and credibility are essential                        [112], [113].      Arguably, the watershed event in the use of ChatGPT
The potential complications that are being created due to                          was the introduction of plugins by OpenAI. Plugins allow
the uncontrolled usage of these tools result in many issues                        ChatGPT to communicate with third-party sources of data
among which misleading the scientific community carries vital                      and knowledge bases, thereby providing a platform to ex-
importance. Fake citations and references can mislead readers                      tend ChatGPTs capabilities for composition, summarization,
into thinking that a certain piece of information has been                         nuanced tasks such as sentiment analysis and more to any
sourced from a credible and reliable source, when in fact it                       resource on the internet. Moreover, given that ChatGPT has
has not. This can undermine the credibility of the author and                      provided sufficiently acceptable performance for various tasks,
the work they are presenting. Similarly, the research which                        plugins allow for ChatGPT to provide answers to queries
is based on fake citations and references has compromised                          with updated information from the internet which may not be
integrity [114]. This can lead to inaccurate conclusions and
potentially harmful decisions being made based on faulty                               2https://tools.zmo.ai/webChatGPT
                                                                                                     14

                                                     TABLE V: Image generation examples
Prompt:            Different famous personalities in roles other than their original ones
Negative Prompt:   blurry, photorealistic
                                                                                           7851)
Generated Images:
                                     a                                         b           c      d
Prompt:            Generate an image of Monalisa showing her teeth in a wedding ceremony
Negative Prompt:   blurry, low resolution, artistic

Generated Images:                    a                                         b           c      d

Prompt:            Area of rocks, deep inside the forest, divine domain
Negative Prompt:   artistic, blurry, background

Generated Images:
                                     a                                         b           c      d
Prompt:            A man kissing a girl/ Generate an image of a guy/ woman
Negative Prompt:   artistic, blurry, background, young

Generated Images:
                                     a                                         b           c      d
                                                                                                                                                                      15

                                                                         TABLE VI: Publicly available AI /LLM tools
                               Tools                   Function                                                      Link                         Availability
                               ChatGPT                 Conversational AI Chatbot                                     https://chat.openai.com/     Both

                               RoomGPT                 Redesign your room in eight different themes                  https://www.roomgpt.io/      Public

                               HomGPT                  Redesign your home and office                                 http://homgpt.com/           Subscription based

                               PDFGPT.IO               Turns PDF into the knowledge base for a ChatGPT type          https://pdfgpt.io/           Subscription based
                                                       interface

                               TexGPT                  Harnesses GPT-3’s power to help you write in Overleaf         https://blog.writefull.com
                                                                                                                     /texgpt-harness-the-power
                                                                                                                     -of-ChatGPT-in-overleaf/

                               BloombergGPT            A Large Language Model for Finance                            NA                           NA

                               AutoGPT                 Auto-prompting without the user intervention                  https://autogpt.net/         Public

                               AgentGPT                Autonomous AI agent in the browser                            https://agentgpt.reworkd.a   Public
                                                                                                                     i/

                               XrayGPT                 Automated analysis of chest radiographs based on the          https://github.com/mbzua     Public
                                                       given x-ray                                                   i-oryx/XrayGPT

                               Video-ChatGPT           A vision language model for video understanding and           https://github.com/mbzua     Public
                                                       conservation about videos                                     i-oryx/Video-ChatGPT

                               ClimateGPT              Large language model for a conversation about the cli-        https://github.com/mbzua     Public
                                                       mate in English and Arabic                                    i-oryx/ClimateGPT

                               CodeGPT                 An AI assistant to find errors in code, debug code, and       https://code-gpt-docs.verc   Public
                                                       more                                                          el.app/

                               BiomedGPT               A Unified and Generalist Biomedical Generative Pre-           https://github.com/taokz     Public
                                                       trained Transformer for Vision, Language, and Multi-          /BiomedGPT
                                                       modal Tasks

                               Elicit                  AI research assistant, automated literature reviews           https://elicit.org/          Public

                               Citation AI             AI research assistant to generate real evidence-based         https://consensus.app/sear   Subscription based
                                                       answers                                                       ch/

                               Midjourey AI            AI tool to create realistic synthetic images                  https://www.midjourney.o     Subscription based
                                                                                                                     rg

                               DALL.E2                 DALL·E 2 is an AI system that can create realistic images     https://openai.com/produ     Subscription based
                                                       and art from a text description                               ct/dall-e-2

                               VALL-E                  An audio synthesization tool                                  https://github.com/enhuiz/   Public
                                                                                                                     vall-e

                               AI Avatar               Avatar generation                                             https://ai-avatar-generator  Public
                                                                                                                     .com/

present in its training dataset. This also has the advantage of
providing references for queries to add credibility to answers.
For e.g., Bing, the search engine by Microsoft works with
OpenAI’s ChatGPT through its API to allow its users to ask
questions from its Bing search system and get answers with
references/sources mentioned. The integration of LLMs in to
search engines, thereby allowing users to get answers to human
like queries has spearheaded the search engine business in
to a new direction. Moreover, this addition of credibility is
an important consideration to enable use of ChatGPT and
similar LLMs in other critical tasks. While, at the time of
this manuscript, OpenAI still hasn’t rolled out plugin de-
velopment access to all developers, there have been several
notable use cases that have already come out. For example,

twelve companies have been listed on the OpenAI website 3,
namely, Expedia, FiscalNote, Instacart, KAYAK, Klarna, Milo,
OpenTable, Shopify, Slack, Speak, Wolfram, and Zapier to
have created the first plugins. The power that plugins provide
in terms of flexibility to develop new applications has been
a big drawer of interest towards plugin development. Apart
from the above-mentioned early developers, three plugins are
already made available by OpenAI. The first is the web-
browser plugin and the other is the code interpreter plugin. The
web browser plugin enables ChatGPT to access the internet
for information gathering which it can use to answer a query
given to it by the user. As mentioned before, this plugin allows
ChatGPT to circumvent the time limitation for its training data

   3https://openai.com/blog/ChatGPT-plugins
                                                                                                                                                                16

                                             TABLE VII: Comparison of Bard, ChatGPT, and Bing Chat
           Feature                     ChatGPT                                   Bard                                       Bing Chat
           Accuracy                    Not as accurate as Bard                   Generally more accurate than Chat-         Most accurate
                                                                                 GPT

           Versatile                   Generally more versatile than Bard        Can generate text, translate lan-          Not as versatile as ChatGPT or
                                                                                 guages, and write different kinds of       Bard
                                                                                 creative content

           Primary Purpose             Creative text generation                  Conversational AI                          Information retrieval

           Integration                 Standalone model                          Standalone model                           Integrated with Bing search engine

           Easy to use                 User-friendly                             User friendly                              Not as user-friendly as ChatGPT or
                                                                                                                            Bard

           Access to online data       No, trained on data available till        Yes                                        Yes
                                       2021

           Cost                        GPT 3.5 free / GPT-4 (20 USD per          Free                                       Free
                                       month)

           Availability                Publicly available                        Publicly available                         Publicly available

           Architecture                Generative                pre-trained     Pathways       Language        models      Next Generation GPT [117]
                                       transformer [62]                          (PaLM2) [78]

           Plagriasm detector          Yes                                       No                                         No

           Limitations                 May generate less coherent text           May generate incorrect responses           May provide limited or incomplete
                                                                                                                            information
                                                           You are an expert            by allowing it to use the latest information on the internet
                                  Vector                    Please answer               through the Bing Search API and a text-based web browser.
                                Database                      <question>                An example of using this API is shown in Fig. 12 where the
                                                            Using source                prompt Explain the GPT4 architecture has been used.
                                                        documents <docs>                    The Code interpreter is a built-in Python code interpreter
                                                                                        which can be used for performing logical calculations as well
                                                                                        as writing code. The interpreter can use the language model’s
                               Document                           Prompt                understanding of a human language description of a problem
                               Fragment                           Wrapper               and use that as input to develop Python code for the problem’s
                                 Search                                                 solution.
                                                                   GPT                      A third knowledge-based retrieval plugin has also been
                                                                                        open-sourced4 which can be used by developers as need be.
                                                                                        This plugin can be used to enable ChatGPT to access data
                          (a) Question & AnsweringDB                                    and then use it to gather useful or relevant information from
                                                                                        the data. These can be files, emails, notes etc. All this by
                        Idea,                  GPT                                      using queries or questions in normal human language. Once
                       Article                                          Candidate       deployed and registered with OpenAI, this plugin can make
                                             Python                       Output        use of OpenAI embeddings along with a selection of databases
                                               Domain                                   for indexing or searching through documents.
                                              Knowledge                                     Lastly, third-party plugins are also an option. These can
                           (b) Creative Applications                                    be created and have been created by several entities. Fig.
                                                                                        14 demonstrate the use of two third-party plugins, namely
                                                                                        ShowMe which can be used to generate diagrams and Schol-
                         GPT                           GPT                              arAI can be used to access academic journals.
                                                                        Result              Table VIII provides a list of plugins available for ChatGPT
                     Generate steps '                                                   which can be utilized, it should be mentioned that this list
     Build comoany   puild,company         ~Steds > Perform each stepOutput final step  is not exhaustive and more and more plugins are being
                                                                                        developed, especially, third party to perform tasks specific to
                            (c) Multi-step Planning                                     the developer.
Fig. 10: Templates for LLM-based application development.
GPT is taken as an example scenario representing LLMs.                                     4https://github.com/openai/ChatGPT-retrieval-plugin
                                                                                                                                                                             17

                                                                           TABLE VIII: Some ChatGPT Plugins
                             Name                       Task                                      Example use cases
                             Language Translation       Translate between languages               This is particularly useful for business, travel, medical science, ed-
                                                                                                  ucation and law where documents and information from different
                                                                                                  languages might need to be translated and students can use it to learn
                                                                                                  new languages
                             Sentiment Analysis         Determine tone of text or conver-         This can be used for the task of market research, customer analysis
                                                        sation                                    and social media monitoring
                             Spell Checker              Check and correct spelling mis-           This service can be useful for formal and informal communication
                                                        takes                                     such as emails, word processing and also browsing the web
                             Question-Answering         Answer questions for a user query         This can find use in education to build learning platforms, search
                                                                                                  engines, especially when a more ’understandable’ response is required
                                                                                                  and also be used in automated customer service agents
                             Knowledge Graph            Find and present information from         Knowledge graphs can be used for improving on search queries (i.e.
                                                        a database                                search engines), integrating data sources better and of course creating
                                                                                                  recommendations.
                             Speech Recognition         Understand and transcribe speech          This service can be used in audio based customer service, transcription
                                                        audio                                     services through dictation and also provide services to differently abled
                                                                                                  people through audio
                             Emotion Detection          Detect emotion from text or audio         This service can be used for applications relating to market research
                                                                                                  using verbal ques, interaction in vehicles to improve safety, used for
                                                                                                  healthcare as well as assessing reactions to games and other media

       VII. GUIDELINES FOR EFFECTIVE USE OF LARGE
                            LANGUAGE MODELS

    In this section, we will provide a list of steps to make best
used of LLMS, as well as guidelines which intended to ensure
the responsible development and use of LLMs.
    By following these steps, we can effectively use LLMs
to perform NLP tasks and improve the performance of our
applications and systems [119].
    • Identify the task: Determine what task you want the
       LLM to perform. LLMs can be used for a wide range of
       NLP tasks, such as text classification, sentiment analysis,
       question answering, and text generation [120], [121],
       [122].
    • Choose the right model: Choose a pre-trained LLM that
       is suitable for your task. There are several pre-trained
       LLMs available, such as GPT-3, BERT, and RoBERTa.
       Each model has different strengths and weaknesses, so
       it’s important to choose the one that best fits your needs
       [76].
    • Fine-tune the model:Fine-tune the pre-trained model on
       your specific task. This involves training the model on
       your own dataset to adapt it to your specific task. Fine-
       tuning involves adjusting the model’s parameters, such
       as learning rate, batch size, and number of epochs, to
       optimize its performance on your task [123].
    • Evaluate the model: Evaluate the performance of the
       model on a test dataset. This involves measuring the
       accuracy, precision, recall, and F1 score of the model
       on the test dataset. This step is important to ensure that
       the model is performing well on your task and to identify
       any areas for improvement [124].
    • Deploy the model: Deploy the model in your application
       or system. This involves integrating the model into your
       application or system and exposing it through an API
       or user interface. This step also involves setting up
       monitoring and logging to track the performance of the
       model in production [125].
    • Monitor and retrain the model: Monitor the perfor-
       mance of the model in production and retrain it as needed.
       This involves regularly checking the performance of the
       model and identifying any areas for improvement. If the
       performance of the model degrades over time, it may be
       necessary to retrain the model on new data or adjust its
       parameters [126].
   • Continuously improve the model: Continuously im-
       prove the model by incorporating user feedback and up-
       dating it with new data. This involves collecting feedback
       from users and incorporating it into the model to improve
       its performance. It also involves regularly updating the
       model with new data to keep it up-to-date and relevant
       [127].
   Moreover, the following guidelines will help to ensure the
responsible development and use of LLMs focusing on user
privacy, bias mitigation, ethical considerations, transparency,
competition, collaboration, and environmental impact [128].
By adhering to these guidelines, LLMs can be used to pos-
itively impact society while minimizing potential negative
consequences.
   • Protect User Privacy: LLMs should uphold user privacy
       and protect user data. This includes safeguarding user-
       generated content, such as emails, messages, and personal
       information. Best practices should be followed, such as
       data minimization, anonymization, and encryption, to
       ensure user privacy is not compromised [129].
   • Mitigate Bias: LLMs can inherit and amplify biases
       present in the data they are trained on. Developers and
       researchers should actively identify and mitigate bias in
       their models. This can be achieved through diverse and
       inclusive training data, bias detection techniques, and
       evaluation metrics [130].
   • Address Ethical Implications: LLMs have the potential
       to be used for harmful purposes, such as spreading disin-
       formation or generating deepfakes. Ethical considerations
       should be taken into account, and models should be
       developed in a way that benefits society. This includes
       ensuring accountability, transparency, and responsibility
       in the development and deployment of models [131].
                                                                                                                                                                                                            18
  Generate    code snippet [of    Neural Metwork-     identify between Cats ana Dogs using                 Explain the GPT-4 Architecture
  Pytorch
  Sure; TIl provide you  simple example for building            network using Py Torch: This
  code wll be basic implementation andwon't include data augmentation,advanced
  optimization techniques etc_Ilalso assumethatyou have           dataset ready for use, with                   Cllckeon;
  umane?of cats and dogs properly labeled:
  The structure ol the directones shouldbe something like this:
                                                                                          Copy codc        Generative Pre-trained Transformer     (GPT 4)is the fourth model     OpenAl' s numbered
                                                                                                           "GPT-n" series =  GPT foundation modelsAs       transformer-based mcdel; Ilke its
     databet                                                                                               prececessons    GPT - was pre-trained to predictthe next token       sequence using botn
              tJIn                                                                                         Publicdata and data licensed from third-party providers          Ihen fine-tnediith
                                                                                                           relnforcement learning Irom human and       leedback for human alignment and policy
                                                                                                           compiiance
                                  catoo2. jPQ
                                                                                                           GPT-4 unlike      predecessons can take images as  e       text as input: OpenAlnas not
                                                                                                           Fevealedspecific technical information such      the size cfthe GPT-4 model However it is
                                                                                                           reported tnat GPT-4 improves upon the prevrous versicn; GPT-3.5 despite =retaining some
                                                                                                           the same problems
                                                                                                           OpenAl stated that GPT-4      more reliable  create anqaple       handie much more nuanceo
                         cats                                                                              Instructions than GPT-3.5, Tne, producedtovers Jns          GPT-4, with ccntext wndows
                                  catO01.jPI                                                               3,192 and 32,768 tokens    significant improvement over GPT 3.5and GPT 3 which were
                                  cat00?. JPI                                                                        4096 and 2,049tokens respectively This multimoda capacilty'of GPT-4 gives
                                                                                                           Uimited
                        dons
                                  dogO01.JPg                                                             Fig. 12: Demonstration of the WebBrowsing plugin.
                                  dog00z
                                                                                                                   Please denerate   detalled Cianramor GPI 3 Achitecture
  Here     simple CHN architecture for this problem:
    uhar
                                                                                                                   Here i tne detailod ciagram lcr GPT-3 Architecturo:
     Imnort torch
                  inport        OpLim
    tom torch             data   Trcort  Dataloader
                          impori
       Veting        tanstorations; TeSlzina           converting        LonsoI          MDImalz
                                                                            324))
                                             transfons
                                                                                  4852     456.
                                                                        std [0.229       224,
       Load                                                                                                        Youcin edm thisdiagram onlineilycuwantto makeany changes
                                Dragel older( datasetin                                                                  lagram Illu strates the architectureGPT-- Genefative Fretfained Mranslormer
     test datn      datasets ImageFolder( datasetatest             ttansforetansform)                                            based on tha Transtormer medel; which censiste ofan Enccder anda
                                                                                                                   Decoder
       Create data loaders
     tJI    Joader     Wataloaderttain       data   batch_                                                         Inthe Encoderineattention mechanksm      divided into Self-AttentionandMulti-Head Selfs
           Joade                                                     chuffleeTrue)                                 Attenton: The Encoder also contains  Feed Fonxard Neural Network (FFNA
                           modcl   for tansfcr Icatning
    Rodel     Rodelg                                                                                          Fig. 13: Demonstration of the ShowMe plugin.
         paran       model parareterst)e
         patan'Iequites              False                                                           • Foster Transparency: It is crucial that the inner work-
       Replace aet aver                                                                                   ings of LLMs are transparent and explainable. This
                      Lincarieode              tcatutes
 Plezs oxolain tne code belci                                                                             can help build user trust and facilitate understanding of
     TowerOfHanoiln, from_rod,      rod, aux_rod):                                                        the model’s behavior. Explainability techniques, such as
   Mn =                                                                                                   attention mechanisms and model interpretation tools, can
     retutn
   print( Move dlsk ,E  Firom      Irom_rod; "to rod , to_rcd)srod)                                       be employed to provide insight into the decision-making
   lowerorhanoin- auX rad         fcd from_rod)                                                           process of models [101].
                                                                                                     • Promote Competition: The development and deploy-
 The ccde you've posted Is & Python implementation ofthe Tower of Hanci problem:                          ment of LLMs should not be monopolized by a small
                                                                                                          number of companies or individuals. This can limit in-
 The Torer    Hanoi is a classic problem that invclves moving    stack of disks frem one rodto            novation and negatively affect competition. Collaboration
 anoinelThe rules of the problem are that only one disk canbe moved ata time undatno
Pcint        Lrge  disk be placed                     disk                                                between academia, industry, and government can foster
 The function   TonarOrhano                     strategy to solve this problem:                           competition, while also promoting responsible develop-
    The base       for tne recursion             (tne numbe      dlsks)=      tnis case; tnere'$          ment and use of models [132].
                                       hen                                                           • Encourage                   Collaboration: Collaboration between re-
    nothing    do,so the function simply returns
    In Inarecursive case; the function first moves the top       dik? from the stanting rcd               searchers, developers, and industry should be encour-
            rod  ) to the auxiliary rod            using the destination rod                              aged to promote the responsible development and use of
    temporani holdina olce     This [cone Dy    recursive callto  Tovcrorhanoi
    The function tnen moves the remaining dlisk (which         argest) = tom    Startinarodio
    the destination rod_ This move   printed out forthe  userto 5p0
    Finally the functicn moves the        dlisks that were left onthe auxillary rcd to the
                                                                                                                                                                                19
         Please find me important papers on Large Language Models                                         to LLMs so as to get a desired response from them. Writing
                                                                                                          a prompt appropriately is there very important if one needs to
           UedSchalla                                                                                     use LLMs to assist with tasks in the best manner possible.
         Here      some important papers on Large Language Models:                                        While some formal techniques such as Explicit instruction
                                                                                                          (providing a clear direction to the LLM to do something),
             Pertormance of ChatGPLon USMLE: Potential for Al-assisted medical education using            System Specific Instruction (asking a question from the LLM
            Jarge language models by Tiffany H. Kung et al This paper evaluates the performance           to answer), Formatting with an example (providing a sample
              large language model called ChatGPT on the United States Medical Licensing Exam
            (USMLE) The model performed at or near the passing threshold for all three exams              question and its answer and asking the LLM to provide an
            without any specialized training or reinforcement PDF Link                                    answer in the same manner), Control tokens (use special
             Paraphrasingwith Large Language Models by Sam Witteveen and Martin =          Andrews        keywords in the prompt to help the LLM provide an answer
             This pape= presents    technique for using   llarge language model to perform the task of    while considering special provided criteria) and Interaction and
            paraphrasing on a variety oftexts and subjects PDFLink                                        iteration (interact with model iteratively to reach to a good
             Gcant4-    simulation toolkit by Stefano Agostinelli et al This paper discusses    toolkit
            for simulating the passage =   particles through matter:   has been used     applications In  answer by fine tuning on each reply) have been presented.
            particle physics; nuclear physics, accelerator design; space engineering;and medical          Here, we provide a some sets of commands to help users get
            physics PDE Link                                                                              the most of the LLMs capabilities.
             How Does ChatGPL Perform ontthe United States Medical Licensing_Examination? The
            Implications of Large Language Models for Medical Education and Knowledge                         • Define the role: This should be the first prompt for the
             Assessment by Aidan Gilson etal This study aimed        evaluate the perfarmance of                   LLM. An example of this prompt could be: “Act as
            ChatGPT on questions within the scope ofthe United States Medical Licensing                            a secretary to the Chair of the department”, “Act as a
            Examination Step and Step        exams PDE Link                                                        Lawyer” or “Act as my programming tutor for Python”.
             Enrching Word VectorsWith Subword Information by Piotr Bojanowski et al This paper
            proposes    new approach based on the skipgram model, where each word           represented            By defining a role for the LLM, one can direct it to
                 bag    character n-grams PDE Link                                                                 provide replies or do tasks as a human would do when
                                                                                                                   provided information to work on.
            Fig. 14: Demonstration of the ScholarAI plugin.                                                   • Prompt creation: Another interesting prompt command
                                                                                                                   is to provide ask the model to generate prompts for a
         LLMs. This includes open sourcing models and data, as                                                     certain task. This way, the LLM can be used to generate
         well as facilitating the sharing of research findings and                                                 optimized prompts for tasks that need to be done. An ex-
         best practices [133].                                                                                     ample of this could be: “ You are a large language model
                                                                                                                   and are an expert in generating prompts for ChatGPT.
     • Minimize Environmental Impact: Training LLMs can                                                            Please generate the best prompts on extracting important
         require significant computational resources and energy,                                                   information from my time series data”.
         which can have negative environmental impacts. De-                                                   • Other interesting directions in which Prompts can be
         velopers should strive to create more energy-efficient                                                    given are explanation prompts (e.g., “Explain like I am
         models and explore alternative training methods, such                                                     5 the concept of infinity”, Instructional Guides (e.g.,
         as model distillation or transfer learning, to reduce the                                                 “How do I tie my shoe laces”), Extract information (e.g.:
         environmental footprint of models [134], [135].                                                           one can paste a passage and ask the model to provide
     • Optimization is exploitation: is a statement that holds                                                     answers to questions that one might have), Solve Math
         particular significance in the context of LLMs and AI                                                     problems (e.g., “Find the roots for the quadratic equation,
         technologies [136]. While these technologies have the                                                     2x2 + 3x + 10 ”) and Code help (e.g., “Find the syntax
         potential to revolutionize the way we live and work, they                                                 error in the following code”).
         also have the potential to perpetuate existing inequalities                                          Other interesting aspects of prompting are Negative prompt-
         and introduce new forms of exploitation [137]. The                                               ing and Visual Prompting. Here, a brief discussion is provided
         development and deployment of LLMs often require sig-                                            on each of these types.
         nificant resources, such as data and computational power,                                            1) Negatuve Prompting: Negative prompting provides di-
         which may be controlled by a select few organizations or                                         rections to the LLM about aspects of the prompt that it
         countries, leading to further disparities in economic and                                        should avoid generating or deliberately excluding during the
         technological development [68]. Furthermore, the opti-                                           generation process. Through the use of negative prompts, one
         mization process for these models can introduce biases                                           can fine-tune the results generated by the LLM in response
         and reinforce existing inequalities, leading to the exploita-                                    to a prompt while being able to keep the prompt generation
         tion of individuals or groups who are negatively impacted                                        generic. Another advantage of the use of negative prompting
         by the model’s outputs. Therefore, it is important to                                            is that it allows for moderation of the output content generated
         carefully consider the ethical implications of optimization                                      by the model thereby preventing harmful or inappropriate from
         in the development and deployment of LLMs and AI                                                 being generated.
         technologies [138].                                                                                  2) Visual Prompting: Visual prompting refers to the use
                                                                                                          of visual prompts (such as images or non-visual ones such
A. Prompting                                                                                              as music) when providing directions to a model in addition
     LLMs have given rise to whats called “Prompt Engineer-                                               to plain text prompts. The aim is in this case to provide the
ing”. While there is a lack of a formal definition, prompt en-                                            AI model a starting point or an example/reference that it can
gineering refers to the designing and wording of prompts given                                            use for the generative task given. For images, this may be
                                                                     20

given to modify the image provided or generate something
that is similar in style, color, texture etc. This can help in
generating content that is closer to a user’s expectation from
the generative AI being used. An image-based example of
visual prompting could be providing a picture of an office and
asking the AI to generate a different theme for it, maybe more
nature-centric or in a different color or organizational style.
Visual prompting provides greater control of the generated
output and therefore results in a more accurate result. Using
the provided input image/video can provide generated outputs
that are more consistent with the intentions of the user input
prompt due to the additional reference input. It should be noted
that visual prompting is not related to images only, this is
currently being explored for a host of different applications,
including, text generation (generating something based on
a sample text so as to copy its style of writing for e.g.),
composition of music (wherein the supplied music piece can
be used as a reference for the type of music to compose),
game development (where a defined game environment may
be provided to the model as a starting point and the model
is asked to generate new and unique content) and virtual
and augmented reality (wherein a set of augmented/virtual
reality environments can be provided to further populate/create
current/new environments).

     VIII. DRAWBACKS OF LARGE LANGUAGE MODELS

   Although LLMs have made significant contributions to
NLP, they are not without limitations [30]. This section
highlights a number of these limitations, including biased data,
overreliance on surface-level patterns, limited common sense,
poor ability to reason and interpret feedback, the need for
vast amounts of data and computational resources, limited
generalizability, lack of interpretability, difficulty with rare or
out-of-vocabulary words, limited understanding of syntax and
grammar, limited domain-specific knowledge, susceptibility to
adversarial attacks, ethical concerns, difficulty with context-
dependent language, absence of emotion and sentiment anal-
ysis, limited multilingual capabilities, limited memory, lack
of creativity, restricted real-time capabilities, high costs of
training and maintenance, limited scalability, lack of causality,
inadequate ability to handle multimodal inputs, limited atten-
tion span, limited transfer learning capabilities, insufficient
understanding of the world beyond text, inadequate compre-
hension of human behavior and psychology, limited ability to
generate long-form text, restricted collaboration capabilities,
limited ability to handle ambiguity, inadequate understanding
of cultural differences, limited ability to learn incrementally,
limited ability to handle structured data, and limited ability to
handle noise or errors in input data [139], [140], [141], [142],
[143], [144], [25]. Therefore, it is essential for researchers and
practitioners to acknowledge and address these limitations to
ensure the ethical and effective use of LLMs and to develop
new models that can surpass these limitations.
   • Bias: Language models have the potential to unintention-
      ally demonstrate bias when the training data used in their
      development is biased. According to Schramowski et al.
      [145], large pre-trained models designed to mimic natural
    languages can inadvertently perpetuate unfairness and
    prejudices. Consequently, this can lead to discriminatory
    or inaccurate analyses and recommendations, resulting
    in public criticism across various domains, including
    politics, society, and law. The manifestations of these
    biases are as follows: (i) Training data bias: Language
    models typically rely on extensive datasets of human
    language for training. If these datasets contain biases
    related to factors such as race, gender, or socioeconomic
    status, the model may internalize and reproduce these
    biases in its responses. For example, if the training data
    exhibits a gender bias, the model may generate responses
    that favor a particular gender. (ii) User interaction bias:
    The responses generated by Chatbots are influenced by
    the input received from users. If users consistently pose
    biased or prejudiced questions, the model may learn and
    perpetuate these biases in its responses. Consequently, if
    users frequently ask discriminatory questions targeting a
    specific group, the model may generate responses that
    reinforce such biases. (iii) Algorithmic bias: Biases can
    also be introduced through the algorithms employed in
    training and operating language models and Chatbots. For
    instance, if the model is trained to optimize for a specific
    metric, such as accuracy or engagement, it may prioritize
    generating responses that align with that metric, even if
    those responses are biased in some way. (iv) Contextual
    bias: Chatbots generate responses based on the context
    provided by users. If the context contains bias associated
    with factors like the user’s location or language, the
    model may generate biased responses. For instance, if
    a user asks about a particular culture or religion and
    the model lacks training on that specific cultural or
    religious context, it may produce biased responses due
    to its limited knowledge.
• Information Hallucination: Sometimes, GPT-4 may
    generate information that is not based on its training data,
    leading to outputs that are factually incorrect or purely
    fictional. Hallucinations in LLMs are often the result
    of the model’s attempt to fill in gaps in knowledge or
    context, with assumptions that are based on the patterns
    it has learned during training. This can lead to incorrect or
    misleading outputs, which can be particularly problematic
    in sensitive applications.
    The cause of hallucinations in LLMs is an area of active
    research. Recent advances suggest that it’s a complex
    problem related to the model’s training process, dataset,
    and architectural design. In particular, LLMs might be
    biased towards producing more ”interesting” or fluent
    outputs, leading to a higher risk of hallucination [146].
    There have been several proposed methods to mitigate
    the issue of hallucinations. One approach is to modify
    the training process to explicitly penalize hallucinations,
    such as in the case of ”reality grounding” [147]. Another
    is to provide the model with a larger and more diverse
    dataset, which might reduce the risk of the model making
    incorrect assumptions.
    In addition, researchers are exploring the use of ”verifi-
    able” or ”fact-checkable” data during training, to teach
                                                                   21

   the model to rely more on facts and less on its own
   assumptions [148]. This, however, requires careful con-
   sideration of the data and metrics used.
   Moving forward, more research is needed to better under-
   stand and address hallucinations in LLMs. Some potential
   directions include the development of more sophisticated
   models that can better discern between factual informa-
   tion and assumptions, as well as novel training methods
   and datasets.
• LLMs Explainability: No one can explain a model
   containing 175 billion parameters: The advent of LLMs
   has ushered in unprecedented advancements in NLP
   tasks. However, the sheer complexity and scale of these
   models present challenges in terms of explainability
   [149], [150]. As LLMs continue to grow in size, with
   models containing billions of parameters, the ability to
   comprehensively explain their decision-making processes
   becomes increasingly elusive [151], [152]
   One of the primary limitations of LLM explainability is
   the sheer magnitude of their parameter count. Models
   such as GPT-3, with 175 billion parameters, possess
   an intricate web of interconnected nodes that contribute
   to the model’s functionality. This complexity makes it
   exceedingly difficult for humans to understand and inter-
   pret the decision-making mechanisms employed by the
   model [3]. The lack of transparency [153] hinders the
   ability to gain insights into how specific inputs lead to
   particular outputs [154]. Moreover, the training process
   of LLMs involves vast amounts of data, often collected
   from diverse sources. These models learn patterns and
   correlations within the data, leading to the emergence of
   implicit biases and associations that may not be readily
   apparent or interpretable. Consequently, when a decision
   is made by an LLM, it becomes challenging to dis-
   cern the underlying factors that influenced that decision,
   making it difficult to provide a clear and concise expla-
   nation.Additionally, the intricate architecture of LLMs,
   often consisting of deep neural networks, exacerbates the
   challenge of explainability [155]. The numerous layers
   and complex interactions make it challenging to trace the
   reasoning process of the model. While techniques such as
   attention mechanisms [44] can provide some insights into
   the model’s focus, they do not provide a comprehensive
   understanding of how the model arrives at its final output.
   Finally, the lack of explainability in LLMs raises concerns
   regarding accountability, trust, and ethical considerations
   [156]. In critical domains such as healthcare or finance,
   where decisions can have significant implications, it is
   crucial to have transparency and the ability to explain the
   reasoning behind the model’s predictions [153]. Without
   explainability, stakeholders may be reluctant to fully trust
   and adopt LLMs for sensitive applications.
• Reasoning Errors: GPT-4 can make mistakes in logical
   reasoning, either because of ambiguities in the prompt
   or inherent limitations in its understanding of complex
   logical operations.
• Struggles in Classes of Applications Such as Spelling
   Errors: Some specific tasks, like identifying and correct-
       ing spelling errors, can be challenging for GPT-4 due to
       its statistical nature.
   • Susceptible to Prompt Injection, ’Jail Break’ Attacks,
       Data Poisoning Attacks: GPT-4 is susceptible to various
       adversarial attacks. For instance, a malicious actor might
       inject misleading prompts, perform ’jailbreak’ attacks to
       make the model reveal sensitive information, or use data
       poisoning strategies to manipulate the model’s output.

 IX. IMPACT OF LARGE LANGUAGE MODELS ON SOCIETY
                              AND HUMANS
   Despite the aforementioned limitations, LLMs such as Ope-
nAI’s ChatGPT and Google’s Bard have gained popularity
for their ability to produce human-like responses to user
input [157]. However, the training process of these models
has significant environmental implications, particularly with
respect to the usage of water and energy [158]. This section
will also discuss the environmental impact of LLMs and
propose potential solutions to reduce their adverse effects,
thereby promoting their sustainable use.

A. Environmental
   New studies have revealed that the training process for
GPT-3 alone used up 185,000 gallons of water, equivalent
to what’s needed to fill a cooling tower of a nuclear reactor
[159]. This high consumption of water is primarily due to the
cooling process of data centers, which necessitates a massive
amount of water to regulate the servers’ optimal temperature.
Typically, freshwater sources are utilized to prevent corrosion
and bacterial growth that can occur with seawater, but this
limits the available water sources. Moreover, the development
of newer models, such as GPT-4 [59], is predicted to need
even more significant amounts of water due to their larger

data parameters. Apart from water usage, the training of LLMs
demands a considerable quantity of electricity. The training of
OpenAI’s GPT-3 alone resulted in the release of 502 metric
tons of carbon, which could provide energy to an average
American household for hundreds of years [160]. Furthermore,
the indirect water consumption of data centers located off-
site should also be taken into account since they necessitate a
substantial amount of electricity, leading to carbon emissions
[158].
   To lessen the harmful environmental effects of LLMs,
various remedies can be implemented. One such solution is for
data centers to adopt more eco-friendly cooling systems, such
as using recycled water or implementing advanced cooling
technologies [161]. Additionally, renewable energy sources,
such as solar or wind power, can be utilized to power data
centers, thereby reducing carbon emissions. Limiting the size
and intricacy of LLMs is another potential solution, as smaller
models require less data, resulting in reduced energy and water
consumption [158].

B. Sustainibility, Energy resources
   The development and deployment of AI tools to automate,
and enhance the business and user experience required a sig-
nificant amount of energy to train these systems, particularly
                                                                              22

deep learning models such as ChatGPT.The amount of energy
consumed by AI tools during training can be staggering,
with some estimates suggesting that it can take hundreds of
thousands or even millions of kWh to train a single large-
scale model like GPT-3 [162], [163]. This energy consumption
can have significant implications for power and energy usage,
as well as the environment. The energy consumption of AI
training can be attributed to several factors, including the
hardware used to run the training algorithms, the complexity
and size of the models being trained, and the amount of data
being processed.
   In order to train deep learning models like ChatGPT, spe-
cialized hardware such as GPUs are often used, which can
consume large amounts of energy due to their high processing
power and data transfer requirements. Furthermore, the size
and complexity of these models also contribute to their energy
consumption. The more parameters and layers a model has,
the more energy it will require to train, as each iteration
of the training algorithm requires the model to process large
amounts of data and make complex calculations to adjust the
weights and biases of the model. The energy consumption of
AI training has significant implications for the environment,
particularly in terms of greenhouse gas emissions and climate
change [164]. The energy required to train AI models is
often generated from fossil fuels, such as coal and natural
gas, which emit large amounts of carbon dioxide and other
greenhouse gases into the atmosphere. This can contribute
to global warming and other environmental impacts [165].
This issue highlights the need for responsible and sustainable
practices in AI development and deployment.
   To mitigate the environmental impact of AI training, several
approaches can be taken. One approach is to develop more
energy-efficient algorithms and models, which can reduce
the amount of energy required to train AI systems. Another
approach is to use renewable energy sources, such as solar or
wind power, to generate the energy required for AI training.
Additionally, there are efforts to develop more energy-efficient
hardware, such as neuromorphic computing, which can sig-
nificantly reduce the energy requirements of AI training. In
conclusion, the energy consumption of AI training, particularly
for deep learning models like ChatGPT, can have significant
implications for power and energy usage, as well as the
environment. As AI becomes more pervasive in our daily
lives, it is important to consider the energy requirements of
these systems and develop strategies to mitigate their impact
on the environment [166]. It is crucial to continue developing
AI technologies and leveraging their potential benefits while
being mindful of the environmental impact. By promoting
sustainable practices, investing in energy-efficient computing,
and exploring alternative training methods, we can work
towards a more sustainable integration of AI in society.

C. Sigularity
   Singularity refers to a point in the development of AI where

it becomes more intelligent than humans, thereby triggering
accelerated development of technology. With the increasing
popularity of LLMs for general use, especially after the claim5
by a google employee regarding Googles Chatbot being sen-
tient, the idea of artificial general intelligence and it surpassing
of equating human level intelligence has been a topic of serious
debate in the AI community. The widely used criteria for
determining if a machine has intelligence is the Turing test
[167]. The Turing test measures a machines capability to have
a conversation with a subject that is indistinguishable from
that of a human conversing in its place. If a machine is
judged to be indistinguishable, it is deemed to have passed the
Turing test and therefore demonstrated intelligence on par with
that of humans. While appearing deceivingly simple, the test
considers nuances in human behavior over a range of subjects
and contexts. LLMs as of the date of this publication, have
not yet passed the Turing test in all its forms and therefore
are not deemed to posses human level intelligence. Having said
that, there are two takes on AI reaching or surpassing human
level intelligence, a group which believes that the increasing
use of AI and it reaching human reaching intelligence will
free or greatly reduce the burden of labor for humans as well
as spearhead technological progress to help solve existential
problems to the human race such as Climate Change, Social
equity, food insecurity among others. The benefits are endless,
from the optimization of resources in every domain to making
new scientific discoveries and decreasing human bias and
error. For example; we, humans, solve a problem, optimize
it, and lock our model, and that solution becomes state-of-
the-art. Almost, everyone follows it. However, humans have
limited knowledge and computational power compared to
LLMs. LLMs can further explore the search space to find new
and more optimzed ways of solving a solution.
   However, there is another group of scientists who adhere
to a more pessimistic view towards such development in that
there is a fear that such a powerful system might at one point
become hostile to humans and become uncontrollable. Infact,
in March of 2023, notable personalities in tech published
an open letter6 requesting a pause in LLM AI development.
More recently, personnel from tech companies as well as
researchers have requested politicians to consider the risk of
human extinction due to AI as a top priority 7. In their letter,
they consider the major risks based on the potential to be
misused for societal disruption.
   Hyper disparity among developed and underdeveloped na-
tions. There are talks about the dark side of training LLMs,
there is a need of upper bound in terms of parameters. Need
for new directions to advance the quest to AGI. LLMs are
being trained on human input but how do we train humans
from such a large corpus? (God, Human, and AI)
   It is expected that the debate on singularity and also AI
regulation will continue in the foreseeable future, a balanced
approach needs to be applied with strong and effective AI
regulation where it aligns with benevolent human values.

   5https://www.scientificamerican.com/article/google-engineer-claims-ai-cha
tbot-is-sentient-why-that-matters/
   6https://futureoflife.org/open-letter/pause-giant-ai-experiments/
   7https://www.safe.ai/statement-on-ai-risk
                                                                  23

D. Competition among for-profit organizations
   OpenAI was founded around the premises of having a Large
scale AI company operating as not for profit organization.
However, with the evolution of scale and nature of investment
required, soon it converted to a for profit organization which
almost eliminated the freedom of access of large scale AI to
the masses. However, startups like Hugging face are support-
ing the growth of AI through is massive open source campaign.
Being a for profit organization has some demerits such as
excessive control by the investors, decoupling of development
and issues rising from the general public.

                    X. EXPERTS POINT OF VIEW
A. AI Human Coexistence
   Recently, there have been suggestions made by some experts
in the field of AI to stop the advancement LLMs for six
months [168]. The reasoning behind this suggestion is that
there are concerns about the ethical implications of LLMs
and their potential negative impact on society, including issues

such as privacy, human extinction, job market, bias, and the
concentration of power in the hands of a few large tech
companies.
   Several prominent figures in the field of AI, including
Timnit Gebru, an AI ethics researcher who was fired by
Google in late 2020, and Yoshua Bengio, a prominent AI
researcher and computer scientist, have expressed their support
for this suggestion. They argue that the six-month pause would
allow for more comprehensive and nuanced discussions about
the ethical implications of LLMs, and give researchers the
opportunity to develop more responsible approaches. More
recently, Geoffrey Hinton, a renowned AI expert known as the
”Godfather of AI,” made an announcement confirming that he
had resigned from his position at Google. The reason behind
his resignation was to raise awareness about the potential
”dangers” associated with AI, a technology that he helped to
develop. Elon Musk, the CEO of Tesla and SpaceX, has been
vocal about his concerns regarding the dangers of AI. Musk
believes that AI poses an existential threat to humanity if it
is not developed responsibly. He has called for regulation of
AI development to ensure that it is aligned with human values
and does not become a threat to our safety.
   However, not all experts agree with this suggestion [169].
Some argue that the potential benefits of LLMs, such as their
ability to facilitate communication and information access
across languages, outweigh the potential risks. They also point
out that a pause in research and development could put some
organizations at a disadvantage, as they may fall behind in
the race to develop new and innovative LLM technologies.
It is unlikely that LLMs research and development will be
completely stopped for six months, as it would require a co-
ordinated effort from multiple organizations and governments.
However, it is possible that some organizations may choose to
slow down their work on LLMs in response to these concerns.

B. Consideration of a 6-Month Suspension: Is it Necessary?
   In terms of the positive and negative sides of stopping
LLM development for six months, there are arguments on
both sides. On the positive side, a pause could allow for
more thorough ethical considerations and the development
of more responsible approaches to LLM development and
deployment [170]. This could potentially mitigate some of
the negative impacts of LLMs, such as bias, singularity, and
privacy concerns. On the negative side, a pause could slow
down progress in areas where LLMs could have significant
benefits, such as healthcare, education, and communication
[171]. Additionally, it is possible that some organizations may
choose to continue their work on LLMs in secret, which could
lead to even less accountability and oversight.
   Overall, it is important to consider the potential risks and
benefits of LLMs and to develop responsible approaches to
their development and deployment. While a complete halt
to LLM development may not be feasible or desirable, a
pause or slowdown could provide an opportunity for more
comprehensive ethical discussions and more responsible de-
velopment practices. As AI continues to advance and become
more complex, it is crucial to heed the concerns of experts
like Hinton and examine the possible ethical implications and
risks associated with this technology. To ensure that AI is
used and developed in a responsible manner, we need to take
appropriate measures that prioritize the safety and well-being
of individuals and society as a whole. This includes developing
and implementing robust ethical frameworks and guidelines
that can govern the use of AI and prevent its misuse. Despite
the potential risks, it is essential not to overlook the many
benefits of AI. AI has the potential to improve various aspects
of our lives, from healthcare to transportation, and even to
address some of humanity’s most pressing challenges, such
as climate change, pandemics, another asteroid and poverty.
Therefore, it is crucial to strike a balance between harnessing
the power of AI while also being mindful of its potential risks
and drawbacks. This can be achieved by working towards
the responsible development and use of AI, and fostering
a collaborative effort among stakeholders, including experts,
policymakers, and the public, to ensure that AI is harnessed
for the greater good. Perhaps, an FDA like regulation for
Large Language Models, beyond GPT-4 can be one potential
solution [172].


C. Open Questions
   1) Ethical Considerations: Inadvertently, LLMs may per-
petuate biases inherent in the training data, resulting in out-
puts that are biased or discriminatory. The challenge lies
in identifying and mitigating such biases to ensure fair and
equitable treatment across diverse user groups. To achieve
this, it is crucial to explore what are the methods that can
effectively address bias in the training data and enhance the
fairness of LLMs. Additionally, LLMs possess the capacity to
generate and disseminate misinformation or harmful content,
raising concerns about the accuracy and reliability of their
outputs. How can we ensure that LLMs prioritize accurate and
reliable information? Ensuring that LLMs prioritize accurate
and reliable information necessitates the implementation of
mechanisms that can effectively assess and prioritize the
authenticity and trustworthiness of the generated content. Such
                                                                    24

mechanisms should aim to detect and prevent the spread of
false or harmful information, safeguarding the integrity of
the generated outputs. Furthermore, LLMs often rely on vast
amounts of data for effective training. In order to protect user
privacy and prevent the compromise or misuse of personal
and sensitive information, it becomes essential to implement
measures that prioritize data protection. Incorporating robust
consent mechanisms, data anonymization techniques, and data
retention policies into the development and deployment of
LLMs can help ensure the responsible and ethical handling
of user data.
   2) Humans VS LLMs: Human interactions offer a deep
level of empathy, emotional intelligence, and the ability to
understand complex nuances in communication. Humans pos-
sess the capability to provide personalized responses, adapt to
individual needs, and bring a human touch to conversations.
They can understand context, interpret ambiguous queries, and
provide creative and flexible solutions. Human interactions
are valuable in scenarios where empathy, creativity, critical
thinking, and subjective judgment are crucial, such as therapy
sessions, customer service, and artistic collaborations.
   On the other hand, chatbots powered by AI have their advan-
tages. They can operate 24/7, handle large volumes of inquiries
simultaneously, and provide quick and consistent responses.
Chatbots excel in scenarios where efficiency, scalability, and
rapid information retrieval are essential. They can assist with
routine tasks, answer common questions, and provide instant
access to information. AI-driven chatbots continuously learn
and improve from user interactions, allowing them to become
more accurate and efficient over time.
   Chatbots are becoming increasingly autonomous. They are
now able to make their own decisions and to take actions
without human input. This is why it is crucial to assess the
potentially pose a threat to human safety.
   3) Interpretability: How can we enhance the interpretabil-
ity of LLMs? Despite their impressive capabilities, LLMs
often lack transparency, making it difficult to understand
their decision-making process. Finding ways to interpret and
explain their outputs is a significant challenge. Enhancing
the interpretability of LLMs holds importance for several
reasons. It fosters trust and transparency by enabling users to
understand the reasoning behind a model’s specific response.
It aids in identifying and addressing potential biases, errors,
or unethical behavior exhibited by the model. Additionally,
interpretability contributes to debugging and improving model
performance. However, achieving interpretability in LLMs is
challenging due to their complexity and the nature of their
training processes. LLMs have millions or even billions of
parameters, making it difficult to directly trace their decision-
making process. Furthermore, LLMs are trained using deep
learning techniques, such as transformer architectures, which
are considered black-box models, providing limited insight
into their internal workings. Addressing the interpretability
challenge in LLMs remains an active area of research. The
ultimate goal is to make LLMs more transparent and account-
able while preserving their impressive capabilities.
   4) Data Efficiency: Data efficiency refers to the efficient
use of training data for developing LLMs. As previously
mentioned, LLMs are typically trained using extremely large
amounts of data to gain a performance that is acceptable or
”human-like”. Developing techniques to achieve this will be
an open area of research as it will potentially enable better
or similar performance with less data thereby reducing en-
vironmental impact. Making LLM development data efficient
would allow for targeted development of LLM systems, and
reduce turnaround time by easing off data collection and
labeling burden. Several techniques which are being explored
are transfer learning, meta-learning etc.
   5) Training data contamination from AI-generated content:
Generative AI models are trained to produce outputs that
are human-like for the particular application area they are
designed for. Through the training process, the model learns to
generate an output that might be similar in style, organization
and tone to what a human produces. Moreover, data sources
for such models are typically scraped from the internet. With
the increasing popularity of generative AI and the output of
such content on the internet. A future challenge that might
potentially come up regarding this is that the data present
on the internet will have a significant enough component
generated by AI models and therefore, reduce the human
creativity aspect of the training data. Models, if trained on
such data might end up trying to copy the generation aspects
of previous AI models rather than humans only. One solution
to this could be to use AI detection engines that can determine
content generated by AI before passing it through the model
during the training process. However, sufficient work needs to
be carried out to ensure that there is a dependable mechanism
to perform this task and retain the integrity of data.

D. Recommendations

   In this Section, we provide recommendations for achieving
optimal performance and highlight some of the practical
applications.
   1) Recommendations for Optimal Performance and Achiev-
ing Your Goals: LLMs, such as GPT-4, have proven to be sig-
nificantly useful in numerous tasks due to their vast knowledge
and learning capabilities. However, there are several strategies
one must adopt to achieve optimal performance.
   • Use Advanced architecture: At present, GPT-4 is one
      of the most advanced language models available. It’s im-
      pressive ability to generate highly relevant and coherent
      content makes it a preferred choice for most of the tasks.
   • Use Prompts with Detailed Task Context and Relevant
      Information: LLM’s performance is largely determined
      by the specificity and clarity of the input prompt. Detailed
      task contexts and relevant information help the model
      understand the task at hand better, leading to more
      accurate responses.
   • Retrieve and Add Any Relevant Information to the
      Prompt: Additional information, when included in the
      prompts, helps the model deliver more specific and
      focused responses. If the user’s task involves specific
      knowledge, such as coding or medical information, pro-
      viding relevant data and instructions in the prompt can
      improve the model’s output.
                                                                    25

   • Experiment with Prompt Engineering Techniques:
      Given the complex and non-deterministic nature of
      LLM’s behavior, trying out various prompt engineering
      strategies can lead to significant performance improve-
      ments. Techniques such as providing more explicit in-
      structions, using leading questions, ”double-quoting key-
      word”, or presenting information in different formats may
      help achieve better results.
   2) Applications: Large Language has vast potential for
practical applications, particularly when combined with human
oversight and judgement.
   • Use in Low Stakes Applications, Combine with Hu-
      man Oversight: LLMs are best suited for low stakes
      applications, where errors or inaccuracies can be toler-
      ated. Moreover, combining LLMs with human oversight
      can significantly mitigate the risk of errors, biases, and
      other issues.
   • Source of Inspiration, Suggestions: LLMs can serve
      as an invaluable source of inspiration and suggestions,
      helping users brainstorm ideas, create content, and make
      decisions.
   • Copilots Over Autonomous Agents: Given its limita-
      tions, LLMs are better suited as a ’copilot’ that provides
      assistance and suggestions, rather than an autonomous
      agent that acts without human input or oversight.
   3) Democratizing AI: Democratizing AI [173] is a crucial
movement that seeks to make artificial intelligence accessible
and inclusive for a wide range of individuals and organizations.
By breaking down barriers and providing user-friendly tools,
democratization empowers diverse communities to leverage
the power of AI to solve problems and drive innovation. It
emphasizes the importance of open data, transparency, and
accountability, ensuring that AI systems are unbiased, under-
standable, and ethically grounded. Through democratization,
we can harness the transformative potential of AI for the
benefit of all, promoting a more inclusive and equitable future

                           XI. CONCLUSION
   In this survey, we provided a comprehensive exploration
of LLMs, their implications, technical concepts, and practical
learning and usage. We discussed the potential benefits and
risks of LLMs, and explored the different ways in which
they can be used. We also provided a number of examples
of how LLMs are being used in practice. By delving into the
technical intricacies, effective utilization, and future potential
of LLMs, the survey will contribute to a deeper understanding
and usage of these models within the research community.
The survey has shed light on the key elements that drive the
success of large language models through an examination of
their working principles, diverse architectures and comparison
between chatbots, guidelines for prompting, AI-enabled tools
and plug-ins, optimal strategies for employing LLMs, as well
as advancements in pre-training, fine-tuning, and capability
evaluation.
   Furthermore, the survey has highlighted the importance of
safe and ethical use of AI tools like ChatGPT. It recognizes
the need for developing guidelines and regulations to address
concerns related to security, ethics, the economy, and the
environment. Ensuring the responsible integration of LLMs
in healthcare, academia, and industries is critical, as it en-
ables these tools to effectively support and enhance human
endeavors while upholding the values of integrity, privacy, and
fairness.
   As the field of LLMs continues to evolve and progress,
future research and development efforts should focus on
improving the accuracy and performance of these models,
addressing their limitations, and exploring new ways to use
them. By adopting the guidelines presented in this survey,
researchers and practitioners can contribute to the ongoing
advancement of LLMs and ensure that they are used in a
responsible and beneficial way.

                      AUTHOR CONTRIBUTIONS

   Abbas Shah: Methodology, software, writing—original
draft preparation, project administration, validation, visual-
ization, software formal analysis. Anas Zafar: Software
formal analysis, methodology, visualization, conceptualiza-
tion, software, writing—original draft preparation, validation.
Muhammad Bilal Shaikh: Project administration, methodol-
ogy, software, writing—original draft preparation, visualiza-
tion, validation, software formal analysis. Amgad Muneer:
Project administration, Software validation, writing—original
draft preparation, formal analysis, methodology, validation,
investigation. Muhammad Irfan: Methodology, investigation,
writing—original draft preparation, visualization, validation,
software formal analysis. Qasem Al-Tashi: Methodology,
conceptualization, software, writing—original draft prepara-
tion, visualization, investigation, formal analysis. Rizwan
Qureshi: Conceptualization, methodology, project administra-
tion, software, writing—original draft preparation, visualiza-
tion, formal analysis. Muhammad Usman Hadi: Conceptual-
ization, project administration, methodology, writing—original
draft preparation, formal analysis, visualization, funding acqui-
sition. Seyedali Mirjalili: Supervision, writing—review and
editing. Jia Wu: Supervision, writing—review and editing.
All authors have read and agreed to the published version of
the manuscript.

                    DECLARATION OF INTEREST

   The authors have no conflicts of interest to declare. All
co-authors have seen and agree with the contents of the
manuscript.

   DECLARATION OF GENERATIVE AI AND AI-ASSISTED
           TECHNOLOGIES IN THE WRITING PROCESS

   The authors have used generative artificial intelligence (AI)
and AI-assisted technologies in the writing process and survey
preparation. The authors used these technologies to draw
figures, analyze data, improve readability, writing code and
language. Authors are ultimately responsible and accountable
for the contents of this work.
                                                                                                                                          26

                                    REFERENCES
 [1] S. Pinker and A. Morey, “The language instinct: How the mind creates
       language (unabridged edition),” Brilliance Audio, 2014.
 [2] M. D. Hauser, N. Chomsky, and W. T. Fitch, “The faculty of language:
       what is it, who has it, and how did it evolve?,” science, vol. 298,
       no. 5598, pp. 1569–1579, 2002.
 [3] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
       B. Zhang, J. Zhang, Z. Dong, et al., “A survey of large language
       models,” arXiv preprint arXiv:2303.18223, 2023.
 [4] I. Turing, “Computing machinery and intelligence-am turing,” Mind,
       vol. 59, no. 236, p. 433, 2007.
 [5] Y. K. Dwivedi, N. Kshetri, L. Hughes, E. L. Slade, A. Jeyaraj, A. K.
       Kar, A. M. Baabdullah, A. Koohang, V. Raghavan, M. Ahuja, et al.,
       ““so what if chatgpt wrote it?” multidisciplinary perspectives on op-
       portunities, challenges and implications of generative conversational ai
       for research, practice and policy,” International Journal of Information
       Management, vol. 71, p. 102642, 2023.
 [6] M. Du, F. He, N. Zou, D. Tao, and X. Hu, “Shortcut learning of large
       language models in natural language understanding: A survey,” arXiv
       preprint arXiv:2208.11857, 2022.
 [7] F. Jelinek, Statistical methods for speech recognition. MIT press, 1998.
 [8] J. Gao and C.-Y. Lin, “Introduction to the special issue on statistical
       language modeling,” 2004.
 [9] R. Rosenfeld, “Two decades of statistical language modeling: Where do
       we go from here?,” Proceedings of the IEEE, vol. 88, no. 8, pp. 1270–
       1278, 2000.
[10] A. Stolcke, “Srilm-an extensible language modeling toolkit,” in Seventh
       international conference on spoken language processing, 2002.
[11] Y. Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic
       language model,” Advances in neural information processing systems,
       vol. 13, 2000.
[12] S. Kombrink, T. Mikolov, M. Karafi´at, and L. Burget, “Recurrent
       neural network based language modeling in meeting recognition.,” in
       Interspeech, vol. 11, pp. 2877–2880, 2011.
[13] T. Mikolov, M. Karafi´at, L. Burget, J. Cernock`y, and S. Khudanpur,
       “Recurrent neural network based language model.,” in Interspeech,
       vol. 2, pp. 1045–1048, Makuhari, 2010.
[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
       Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
       Advances in neural information processing systems, vol. 30, 2017.
[15] M. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
       and L. Zettlemoyer, “Deep contextualized word representations. arxiv
       2018,” arXiv preprint arXiv:1802.05365, vol. 12, 2018.
[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
       of deep bidirectional transformers for language understanding,” arXiv
       preprint arXiv:1810.04805, 2018.
[17] M. Shanahan, “Talking about large language models,” arXiv preprint
       arXiv:2212.03551, 2022.
[18] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and
       D. Zhou, “Chain of thought prompting elicits reasoning in large
       language models,” arXiv preprint arXiv:2201.11903, 2022.
[19] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and X. Hu,
       “Harnessing the power of llms in practice: A survey on chatgpt and
       beyond,” arXiv preprint arXiv:2304.13712, 2023.
[20] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv,
       L. Cui, O. K. Mohammed, Q. Liu, et al., “Language is not all
       you need: Aligning perception with language models,” arXiv preprint
       arXiv:2302.14045, 2023.
[21] A. Koubaa, “Gpt-4 vs. gpt-3.5: A concise showdown,” 2023.
[22] Y. Du, Z. Liu, J. Li, and W. X. Zhao, “A survey of vision-language
       pre-trained models,” arXiv preprint arXiv:2202.10936 , year=2022.
[23] G. Mialon, R. Dess`ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,
       R. Raileanu, B. Rozi`ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-
       maz, et al., “Augmented language models: a survey,” arXiv preprint
       arXiv:2302.07842, 2023.
[24] R. Qureshi, M. Irfan, H. Ali, A. Khan, A. S. Nittala, S. Ali, A. Shah,
       T. M. Gondal, F. Sadak, Z. Shah, et al., “Artificial intelligence and
       biosensors in healthcare and its clinical relevance: A review,” IEEE
       Access, 2023.
[25] E. Kasneci, K. Seßler, S. K¨uchemann, M. Bannert, D. Dementieva,,
       F. Fischer, U. Gasser, G. Groh, S. G¨unnemann, E. H¨ullermeier, et al.
       “Chatgpt for good? on opportunities and challenges of large language
       models for education,” Learning and Individual Differences, vol. 103,
       p. 102274, 2023.
[26] M. Sallam, “The utility of chatgpt as an example of large languagemodels in healthcare education, research and practice: Systematic
        review on the future perspectives and potential limitations,” medRxiv,
        pp. 2023–02, 2023.
[27] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,
        P. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A large
        language model for finance,” arXiv preprint arXiv:2303.17564, 2023.
[28] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka-
        plan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al.,
        “Evaluating large language models trained on code,” arXiv preprint
        arXiv:2107.03374 , year=2021.
[29] T. Eloundou, S. Manning, P. Mishkin, and D. Rock, “Gpts are gpts:
        An early look at the labor market impact potential of large language
        models,” arXiv preprint arXiv:2303.10130 , year=2023.
[30] Z. Sun, “A short survey of viewing large language models in legal
        aspect,” arXiv preprint arXiv:2303.09136 , year=2023.
[31] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
        T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,
        A. Joulin, E. Grave, and G. Lample, “Llama: Open and efficient
        foundation language models,” 2023.
[32] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus,
        E. Li, X. Wang, M. Dehghani, S. Brahma, et al., “Scaling instruction-
        finetuned language models,” arXiv preprint arXiv:2210.11416, 2022.
[33] M. Suzgun, N. Scales, N. Sch¨arli, S. Gehrmann, Y. Tay, H. W. Chung,
        A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al., “Challenging
        big-bench tasks and whether chain-of-thought can solve them,” arXiv
        preprint arXiv:2210.09261, 2022.
[34] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
        H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri,
        G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan,
        S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian,
        C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis,
        E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,
        J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,
        A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Rad-
        ford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder,
        B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba,
        “Evaluating large language models trained on code,” 2021.
[35] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu,
        C. Lv, Y. Zhang, J. Lei, et al., “C-eval: A multi-level multi-
        discipline chinese evaluation suite for foundation models,” arXiv
        preprint arXiv:2305.08322, 2023.
[36] W. Chen and E. W. X. M. J. X. T. X. X. W. P. L. Ming Yin,
        Max Ku, “Theoremqa: A theorem-driven question answering dataset,”
        arXiv preprint arXiv:2305.12524, 2023.
[37] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz,
        E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al., “Sparks of
        artificial general intelligence: Early experiments with gpt-4,” arXiv
        preprint arXiv:2303.12712, 2023.
[38] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,
        S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., “Palm 2 technical
        report,” arXiv preprint arXiv:2305.10403, 2023.
[39] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi,
        “Codet5+: Open code large language models for code understanding
        and generation,” arXiv preprint arXiv:2305.07922, 2023.
[40] S. Barke, M. B. James, and N. Polikarpova, “Grounded copilot: How
        programmers interact with code-generating models,” Proceedings of the
        ACM on Programming Languages, vol. 7, no. OOPSLA1, pp. 85–111,
        2023.
[41] L. Mescheder, S. Nowozin, and A. Geiger, “Adversarial variational
        bayes: Unifying variational autoencoders and generative adversarial
        networks,” in International conference on machine learning, pp. 2391–
        2400, PMLR, 2017.
[42] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta,
        and A. A. Bharath, “Generative adversarial networks: An overview,”
        IEEE signal processing magazine, vol. 35, no. 1, pp. 53–65, 2018.
[43] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang, X. Jiang, Z. Yang,
        K. Wang, X. Zhang, et al., “Pangu-alpha: Large-scale autoregressive
        pretrained chinese language models with auto-parallel computation,”
        arXiv preprint arXiv:2104.12369, 2021.
[44] H. Zhao, J. Jia, and V. Koltun, “Exploring self-attention for image
        recognition,” in Proceedings of the IEEE/CVF conference on computer
        vision and pattern recognition, pp. 10076–10085, 2020.
[45] J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff, “Masked
        language model scoring,” arXiv preprint arXiv:1910.14659, 2019.
                                                                                    27

[46] A. Muneer and S. M. Fati, “A comparative analysis of machine learning
       techniques for cyberbullying detection on twitter,” Future Internet,
       vol. 12, no. 11, p. 187, 2020.
[47] B. D. Lund, T. Wang, N. R. Mannuru, B. Nie, S. Shimray, and Z. Wang,
       “Chatgpt and a new academic reality: Artificial intelligence-written
       research papers and the ethics of the large language models in scholarly
       publishing,” Journal of the Association for Information Science and
       Technology, vol. 74, no. 5, pp. 570–581, 2023.
[48] E. D. Liddy, “Natural language processing,” 2001.
[49] X. Liu and W. B. Croft, “Statistical language modeling,” Annual Review
       of Information Science and Technology, vol. 39, p. 1, 2004.
[50] B.-H. Juang and L. R. Rabiner, “Automatic speech recognition–a brief
       history of the technology development,” Georgia Institute of Technol-
       ogy. Atlanta Rutgers University and the University of California. Santa
       Barbara, vol. 1, p. 67, 2005.
[51] P. Azunre, Transfer learning for natural language processing. Simon
       and Schuster, 2021.
[52] A. Kovaˇcevi´c and D. Keˇco, “Bidirectional lstm networks for abstractive
       text summarization,” in Advanced Technologies, Systems, and Applica-
       tions VI: Proceedings of the International Symposium on Innovative
       and Interdisciplinary Applications of Advanced Technologies (IAT)
       2021, pp. 281–293, Springer, 2022.
[53] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey,
       M. Krikun, Y. Cao, Q. Gao, K. Macherey, et al., “Google’s neural
       machine translation system: Bridging the gap between human and
       machine translation,” arXiv preprint arXiv:1609.08144, 2016.
[54] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi,
       P. Cistac, T. Rault, R. Louf, M. Funtowicz, et al., “Transformers:
       State-of-the-art natural language processing,” in Proceedings of the
       2020 conference on empirical methods in natural language processing:
       system demonstrations, pp. 38–45, 2020.
[55] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., “Improv-
       ing language understanding by generative pre-training,” 2018.
[56] N. A. Akbar, I. Darmayanti, S. M. Fati, and A. Muneer, “Deep learning
       of a pre-trained language model’s joke classifier using gpt-2,” Journal
       of Hunan University Natural Sciences, vol. 48, no. 8, 2021.
[57] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
       A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language mod-
       els are few-shot learners,” Advances in neural information processing
       systems, vol. 33, pp. 1877–1901, 2020.
[58] L. Floridi and M. Chiriatti, “Gpt-3: Its nature, scope, limits, and
       consequences,” Minds and Machines, vol. 30, pp. 681–694, 2020.
[59]
[60] A. Karpathy, “State of GPT.” https://www.youtube.com/watch?v=bZ
       Qun8Y4L2A, 2023.
[61] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien,
       E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al.,

       “Pythia: A suite for analyzing large language models across training
       and scaling,” arXiv preprint arXiv:2304.01373 , year=2023.
[62] M. R. Chavez, T. S. Butler, P. Rekawek, H. Heo, and W. L. Kinzler,
       “Chat generative pre-trained transformer: why we should embrace this
       technology,” American Journal of Obstetrics and Gynecology, 2023.
[63] H. Hassani and E. S. Silva, “The role of chatgpt in data science: how
       ai-assisted conversational interfaces are revolutionizing the field,” Big
       data and cognitive computing, vol. 7, no. 2, p. 62, 2023.
[64] S. Praveen and V. Vajrobol, “Understanding the perceptions of health-
       care researchers regarding chatgpt: a study based on bidirectional
       encoder representation from transformers (bert) sentiment analysis and
       topic modeling,” Annals of Biomedical Engineering, pp. 1–3, 2023.
[65] W. Zhao, H. Hu, W. Zhou, J. Shi, and H. Li, “Best: Bert pre-training for
       sign language recognition with coupling tokenization,” arXiv preprint
       arXiv:2302.05075 , year=2023.
[66] L. Jiarong, X. Hong, J. Wenchao, Y. Jianren, and W. Tao, “Knowledge
       enhanced bert based on corpus associate generation,” in Machine
       Learning for Cyber Security: 4th International Conference, ML4CS
       2022, Guangzhou, China, December 2–4, 2022, Proceedings, Part III,
       pp. 533–547, Springer, 2023.
[67] M. Irfan, A. I. Sanka, Z. Ullah, and R. C. Cheung, “Reconfigurable
       content-addressable memory (CAM) on FPGAs: A tutorial and survey,”
       Future Generation Computer Systems, vol. 128, pp. 451–465, 2022.
[68] L. Fan, L. Li, Z. Ma, S. Lee, H. Yu, and L. Hemphill, “A bibliometric
       review of large language models research from 2017 to 2023,” arXiv
       preprint arXiv:2304.02020, 2023.
[69] J. Su, S. Yu, and D. Luo, “Enhancing aspect-based sentiment analysis
       with capsule network,” IEEE Access, vol. 8, pp. 100551–100561, 2020.
[70] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and
        Q. V. Le, “Xlnet: Generalized autoregressive pretraining for language
        understanding,” Advances in neural information processing systems,
        vol. 32, 2019.
[71] A. Qamar, F. B. Muslim, F. Gregoretti, L. Lavagno, and M. T.
        Lazarescu, “High-level synthesis for semi-global matching: Is the juice
        worth the squeeze?,” IEEE Access, vol. 5, pp. 8419–8432, 2016.
[72] W. Ahmad, B. Ayrancioglu, and I. Hamzaoglu, “Low error efficient
        approximate adders for fpgas,” IEEE Access, vol. 9, pp. 117232–
        117243, 2021.
[73] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
        Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learn-
        ing with a unified text-to-text transformer,” The Journal of Machine
        Learning Research, vol. 21, no. 1, pp. 5485–5551, 2020.
[74] N. S. Keskar, B. McCann, L. R. Varshney, C. Xiong, and R. Socher,
        “Ctrl: A conditional transformer language model for controllable gen-
        eration,” arXiv preprint arXiv:1909.05858, 2019.
[75] P. Li, M. Zhang, P. Lin, J. Wan, and M. Jiang, “Conditional embedding
        pre-training language model for image captioning,” Neural Processing
        Letters, vol. 54, no. 6, pp. 4987–5003, 2022.
[76] I. Dergaa, K. Chamari, P. Zmijewski, and H. B. Saad, “From human
        writing to artificial intelligence generated text: examining the prospects
        and potential threats of chatgpt in academic writing,” Biology of Sport,
        vol. 40, no. 2, pp. 615–622, 2023.
[77] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,
        E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark,
        et al., “Training compute-optimal large language models,” arXiv
        preprint arXiv:2203.15556, 2022.
[78] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
        P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., “Palm: Scaling
        language modeling with pathways,” arXiv preprint arXiv:2204.02311
        , year=2022.
[79] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,
        J. Aslanides, S. Henderson, R. Ring, S. Young, et al., “Scaling language
        models: Methods, analysis & insights from training gopher,” arXiv
        preprint arXiv:2112.11446 , year=2021.
[80] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-
        Showk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume, et al.,
        “Scaling laws and interpretability of learning from repeated data,” arXiv
        preprint arXiv:2205.10487 , year=2022.
[81] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang,
        “Quantifying memorization across neural language models,” arXiv
        preprint arXiv:2202.07646, 2022.
[82] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon,
        C. Elepa˜no, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo,
        et al., “Performance of chatgpt on usmle: Potential for ai-assisted
        medical education using large language models,” PLoS digital health,
        vol. 2, no. 2, p. e0000198, 2023.
[83] M. Sallam, “Chatgpt utility in healthcare education, research, and
        practice: systematic review on the promising perspectives and valid
        concerns,” in Healthcare, vol. 11, p. 887, MDPI, 2023.
[84] A. Gilson, C. W. Safranek, T. Huang, V. Socrates, L. Chi, R. A. Taylor,
        D. Chartash, et al., “How does chatgpt perform on the united states
        medical licensing examination? the implications of large language mod-
        els for medical education and knowledge assessment,” JMIR Medical
        Education, vol. 9, no. 1, p. e45312, 2023.
[85] A. Rao, J. Kim, M. Kamineni, M. Pang, W. Lie, and M. D. Succi,
        “Evaluating chatgpt as an adjunct for radiologic decision-making,”
        medRxiv, pp. 2023–02, 2023.
[86] D. Duong and B. D. Solomon, “Analysis of large-language model
        versus human performance for genetics questions,” medRxiv, pp. 2023–
        01, 2023.
[87] N. Fijaˇcko, L. Gosak, G. ˇStiglic, C. T. Picard, and M. J. Douma, “Can
        chatgpt pass the life support exams without entering the american heart
        association course?,” Resuscitation, vol. 185, 2023.
[88] R. S. D’Amico, T. G. White, H. A. Shah, and D. J. Langer, “I asked
        a chatgpt to write an editorial about how we can incorporate chatbots
        into neurosurgical research and patient care. . . ,” 2022.
[89] A. Holzinger, K. Keiblinger, P. Holub, K. Zatloukal, and H. M¨uller,
        “Ai for life: Trends in artificial intelligence for biotechnology,” New
        Biotechnology, vol. 74, pp. 16–24, 2023.
[90] M. R. Haque and S. Rubya, “An overview of chatbot-based mobile
        mental health apps: Insights from app description and user reviews,”
        JMIR mHealth and uHealth, vol. 11, no. 1, p. e44838, 2023.
[91] S. A. Basit, R. Qureshi, S. Musleh, R. Guler, M. S. Rahman, K. H.
        Biswas, and T. Alam, “Covid-19base v3: Update of the knowledgebase
                                                                                                                            28

           for drugs and biomedical entities linked to covid-19,” Frontiers in
           Public Health, vol. 11, p. 1125917, 2023.
  [92] mbzuai oryx, “Xraygpt: Chest radiographs summarization using med-
           ical vision-language models,” 2023.
  [93] J. S. () and W. Y. (), “Unlocking the power of chatgpt: A framework
           for applying generative ai in education,” ECNU Review of Education,
           vol. 0, no. 0, p. 20965311231168423, 0.
  [94] “An era of chatgpt as a significant futuristic support tool: A study
           on features, abilities, and challenges,” BenchCouncil Transactions on
           Benchmarks, Standards and Evaluations, vol. 2, no. 4, p. 100089, 2022.
  [95] H. Crompton and D. Burke, “Artificial intelligence in higher education:
           the state of the field,” International Journal of Educational Technology
           in Higher Education, vol. 20, no. 1, p. 22, 2023.
  [96] E. Kasneci, K. Sessler, S. K¨uchemann, M. Bannert, D. Dementieva,
           F. Fischer, U. Gasser, G. Groh, S. G¨unnemann, E. H¨ullermeier, S. Kr-
           usche, G. Kutyniok, T. Michaeli, C. Nerdel, J. Pfeffer, O. Poquet,
           M. Sailer, A. Schmidt, T. Seidel, M. Stadler, J. Weller, J. Kuhn,
           and G. Kasneci, “Chatgpt for good? on opportunities and challenges
           of large language models for education,” Learning and Individual
           Differences, vol. 103, p. 102274, 2023.
  [97] “Khan academy explores the potential for gpt-4 in a limited pilot
           program,” 2023.
  [98] “Harnessing gpt-4 so that all students benefit. a nonprofit approach for
           equal access,” 2023.
  [99] E. Hannan and S. Liu, “Ai: new source of competitiveness in higher ed-
           ucation,” Competitiveness Review: An International Business Journal,
           vol. 33, no. 2, pp. 265–279, 2023.
[100] Z. Lin, Z. Song, Z. Dai, and Q. V. Le, “Fingpt: Open-source financial
           large language models,” arXiv preprint arXiv:2306.06031, 2023.
[101] M. Fraiwan and N. Khasawneh, “A review of chatgpt applications in
           education, marketing, software engineering, and healthcare: Benefits,
           drawbacks, and research directions,” arXiv preprint arXiv:2305.00237,
           2023.
[102] D. Tiro, “The possibility of applying chatgpt (ai) for calculations
           in mechanical engineering,” in New Technologies, Development and
           Application VI: Volume 1, pp. 313–320, Springer, 2023.
[103] Y. Wardat, M. A. Tashtoush, R. AlAli, and A. M. Jarrah, “Chatgpt:
           A revolutionary tool for teaching and learning mathematics,” Eurasia
           Journal of Mathematics, Science and Technology Education, vol. 19,
           no. 7, p. em2286, 2023.
[104] S. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz,
           P. C. Petersen, A. Chevalier, and J. Berner, “Mathematical capabilities
[105] X. Wang, N. Anwer, Y. Dai, and A. Liu, “Chatgpt of chatgpt,” arXiv preprint arXiv:2301.13867 , year=2023.for design,

           manufacturing, and education,” 2023.
[106] S. Badini, S. Regondi, E. Frontoni, and R. Pugliese, “Assessing the
           capabilities of chatgpt to improve additive manufacturing troubleshoot-
           ing,” Advanced Industrial and Engineering Polymer Research, 2023.
[107] E. Adamopoulou and L. Moussiades, “Chatbots: History, technol-
           ogy, and applications,” Machine Learning with Applications, vol. 2,
           p. 100006, 2020.
[108] J. Rudolph, S. Tan, and S. Tan, “War of the chatbots: Bard, bing chat,
           chatgpt, ernie and beyond. the new ai gold rush and its impact on
           higher education,” Journal of Applied Learning and Teaching, vol. 6,
           no. 1, 2023.
[109] T. Reddy, R. Williams, and C. Breazeal, “Text classification for ai
           education.,” in SIGCSE, p. 1381, 2021.
[110] J. Pachouly, S. Ahirrao, K. Kotecha, G. Selvachandran, and A. Abra-
           ham, “A systematic literature review on software defect prediction
           using artificial intelligence: Datasets, data validation methods, ap-
           proaches, and tools,” Engineering Applications of Artificial Intelli-
           gence, vol. 111, p. 104773, 2022.
[111] K. Nguyen-Trung, A. K. Saeri, and S. Kaufman, “Applying chatgpt
           and ai-powered tools to accelerate evidence reviews,” 2023.
[112] N. Gleason, “Chatgpt and the rise of ai writers: How should higher
           education respond?,” Times Higher Education, 2022.
[113] G. Cooper, “Examining science education in chatgpt: An exploratory
           study of generative artificial intelligence,” Journal of Science Education
           and Technology, vol. 32, pp. 444–452, 2023.
[114] L. Skavronskaya, A. H. Hadinejad, and D. Cotterell, “Reversing the
           threat of artificial intelligence to opportunity: a discussion of chatgpt in
           tourism education,” Journal of Teaching in Travel & Tourism, vol. 23,
           no. 2, pp. 253–258, 2023.
[115] B. Yetis¸tiren, I. Ozsoy, M. Ayerdem, and E. T¨¨                                  uz¨un, “Evaluating the
           code quality of ai-assisted code generation tools: An empirical study
           on github copilot, amazon codewhisperer, and chatgpt,” arXiv preprint
           arXiv:2304.10778, 2023.
[116] A. M. Dakhel, V. Majdinasab, A. Nikanjam, F. Khomh, M. C. Des-
           marais, and Z. M. J. Jiang, “Github copilot ai pair programmer: Asset
           or liability?,” Journal of Systems and Software, vol. 203, p. 111734,
           2023.
[117] T. Teubner, C. M. Flath, C. Weinhardt, W. van der Aalst, and O. Hinz,
           “Welcome to the era of chatgpt et al. the prospects of large language
           models,” Business & Information Systems Engineering, pp. 1–7, 2023.
[118] C. Xu, Y. Xu, S. Wang, Y. Liu, C. Zhu, and J. McAuley, “Small
           models are valuable plug-ins for large language models,” arXiv preprint
           arXiv:2305.08848, 2023.
[119] J. Zhang, R. Xie, Y. Hou, W. X. Zhao, L. Lin, and J.-R. Wen, “Rec-
           ommendation as instruction following: A large language model em-
           powered recommendation approach,” arXiv preprint arXiv:2305.07001,
           2023.
[120] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li,
           M. He, Z. Liu, et al., “Summary of chatgpt/gpt-4 research and per-
           spective towards the future of large language models,” arXiv preprint
           arXiv:2304.01852, 2023.
[121] X. He, X. Shen, Z. Chen, M. Backes, and Y. Zhang, “Mgtbench:
           Benchmarking machine-generated text detection,” arXiv preprint
           arXiv:2303.14822, 2023.
[122] M. T. I. Khondaker, A. Waheed, E. M. B. Nagoudi, and M. Abdul-
           Mageed, “Gptaraeval: A comprehensive evaluation of chatgpt on arabic
           nlp,” arXiv preprint arXiv:2305.14976, 2023.
[123] J. Kim, J. H. Lee, S. Kim, J. Park, K. M. Yoo, S. J. Kwon, and D. Lee,
           “Memory-efficient fine-tuning of compressed large language models
           via sub-4-bit integer quantization,” arXiv preprint arXiv:2305.14152,
           2023.

[124] S. Arora, B. Yang, S. Eyuboglu, A. Narayan, A. Hojel, I. Trummer,
           and C. R´e, “Language models enable simple systems for generat-
           ing structured views of heterogeneous data lakes,” arXiv preprint
           arXiv:2304.09433, 2023.
[125] S. R. Bowman, “Eight things to know about large language models,”
           arXiv preprint arXiv:2304.00612, 2023.
[126] M. J. Ali, “Chatgpt and lacrimal drainage disorders: performance
           and scope of improvement,” Ophthalmic Plastic and Reconstructive
           Surgery, vol. 39, no. 3, p. 221, 2023.
[127] Y. Fu, H. Peng, T. Khot, and M. Lapata, “Improving language model
           negotiation with self-play and in-context learning from ai feedback,”
           arXiv preprint arXiv:2305.10142, 2023.
[128] P. P. Ray, “Chatgpt: A comprehensive review on background, appli-
           cations, key challenges, bias, ethics, limitations and future scope,”
           Internet of Things and Cyber-Physical Systems, 2023.
[129] H. Matsumi, D. Hallinan, D. Dimitrova, E. Kosta, and P. De Hert, Data
           Protection and Privacy, Volume 15: In Transitional Times. Bloomsbury
           Publishing, 2023.
[130] P. Hacker, A. Engel, and M. Mauer, “Regulating chatgpt and other
           large generative ai models,” arXiv preprint arXiv:2302.02337, 2023.
[131] S. A. Khowaja, P. Khuwaja, and K. Dev, “Chatgpt needs spade
           (sustainability, privacy, digital divide, and ethics) evaluation: A review,”
           arXiv preprint arXiv:2305.03123, 2023.
[132] A. Chan, H. Bradley, and N. Rajkumar, “Reclaiming the digital
           commons: A public data trust for training data,” arXiv preprint
           arXiv:2303.09001, 2023.
[133] W. H. Deng, B. Guo, A. Devrio, H. Shen, M. Eslami, and K. Hol-
           stein, “Understanding practices, challenges, and opportunities for user-
           engaged algorithm auditing in industry practice,” in Proceedings of
           the 2023 CHI Conference on Human Factors in Computing Systems,
           pp. 1–18, 2023.
[134] M. Kraus, J. A. Bingler, M. Leippold, T. Schimanski, C. C.
           Senni, D. Stammbach, S. A. Vaghefi, and N. Webersinke, “Enhanc-
           ing large language models with climate resources,” arXiv preprint
           arXiv:2304.00116, 2023.
[135] E. Agathokleous, C. J. Saitanis, C. Fang, and Z. Yu, “Use of chatgpt:
           What does it mean for biology and environmental science?,” Science
           of The Total Environment, p. 164154, 2023.

[136] Y. Shen, L. Heacock, J. Elias, K. D. Hentel, B. Reig, G. Shih, and
           L. Moy, “Chatgpt and other large language models are double-edged
           swords,” 2023.
[137] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang,
           and C. Gan, “Principle-driven self-alignment of language mod-
           els from scratch with minimal human supervision,” arXiv preprint
           arXiv:2305.03047, 2023.
[138] E. Ferrara, “Should chatgpt be biased? challenges and risks of bias in
           large language models,” arXiv preprint arXiv:2304.03738, 2023.
                                                                                                                                                                                            29

[139] Y. Wolf, N. Wies, Y. Levine, and A. Shashua, “Fundamental lim-                                   [161] A. S. George, A. H. George, and A. G. Martin, “The environmental
         itations of alignment in large language models,” arXiv preprint                                        impact of ai: A case study of water consumption by chat gpt,” Partners
         arXiv:2304.11082, 2023.                                                                                Universal International Innovation Journal, vol. 1, no. 2, pp. 97–104,
[140] R. Tang, Y.-N. Chuang, and X. Hu, “The science of detecting llm-                                          2023.
         generated texts,” arXiv preprint arXiv:2303.07205, 2023.                                      [162] S. Biswas, “Potential use of chat gpt in global warming,” Ann Biomed
[141] F. Ufuk, “The role and limitations of large language models such                                          Eng, vol. 51, pp. 1126–1127, 2023.
         as chatgpt in clinical settings and medical journalism,” Radiology,                           [163] Z. Yao, Y. Lum, A. Johnston, and et al., “Machine learning for a
         vol. 307, no. 3, p. e230276, 2023.                                                                     sustainable energy future,” Nat Rev Mater, vol. 8, pp. 202–215, 2023.
[142] R. Bhayana, S. Krishna, and R. R. Bleakney, “Performance of chatgpt                              [164] X. Zhi and J. Wang, “Editorial: Ai-based prediction of high-impact
         on a radiology board-style examination: Insights into current strengths                                weather and climate extremes under global warming: A perspective
         and limitations,” Radiology, p. 230582, 2023.                                                          from the large-scale circulations and teleconnections,” Frontiers in
[143] C.-H. Chiang and H.-y. Lee, “Can large language models be an                                              Earth Science, vol. 11, 2023.
         alternative to human evaluations?,” arXiv preprint arXiv:2305.01937,                          [165] J. Zhong, Y. Zhong, M. Han, T. Yang, and Q. Zhang, “The impact of ai
         2023.                                                                                                  on carbon emissions: evidence from 66 countries,” Applied Economics,
[144] X. Yang, Y. Li, X. Zhang, H. Chen, and W. Cheng, “Exploring the                                           vol. 0, no. 0, pp. 1–15, 2023.
         limits of chatgpt for query or aspect-based text summarization,” arXiv                        [166] M. A. Habila, M. Ouladsmane, and Z. A. Alothman, “Chapter 21
         preprint arXiv:2302.08081, 2023.                                                                       - role of artificial intelligence in environmental sustainability,” in
[145] P. Schramowski, C. Turan, N. Andersen, C. A. Rothkopf, and K. Ker-                                        Visualization Techniques for Climate Change with Machine Learning
         sting, “Large pre-trained language models contain human-like biases                                    and Artificial Intelligence (A. Srivastav, A. Dubey, A. Kumar, S. Kumar
         of what is right and wrong to do,” Nature Machine Intelligence, vol. 4,                                Narang, and M. Ali Khan, eds.), pp. 449–469, Elsevier, 2023.
         no. 3, pp. 258–268, 2022.                                                                     [167] A. M. Turing, “Computing machinery and intelligence,” Mind,
[146] T. McCoy, E. Pavlick, and T. Linzen, “Right for the wrong reasons:                                        vol. LIX, pp. 433–460, 1950.
         Diagnosing syntactic heuristics in natural language inference,” in                            [168] F. O. Letters, “Pause giant ai experiments: An open letter,” Future
         Proceedings of the 57th Annual Meeting of the Association for Compu-                                   of Life Institution. https://futureoflife. org/open-letter/pause-giant-ai-
         tational Linguistics, (Florence, Italy), pp. 3428–3448, Association for                                experiments, 2023.
         Computational Linguistics, July 2019.                                                         [169] M. Ienca, “Don’t pause giant ai for the wrong reasons,” Nature Machine
[147] J. Weston, E. Dinan, and A. Miller, “Retrieve and refine: Improved                                        Intelligence, pp. 1–2, 2023.
         sequence generation models for dialogue,” in Proceedings of the                               [170] B. Lin, D. Bouneffouf, G. Cecchi, and K. R. Varshney, “Towards
         2018 EMNLP Workshop SCAI: The 2nd International Workshop on                                            healthy ai: Large language models need therapists too,” arXiv preprint
         Search-Oriented Conversational AI, (Brussels, Belgium), pp. 87–92,                                     arXiv:2304.00416 , year=2023.
         Association for Computational Linguistics, Oct. 2018.                                         [171] S. Harrer, “Attention is not all you need: the complicated case of
[148] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “FEVER:                                      ethically using large language models in healthcare and medicine,”
         a large-scale dataset for fact extraction and VERification,” in Pro-                                   EBioMedicine, vol. 90, 2023.
         ceedings of the 2018 Conference of the North American Chapter                                 [172] M. Elmahdy and R. Sebro, “A snapshot of artificial intelligence
         of the Association for Computational Linguistics: Human Language                                       research 2019–2021: is it replacing or assisting physicians?,” Journal
         Technologies, Volume 1 (Long Papers), (New Orleans, Louisiana),                                        of the American Medical Informatics Association, p. ocad094, 2023.
         pp. 809–819, Association for Computational Linguistics, June 2018.                            [173] C. T. Wolf, “Democratizing ai? experience and accessibility in the age
[149] D. V. Hada and S. K. Shevade, “Rexplug: Explainable recommendation                                        of artificial intelligence,” XRDS: Crossroads, The ACM Magazine for
         using plug-and-play language model,” in Proceedings of the 44th                                        Students, vol. 26, no. 4, pp. 12–15, 2020.
         International ACM SIGIR Conference on Research and Development
         in Information Retrieval, pp. 81–91, 2021.
[150] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and J. Zhang, “Chat-
         rec: Towards interactive and explainable llms-augmented recommender
         system,” arXiv preprint arXiv:2303.14524 , year=2023.
[151] A. Uchendu, REVERSE TURING TEST IN THE AGE OF DEEPFAKE
         TEXTS. PhD thesis, The Pennsylvania State University, 2023.
[152] E. M. Bonsu and D. Baffour-Koduah, “From the consumers’ side: De-
         termining students’ perception and intention to use chatgpt in ghanaian
         higher education,” Journal of Education, Society & Multiculturalism,
         vol. 4, no. 1, pp. 1–29, 2023.
[153] Q. V. Liao and J. W. Vaughan, “AI Transparency in the Age of LLMs: A
         human-centered research roadmap,” arXiv preprint arXiv:2306.01941
         , year=2023.
[154] G. Vilone and L. Longo, “Notions of explainability and evaluation
         approaches for explainable artificial intelligence,” Information Fusion,
         vol. 76, pp. 89–106, 2021.
[155] N. M. Deshpande, S. Gite, B. Pradhan, and M. E. Assiri, “Explainable
         artificial intelligence–a new step towards the trust in medical diagnosis
         with ai frameworks: A review,” Comput. Model. Eng. Sci, vol. 133,
         pp. 1–30, 2022.
[156] D. Shin, “The effects of explainability and causability on perception,
         trust, and acceptance: Implications for explainable ai,” International
         Journal of Human-Computer Studies, vol. 146, p. 102551, 2021.
[157] M. S. Rahaman, M. T. Ahsan, N. Anjum, H. J. R. Terano, and M. M.Rahman, “From chatgpt-3 to gpt-4: A significant advancement in ai-
         driven nlp tools,” Journal of Engineering and Emerging Technologies,
         vol. 2, no. 1, pp. 1–11, 2023.
[158] M. C. Rillig, M. ˚Agerstrand, M. Bi, K. A. Gould, and U. Sauerland,
         “Risks and benefits of large language models for the environment,”
         Environmental Science & Technology, vol. 57, no. 9, pp. 3464–3466,
         2023.
[159] G. Fergusson, C. Fitzgerald, C. Frascella, M. Iorio, T. McBrien,
         C. Schroeder, B. Winters, and E. Zhou, “Contributions by,”
[160] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia,
         D. Rothchild, D. So, M. Texier, and J. Dean, “Carbon emissions and
         large neural network training,” 2021.
Transformers  Introduction to Large Language
and Large     Models
Language
Models
Language models

•   Remember the simple n-gram language model
   •   Assigns probabilities to sequences of words
   •   Generate text by sampling possible next words
   •   Is trained on counts computed from lots of text
•   Large language models are similar and different:
   •   Assigns probabilities to sequences of words
   •   Generate text by sampling possible next words
   •   Are trained by learning to guess the next word
 Neural Large Language Models (LLMs)

•  • Self-supervised learnersTake a text, remove a word
   •   Use your neural model to guess what the word was
   •   If the model is wrong, use stochastic gradient descent
       to make the model guess better next time
•  • Advantages (?): lot of text (GPT3: 500 billion tokens)All we need is a
   •   (And a lot of compute)
LLMs are built out of transformers
       Transformer: a specific kind of network architecture, like areproduce the tables and figures in this paper solely for use in journalistic or
                                                                                                                                                   Provided proper attribution is provided, Google hereby grants permission to
       fancier feedforward network, but based on attention[cs.CL] 2 Aug 2023scholarly works.
                                                       Attention Is All You Need

                                      Ashish Vaswani⇤        Noam Shazeer⇤        Niki Parmar⇤       Jakob Uszkoreit⇤
                                       Google Brain          Google Brain        Google Research     Google Research
                                  avaswani@google.com      noam@google.com     nikip@google.com      usz@google.com

                                        Llion Jones⇤           Aidan N. Gomez⇤ †              Łukasz Kaiser⇤
                                      Google Research         University of Toronto            Google Brain
                                    llion@google.com        aidan@cs.toronto.edu        lukaszkaiser@google.com

                                                                   Illia Polosukhin⇤ ‡
                                                            illia.polosukhin@gmail.com
 A very approximate timeline
1990 Static Word Embeddings
2003 Neural Language Model
2008 Multi-Task Learning
2015 Attention
2017 Transformer
2018 Contextual Word Embeddings and Pretraining
2019 Prompting
A picture of a transformer language model
           Output      long         and    thanks    for   all
        Softmax over
         Vocabulary                                             …
         Linear Layer

        Layers of
       Transformer
         Blocks

                                                                …

           Input                                                …
        Embeddings

                        So          long     and   thanks  for
Transformers  Introduction to Large Language
and Large     Models
Language
Models
Transformers  Attention
and Large
Language
Models
Instead of starting with the big pictureLet's consider the embeddings for an individual word from a particular layerOutputlong   and   thanks             for    all
                                    Softmax overOutput                                             long                          and  thanks              for   all
                                       Vocabulary                                                                                                                     …
                                   Softmax over
                                     Linear Layer
                                       Vocabulary                                                                                                                     …
                                     Linear Layer

                                     Layers of
                                Transformer
                                     Layers ofBlocks
                                Transformer
                                         Blocks                                                                                                                       …
                                                                                                                                                                      …
                                                Input                                                                                                                 …
                                   Embeddings
                                                Input                                                                                                                 …
                                   Embeddings                                                          So                       long         and  thanks       for
                                                                                                      So                        long         and  thanks       for
 Problem with static embeddings (word2vec)

They are static! The embedding for a word doesn't reflect how its
meaning changes in context.

The chicken didn't cross the street because it was too tired

What is the meaning represented in the static embedding for "it"?
 Contextual Embeddings

•   Intuition: a representation of meaning of a word
    should be different in different contexts!
•   Contextual Embedding: each word has a different
    vector that expresses different meanings
    depending on the surrounding words
•   How to compute contextual embeddings?
   •   Attention
 Contextual Embeddings

The   chicken    didn't    cross    the   street    because    it

What should be the properties of "it"?
The chicken didn't cross          the street because        it was too     tired
The chicken didn't cross          the street because        it was too     trafficy

At this point in the sentence, it's probably referring to either the chicken or the street
 Intuition of attention

Build up the contextual embedding from a word by
selectively integrating information from all the
neighboring words
We say that a word "attends to" some neighboring
words more than others
          Intuition of attention:

                     test
          Layer 6
TheTheself-attention distribution
animalanimal
didn’tdidn’tLayer 5
crosscross
thethe
 Attention definition

A mechanism for helping compute the embedding for
a token by selectively attending to and integrating
information from surrounding tokens (at the previous
layer).

More formally: a method for doing a weighted sum of
vectors.
 Attention is left-to-right

                     a1      a2  a3  a4  a5

Self-Attention
    Layer

                     x1      x2  x3  x4  x5
  ords to other words? Simplified version of attention: a sum of prior wordsVerson 1:                                        score(x, x j) =
                                                                                                                                             Since our representations fori          xi · x j                                                           (1
 e use of our old friend the weighted by their similarity with the current worddot product                                                                                                                 that we used
sult of a dot product is a scalar value ranging from
                                                     ty in Chapter 6, and also played a role in attention inGiven a sequence of token embeddings:                                                                 • to                 •, the la
  sult of this comparison between words
    e more similar the vectors that are being compared. Continuing with ox1         x2                    x3                   x4 3                 x5                    xi                                    i       and          j        as a
 he first step in computing
     quation to add attention to the computation of thisy                                                                                      would be to compute three scores:                                                                    x3
      x3 · x3Weighted by their similarity to xi. Then to make effective use of these scores, we’ll normalize t
                                                                                                              Produce: ai = a weighted sum of x1 through x5
RS AND ax to create a vector of weights, LARGE LANGUAGE MODELSai j, that indicates the proporti
of each input to the input element i i that is the current focus of attentio
    rson 1:                                             score(x, x j) = i                                                                             x           · x j                                                               (10.4)
ch weighted by its  ai j                                                     =Xexp(score(x, x j))softmax(score(x, x j)) a value.                                          i       8 j  i                                                               (1
 t is a scalar value ranging fromi                                                                                                                                                •                   to  •, the larger
  vectors that are being compared. Continuing with ourai                 = =                   Pi ai jx j                                                                                    8 j  i                         (10.7)                     (1
                                                                                            ji                             exp(score(x, xk)) i
         Intuition of attention:

                    test
         Layer 6

           self-attention distributionTheThe
animalanimal
didn’tdidn’tLayer 5
crosscross
thethex1 x2 x3 x4 x5 x6 x7                    xi
       Intuition of attention:                        query

         Layer 6
                    test

           self-attention distribution
TheThe
         Layer 5animalanimal
didn’tdidn’t
                                x1 x2 x3 x4 x5 x6 x7       xicrosscross
                                  valuesthethe
Attention
Actually it's slightly more complicated, but I won't get into that
High-level idea: instead of just having a query and a set of values,
each embedding actually also has a key
              Intuition of attention:                                                              query
                                                                                                   test
                        Layer 6

                              self-attention distribution
TheThe
                        Layer 5animalanimal
didn’tdidn’t
                                                           x1 x2 x3 x4 x5 x6 x7                                xicrosscrosskeysk
                                                                v                k
                                                                                 v  k
                                                                                    v  k
                                                                                       v  k
                                                                                          v  k
                                                                                             v  k
                                                                                                vthethevalues
 Summary
Attention is a method for enriching the representation of a token by
incorporating contextual information
The result: the embedding for each word will be different in different
contexts!
Contextual embeddings: a representation of word meaning in its
context.
Transformers  Attention
and Large
Language
Models
Transformers  The rest of the transformer
and Large     applied to language modeling
Language
Models
The transformer
Attention is just part of computing embeddings in a transformer.

Let's see more of the mechanism
Reminder: transformer language model
           Output      long         and   thanks    for   all
        Softmax over
         Vocabulary                                            …
         Linear Layer

        Layers of
       Transformer
         Blocks

                                                               …

           Input                                               …
        Embeddings

                        So          long    and   thanks  for
The residual stream: each token gets passed up and
modified
                        hi-1       hi                 hi+1

                                   +

                                         Feedforward
                                         Layer Norm
                …                                           …
                                    +
                                          MultiHead
                                          Attention
                                         Layer Norm

                        xi-1       xi                 xi+1
We'll need nonlinearities, so a feedforward layer
                       hi-1        hi                 hi+1

                                   +

                                         Feedforward
                                         Layer Norm
                …                                           …
                                   +
                                          MultiHead
                                          Attention
                                         Layer Norm

                       xi-1        xi                 xi+1
A transformer is a stack of these blocks
                                             hi-1  hi             hi+1

                                                   +

                                                       Feedforward

                                                       Layer Norm
                                 Block 2  …                             …
                                                   +
                                                        MultiHead
                                                        Attention

                                                       Layer Norm

                                             xi-1  xi             xi+1

                                             hi-1  hi             hi+1

                                                   +

                                                       Feedforward
                                                       Layer Norm
                                          …                             …
                                 Block 1           +    MultiHead
                                                        Attention
                                                       Layer Norm

                                             xi-1  xi             xi+1
Inputs

                                  Transformer Block

        X = Composite
         Embeddings
       (word + position)    +      +          +         +      +
                          Janet  will      back      the    bill
           Word
       Embeddings
             Position         1      2         3         4      5
          Embeddings              will
                          Janet              back      the    bill
             Language                modeling head


    Language Model Head
      takes hLN and outputs a
   distribution over vocabulary V

                   hL1          hL2
  Layer L
Transformer
   Block
                                        …
                   w1           w2
y1       y2     …        y|V|     Word probabilities    1 x |V|
                                  Softmax over vocabulary V
u1       u2     …        u|V|     Logits    1 x |V|
       Unembedding
          layer = ET              Unembedding layer       d x |V|
             hLN     1 x d




             wN
                                     Sample token to generate              wN+1
                                          at position N+1

The final transformer                Token probabilities             y1    y2    …      y|V|
                                                                      Language Model

model                                                                        Head
                                                                 h1 h2   …    hN

                                                                   Layer Norm
                                            Layer L                Feedforward
                                                                   Layer Norm
                                                             Multihead Self-Attention

                                                                      …
                                                                 h1 h2   …    hN

                                                                   Layer Norm
                                             Layer 2               Feedforward
                                                                   Layer Norm
                                                             Multihead Self-Attention

                                                                 h1 h2   …    hN

                                                                   Layer Norm
                                             Layer 1               Feedforward
                                                                   Layer Norm
                                                             Multihead Self-Attention

                                               X                 x1 x2   …     xN

                       Add token + position embeddings        E[w1] E[w2] … E[wN]      +     P1  P2  …  PN
                                          Input tokens          w1 w2    …     wN
Transformers  The rest of the transformer
and Large     applied to language modeling
Language
Models
          Pretraining (and how to train
Large     transformers for language
Language  modeling)
Models
 Pretraining

The big idea that underlies all the amazing
performance of language models

First pretrain a transformer model on enormous
amounts of text
Then apply it to new tasks.
 Intuition of language model training

We just train them to predict the next word!
1. Take a corpus of text
2. At each time step t
   i.   ask the model to predict the next word
   ii.  train the model using gradient descent to minimize the
        error in this prediction
 Intuition of language model training: loss

•   Same loss function: cross-entropy loss
   •   We want the model to assign a high probability to true
       word w
   •   = want loss to be high if the model assigns too low a
       probability to w
•   CE Loss: The negative log probability that the model
    assigns to the true next word w
   •   If the model assigns too low a probability to w
   •   We move the model weights in the direction that assigns a
       higher probability to w
Training a transformer language model

   Next word     long         and      thanks    for    all

     Loss                                                    …  =
 Softmax over
  Vocabulary
  Linear Layer

  Transformer
     Block                                                   …

    Input                                                    …
 Embeddings

                  So          long      and    thanks  for
  Pretraining Data: mostly from the web (Common
  Crawl)                                    Composition of the Pile by Category
The Pile:                                         Academic  InternetProse  Dialogue Misc
                                                                                            Bibliotik
                                                      Pile-CC                               PG-19         BC2
              PubMed Central           ArXiv
                                                                                                       Subtitles
                                                                             StackExchange             IRC   EP
                                           PMA                                              Github
              FreeLaw           USPTO      Phil  NIH  OpenWeb Text2          Wikipedia      DM Math    HN    YT
                                        Figure 1: Treemap of Pile components by effective size.
 What does a model learn from pretraining?

•   There are canines everywhere! One dog in the
    front room, and two dogs
•   It wasn't just big it was enormous
•   The author of "A Room of One's Own" is Virginia
    Woolf
•   The doctor told me that he
•   The square root of 4 is 2
 Big idea

Text contains enormous amounts of knowledge
Pretraining on lots of text with all that
knowledge is what gives language models their
ability to do so much
          Pretraining (and how to train
Large     transformers for language
Language  modeling)
Models
          Large Language Models:
Large     Applying pretrained models to
Language  new tasks
Models
 Big idea

Many tasks can be turned into tasks of
 predicting words!
                                                           Encoder-                             •       Good parts of decoders and encodeEncoders
          Three architectures for large language modelsPretraining for three types of architec
                                                           Decoders                             •       What’s the best way to pretrain the
                                                        The neural architecture influences the type of pretra
                                                                                                •       Language models! What we’ve seen•
                                                           Decoders                             •       Nice to generate from; can’t conditEncoders  •
32
        Decoders                                                    Encoders                                             Encoder-decoders
        GPT, Claude,                                                BERT family, Encoder-Flan-T5, Whisper•
        Llama 2                                                     HuBERT                                        Decoders                           •
                                                                                                                   32
        Mixtral




 Gets bidirectionEncoder-
 How do we traiDecoders


Good parts of dDecoders
What’s the bes
 Decoders                                     •
                                    Decoders  •
Also called:
•  Causal LLMs                  32
•  Autoregressive LLMs
•  Left-to-right LLMs

•  Predict words left to right
Conditional Generation:                     Generating text
conditioned on previous text!                                      Completion Text

                                                              all     the

                                      Sample from Softmax

                                                linear layer

           Transformer
             Blocks
                                                                                 …

              Input
          Embeddings

                       So      long      and       thanks     for      all      the

                                     Prefix Text
   t
dings                             Framing lots of tasks as conditional generation
                             So                                       long                                and         thanks               for            all  the
                               Sentiment analysis: “I like Jackie Chan”
                               1.                   We give the language model this string:"IPrefix Text
  utoregressive text completion with transformer-based large language models.The    sentiment                                of  the            sentence
                                                    like                                    Jackie             Chan"                  is:
 word “negative” to see which is higher:2.          And see what word it thinks comes next:
                                   P(positive|The sentiment of the sentence “I like Jackie Chan” is:)
                                 P(negative|The sentiment of the sentence “I like Jackie Chan” is:)

 If the word “positive” is more probable, we say the sentiment of the sentenc
  e word “positive” is more probable, we say the sentiment of the sente
                                                                       If the word “positive” is more probable, we say the sentiment of the sente
  itive, otherwise we say the sentiment is negative.
                                                    positive, otherwise we say the sentiment is negative.Framing lots of tasks as conditional generation
   We can also cast more complex tasks as word prediction.We can also cast more complex tasks as word prediction.                                                                           Consider th
                                                                                                                                                                                              Consider t
        of answering simple questions, a task we return to in Chapter 14. In this ta
 nswering simple questions, a task we return to in Chapter 14. In this ta
        system is given some question and must give a textual answer. We can cast thQA: “Who wrote The Origin of Species”
 tem is given some question and must give a textual answer. We can cast t
        of question answering as word prediction by giving a language model a questio1. We give the language model this string:
 uestion answering as word prediction by giving a language model a questi
        a token like A: suggesting that an answer should come next:
ken like Q: Who wrote the book ‘‘The Origin of Species"? A: suggesting that an answer should come next:                                                                                                   A:
        If we ask a language model to compute
                                             Q: Who wrote the book ‘‘The Origin of Species"?                                                                                                                  A:
  e ask a language model to compute2.              And see what word it thinks comes next:
                                                         P(w|Q: Who wrote the book “The Origin of Species”? A:)
        and look at which words P(w|Q: Who wrote the book “The Origin of Species”? A:)w                                                     have high probabilities, we might expect to se
        Charles is very likely, and then if we choose Charles and continue and ask
  look at which words                                                                                               w           have high probabilities, we might expect to s
                                        The neural architecture influences
 Encoders
                                                                Encoders
Many varieties!
•  Popular: Masked Language Models (MLMs)
•  BERT family                                                 Encoder-
•  Trained by predicting words from surroundingDecoders
   words on both sides
•  Are usually fine-tuned (trained on supervised data)
   for classification tasks.                                   Decoders
 Encoder-Decoders                                         En
                                                          D

•   Trained to map from one sequence to another
•   Very popular for:                                     D
   •  machine translation (map from one language to
      another)
   •  speech recognition (map from acoustics to words)32
 Many more things I didn't talk about
Instruction Fine-tuning
Preference Alignment
Prompt Engineering

Where to learn these? CS224N!
          Large Language Models:
Large     Applying pretrained models to
Language  new tasks
Models
          Harms of Large Language
Large     Models
Language
Models
                                           ChatbotsMay HHallucinate'
      Hallucination                     More Often Than Many Realize
                What Can You Do WhenAI Lies
                About You?
               People have little protection or recourse when the technology
                creates and spreads falsehoods about them:
 Air Canada loses court case after its chatbot hallucinated
fake policies to a customer
 The airline argued that the chatbot itself was liable: The court disagreed.
 Current research direction to address hallucination

Retrieval-Augmented Generation (RAG)

Use information retrieval to retrieve some passages
from a high-quality source

Then use a language model to generate an answer
from those passages
                 Copyright
                                                                 Authors Sue OpenAI Claiming Mass Copyright
                                                                Infringement ofHundreds of Thousands of Novels
 The Times Sues OpenAI and Microsoft
Over A.I Use of Copyrighted Work
Millions of articles from The New York Times were used to train
chatbots that now compete with it; the lawsuit said.
Reexamining "Fair Use" in the Age of
Al
Generative Al claimsproduce new language and images, but when thosc deas are based on copyrighted
material, who gets the credit? new paper from Stanford University looks for answers
     2023 4ndrev Myers






                                                                                                    etkdues
 Privacy
  How Strangers Got My Email
Address From ChatGPTs Model
   Toxicity and Abuse
The New AI-Powered Bing Is Threatening Users:
              Cleaning Up ChatGPT Takes Heavy Toll onHuman Workers
          Contractors in Kenya say they were traumatized by effort to screen out descriptions ofviolence and sexual abuse during run-up to OpenAIs hit chatbot
Misinformation
 Chatbots are generating false and
 misleading information about U.S:
 elections
Vast growth in interest in Ethics of LLMs and AINumber %f Accepted FAccT Conference Submissions by Affiliation, 2018-22
        Source: FAccT, 2022Chart: 2023 Al Index Report
            800              Education                                                                                         772
                             Industry
            700              Government
                             Nonprofit
                             Other
            600
        1 500                                                                                                                  503
            400
            300                                                                                                         302
                                                                                          244
            200                                             166                                                         227     181
                                                                                          200
            100                                             439
                              65                                                                                         53     70
                             2018                          2019                          2020                           2021  2022
                                                                                                                                 HAI: AI Index 2023
          Harms of Large Language
Large     Models
Language
Models
          Our last class together!
Large
Language
Models
Learning goals
  Write regular expressions for text tasks
  Apply the edit distance algorithm
  Build a supervised classifier
  Build a search engine
  Work with neural word embeddings
  Train a neural network
  Build a recommendation engine
  Build a chatbot
  Prompt a large language model
      What's next? Spring 2024 NLP adjacent courses
CS224N: Natural Language Processing with Deep Learning (Chris Manning)
Algorithmic internals: transformers, GPT, parsing, machine translation and other
applications. More of the gory details! More math, more machine learning
CS224C: NLP for Computational Social Science (Diyi Yang)
…machine learning and theories from social science to study human behaviors and
important societal questions at scale. NLP, social networks, causal inference, application
to social topics like hate speech, misinformation, and social movements.
CS 224S: Spoken Language Processing: (Andrew Maas)
Introduction to spoken language technology with an emphasis on dialogue and
conversational systems.
CS 336: Language Modeling from Scratch (Tatsu Hashimoto and Percy Liang)65
every aspect of language model creation, including data collection and cleansing for pre-
training, transformer model construction, model training, and evaluation before
deployment. Application required.
CS 246: Mining Massive Data Sets (Jure Leskovec)
   Next year courses!
CS 224V: Conversational Virtual Assistants with Deep Learning (Monica Lam)
Topics include: (1) growing LLMs' knowledge, (2) stopping LLMs from hallucination
(3) experimentation and evaluation of conversational assistants based on LLMs, (5)
controlling LLMs to achieve tasks, (6) persuasive LLMs, (7) multilingual assistants, and
(8) combining voice and graphical interfaces.
CS329X Human Centered NLP) (Diyi Yang)
human-centered design thinking in NLP, human-in-the-loop algorithms, fairness, and
accessibility.
CS329R Race and NLP (Dan Jurafsky and Jennifer Eberhardt)
                                                                                          66
Integrate methods from natural language processing with social psychological
perspectives on race to build practical systems that address significant societal issues
Fun courses outside of CS
 Spring:
 Linguist 173:     Invented Languages
 Linguist 134A: The Structure of Discourse
 Linguist 156: Language, Gender, and Sexuality
 COMM 154: The Politics of Algorithms

 Next year:                                                67
 Linguistics 150: Language and Society
 Linguistics 130a: Introduction to semantics & pragmatics
         Parameter Efficient Fine-Tuning (PEFT)

                                        Malay Agarwal

Contents
Introduction                                                                           1

PEFT Methods in General                                                                2
     Selective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   2
     Reparameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      3
     Additive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    3

Low Rank Adaptation (LoRA)                                                             3
     Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    3
     Practical Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       4
     Multiple Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      4
     Base Model vs Full Fine-Tuning vs LoRA . . . . . . . . . . . . . . . .            5
     Choosing The Rank r . . . . . . . . . . . . . . . . . . . . . . . . . . .         5

Soft Prompts                                                                           6
     Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    6
     Prompt Tuning vs Full Fine-tuning . . . . . . . . . . . . . . . . . . . .         7
     Multiple Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      8
     Interpretability of Soft Prompts . . . . . . . . . . . . . . . . . . . . . .      9

Useful Resources                                                                      10

Introduction
Full-fine tuning of large language LLMs is challenging. Fine-tuning requires
storing training weights, optimizer states, gradients, forward activations and
temporary memory. Things to store other than the weights can take up to 12-20
times more memory than the weights themselves.
In full fine-tuning, every weight of the model is updated during training.
PEFT methods only update a subset of the weights. They involve freezing most
of the layers in the model and allowing only a small number of layers to be

                                                    1
trained. Other methods don’t change the weights at all and instead, add new
layers to the model and train only those layers.
Due to this, the number of trainable weights is much smaller than the number
of weights in the original LLM. This reduces the overall memory requirement for
training, so much so that PEFT can often be performed on a single GPU.
Since most of the LLM is left unchanged, PEFT is also less prone to Catastrophic
Forgetting.                                                  PEFT weights
                               QA PEFT                                MBs
 GBs
         LLM                   Summarize                              MBs          QA
                               PEFT                                               LLM
                               Generate                               MBs
                               PEFT
PEFT weights are trained separately for each task. They are combined with the
original weights of the LLM for inference. This makes them easily swappable,
allowing efficient adaptation of the model to different tasks.
PEFT involves multiple trade-offs:
   • Parameter Efficiency
   • Training Speed
   • Inference Costs
   • Model Performance
   • Memory Efficiency

PEFT Methods in General
Selective
We select a subset of initial LLM parameters to fine-tune.
There are several approaches to select which subset of parameters we want to
fine-tune. We can decide to train:
   • Only certain components of the model.
   • Specific layers of the model.
   • Individual parameter types
The performance of these approaches and the selective method overall is mixed.

                                                 2
There are significant trade-offs in parameter efficiency and compute efficiency
and hence, these methods are not very popular.

Reparameterization
The model weights are reparameterized using a low-rank representation.
Example techniques are Low Rank Adaptation (LoRA).

Additive
New, trainable layers or parameters are added to the model.
There are generally two methods:
   • Adapters - New trainable layers are added to the model, typically inside
       the encoder or decoder blocks, after the FFNN or the attention layers.
   • Prompt Tuning - The model architecture is kept fixed and instead, the
       input (prompt) is manipulated to obtain better performance. This can
       be done by adding trainable parameters to the prompt embeddings, or
       keeping the input fixed and retraining the embedding weights. Example
       techniques include Soft Prompts.

Low Rank Adaptation (LoRA)
Introduction
LoRA is a PEFT technique based on reparameterization.
The encoder and decoder blocks of a Transformer consist of self-attention (in
the form of Multi-Headed Attention) layers. Weights are applied to the input
embedding vectors to obtain an attention map for the input prompt.
In full fine-tuning, every weight in these layers is updated. In LoRA:
   • All the model parameters are frozen.
   • Two (smaller) rank decomposition matrices A and B are injected with
       the original weights. The dimensions of the matrices are such that their
       product has the same dimension as that of the original weight matrices.
   • The weights in the smaller matrices are trained via fine-tuning.
For inference:
   • We multiply the two low rank matrices to obtain B × A, which has the
       same dimensions as the frozen weights of the model.
   • We add B × A to the original frozen weights.
   • The model weights are replaced with these new weights.

                                                3
We now have a fine-tuned model which can carry out the task(s) we have fine-
tuned it for. Since the model has the same number of parameters as original,
there is little to no impact on inference latency.
Researchers have found that applying LoRA just to the self-attention layers is
often enough to fine-tune for a task and achieve performance gains. However, in
principle, we can use LoRA in other components such as the feed-forward layers.
Since most of the parameters are the model are in the attention layers, we get
the biggest savings when we apply LoRA in those layers.

Practical Example
Consider the Transformer model presented in the Attention Is All You Need
paper. According to the paper, the model has dimensions d × dK = 512 × 64 in
the attention layer. There are thus 32, 768 trainable parameters in the model.
If we use LoRA with rank r = 8:
    • A has dimensions r × dK = 8 × 64, giving 512 parameters.
    • B has dimensions d × r = 512 × 8, giving 4096 trainable parameters.

                      Change = 32768 − (512 + 4096)∗ 100 ≈ 86%32768
Thus, we have an 86% decrease in the number of parameters we need to train.
Due to this drastic reduction in the amount of compute required, LoRA can
often be performed on a single GPU.

Multiple Tasks
LoRA also makes it easy to fine-tune a model for different tasks. We can train
the model using the rank decomposition matrices for each of the tasks. This will
give us a pair of A and B matrices for each task.
During inference, we can swap out the matrices depending on the task we want
the model to do and update the weights (by adding to the frozen weights).

                                                  4
     Task A                                  *
     Task B
Base Model vs Full Fine-Tuning vs LoRA
                      Base model         +80.63%            Full fine-tune       3.20%      LoRA fine tune
                       ROUGE                                   ROUGE                             ROUGE
                                                  flan_t5; base_instruct_full
     FLAN-TS                                        rouge]      0. 4216                flan      base instruct lora
                                                    rouge2      0 . 1804                 rouge]         4081
                                                    rougel         3384                  rouge2         1633
                                                    rougeL sum      0 . 3384 }           rougeLsum"
                                                                                                   rougel3251
       Dialog                                                                                              3249}
   summarization
                 flan_t5 base 2334rouge]
                    rouge2        0760
  Baseline          rougel      0. 2014
                    rougeLsum       0.2015}
  score
We can see that the LoRA model almost matches the fully fine-tuned model in
performance and both outperform the base model (no fine-tuning).
In other words, LoRA can achieve performance which is close to full fine-tuning
while significantly reducing the number of parameters that need to be trained.

Choosing The Rank r
In general:
         The smaller the rank r, the smaller the number of trainable
         parameters and the bigger the savings on compute.

                                                              5
According to the LoRA paper:
    • Effectiveness of higher rank appears to plateau. That is, after a certain
       rank value, making it larger generally has no effect on performance.
    • 4 ≤ r ≤ 32 (in powers of 2) can provide a good trade-off between reducing
       trainable parameters and preserving performance.
    • Relationship between rank and dataset size needs more research.
    Rank        val_loss     BLEU        NIST        METEOR         ROUGEL        CIDEr
                 1.23        68.72       8.7215     0.4565          0.7052        2.4329
        4        1.21        69.17       8.7413     0.4590          0.7052        2.4639
                 1.18        70.38       8.8439     0.4689          0.7186        2.5349
                 1.17        69.57       8.7457     0.4636          0.7196        25196
      2561286416E            69.61
                             68.73
                             68.92
                                  69.33
                             69.24       8.7483
                                         8.6718
                                         8.6982
                                               8.7736
                                         8.7174     0.4651
                                                          0.4629
                                                    0.4642
                                                    0.4628
                                                    0.4629          0.7177
                                                                    0.7105
                                                                    0.7127
                                                                    0.7128
                                                                          0.7180  2.4985
                                                                                  2.5255
                                                                                  2.5030
                                                                                  2.5012
                                                                                        2.5070
      512                    68.78       8.6857     0.4637          0.7128        2.5025
      1024       1.17        69.37       8.7495     0.4659          0.7149        2.5090
Soft Prompts
Introduction
Prompt tuning is not prompt engineering.
Prompt engineering involves modifying the language of the prompt in order
to “urge” the model to generate the completion that we want. This could be as
simple as trying different words, phrases or including examples for In-Context
Learning (ICL). The goal is to help the model understand the nature of the task
and to generate better completions.
This involves some limitations:
    • We require a lot of manual effort to write and try different prompts.
    • We are also limited by the length of the context window.
Prompt tuning adds trainable “soft prompts” to inputs that are learnt during
the supervised fine-tuning process.
The set of trainable tokens is called a soft prompt. It is prepended to the
embedding vectors that represent the input prompt. The soft prompt vectors
have the same length as the embeddings. Generally, 20-100 “virtual tokens” can
be sufficient for good performance.

                                                6
                         Soft prompt
  Same length as
  token vectors
        Typically 20-100             The teacher teaches the student with the book
         tokens
The tokens that represent natural language correspond to a fixed location in the
embedding vector space. On the other hand, soft prompts are not fixed discrete
words of natural language and can take on any values within the continuous
multidimensional embedding space.

Prompt Tuning vs Full Fine-tuning
Prompt tuning does not involve updating the model. Instead, the model is
completely frozen and only the soft prompt embedding vectors are updated to
optimize the performance of the model on the original prompt.
This is very efficient since a very small number of parameters are being trained
(10, 000 to 100, 000).

                                                  7
          Weights of model frozen and
          soft prompt trained
            10K    10OK of parameters
            updated
In comparison, full fine-tuning involves training millions to billions of parameters.

Multiple Tasks
Like LoRA, soft prompts are easily swappable. Thus, we can train different
soft prompts for different tasks and swap them according to our needs during
inference.

                                                  8
                Task A
                Task B
Interpretability of Soft Prompts
Soft prompts are not easily interpretable. Since they can take on any value within
the continuous multidimensional embedding space, they do not correspond to
any known tokens in the vocabulary of the model.
However, analysis of the nearest neighbors of soft prompts shows that they
form tight semantic clusters. Words closest to the soft prompt tokens have
similar meanings. These words usually have some meaning related to the task,
suggesting that the prompts are learning word-like representations.

                                                  9
 completely          100%
     totally
   altogether
           entirely
Useful Resources
   • LoRA paper.
   • Microsoft repository on LoRA.
   • QLoRA: Efficient Fine-tuning of Quantized LLMs.
   • QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language
      Models.
   • Prompt Tuning paper.
   • PEFT Python package by HuggingFace.
   • Lab 2 - Code example where FLAN-T5 is fine-tuned.

                                            10
             Pre-training Large Language Models
                                       Malay Agarwal

Contents
Choosing a Model                                                                      1
    Model Hubs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      2

Training Large Language Models                                                        2
    Initial Training Process (Pre-training) . . . . . . . . . . . . . . . . . .       2
    Training Objectives for Transformer Variants . . . . . . . . . . . . . .          3
           Encoder-only Models (Autoencoding Models) . . . . . . . . . . .            3
           Decoder-only Models (Autoregressive Models) . . . . . . . . . . .          4
           Encoder-Decoder Models (Sequence-to-Sequence Models) . . . .               4

Computational Challenges in Training LLMs                                             5
    Quantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      5
           FP16 and BF16 . . . . . . . . . . . . . . . . . . . . . . . . . . . .      6
           INT8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   6

Scaling Choices                                                                       6

Compute Budget                                                                        7
    Petaflops/s-day . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     7
    Compute Budget, Dataset Size, Model Size vs Model Performance . .                 8
    Compute-Optimal Models . . . . . . . . . . . . . . . . . . . . . . . . .          8

Domain Adaptation                                                                     9
    Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    9
    Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      9
           Legal Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     9
           Medical Domain . . . . . . . . . . . . . . . . . . . . . . . . . . .      10
    Case Study - BloombergGPT - Domain Adaptation for Finance . . . .                11

Choosing a Model
After figuring out the scope of our application, the next step is to select the
model we will work with.

                                                  1
We have two options:
   1. Choose a pre-trained foundation model.
   2. Train our own model to create a custom LLM.
There are specific use cases where the second option might make more sense but
in general, we will develop our application using a pre-trained foundation model.

Model Hubs
There are many open-source and paid models available that we can use for our
application.
Many of the developers of these models have made available “hubs” where we
can browse and test the models.
One of the most useful features of these hubs is the inclusion of model cards.
Model cards describe important details of a model such as the best use case of
the model, how it was trained and its limitation.
Example: Model card for LLaMa: Model Card.

Training Large Language Models
Variants of the Transformer model are suited to different tasks.
The differences in these variants can be understood by taking a look at how
these variants are trained. This, in turn, can help us make an informed decision
regarding the model we want to use for our application by helping us better
navigate model hubs.

Initial Training Process (Pre-training)
The initial training process of an LLM is called as pre-training. LLMs work by
learning a deep statistical representation of language and this deep representation
is developed during pre-training.
At a high-level, during pre-training, the model is fed large amounts of unstruc-
tured textual data, ranging from gigabytes to petabytes in size. The data is
pulled from many sources such as web crawling and corpora of text compiled
specifically to train LLMs.
The pre-training process is self-supervised. The model internalizes the patterns
and structures present in the language. These patterns then unable the model to
complete its training objective, which depends on the architecture of the model.
In other words, during pre-training, the model weights get updated to minimize
the loss of training objective.
Clearly, this step requires a lot of compute and the use of GPUs.

                                                  2
Additionally, since the data is coming from public sources such as the internet,
there is often a data quality filter applied before feeding the data to the LLM
so that the training data is of high quality, has low bias and does not have
harmful content. Due to this, only about 1-3% of the original tokens are used
for pre-training.

Training Objectives for Transformer Variants
The three configurations of a Transformer are trained with different training
objectives and thus, learn to perform different tasks.

Encoder-only Models (Autoencoding Models)
The encoder-only variants of Transformers are also called autoencoding mod-
els.
They are pre-trained using Masked Language Modeling (MLM). In MLM,
tokens in the input sequence are randomly masked and the training objective is
to predict the masked tokens in order to reconstruct the original input sequence.
This is also called a denoising objective since the masking of the tokens can be
thought of as adding noise to the input sequence and then predicting the masked
tokens can be thought of as removing that noise from the input sequence.
Autoencoding models build bidirectional context representations of the input
sequence, meaning that model has an understanding of the full context of the
token rather than just the tokens that come before it.
                                       Masked Language Modeling (MLM)
                                  The   ceacher    <HASR>    the    student
            Original text
           The teacher
           Eeaches the
           student                              Encoder-only
                                                    LLM
                                     Objective: Reconstruct text ("denoising"
                                  The   ceacher    teaches   the    student     Bidirectional context
These models are usually suited to tasks that benefit from this bidirectional con-
text such as sentiment analysis, named entity recognition and word classification,
etc.
       Examples: BERT, ROBERTA.

                                                   3
Decoder-only Models (Autoregressive Models)
The decoder-only variants of Transformers are also called autoregressive mod-
els.
They are pre-trained using Causal Language Modeling (CLM). In CLM, the
training objective is to predict the next token based on the previous sequence of
tokens. The tokens of the input sequence are masked and the model can only see
the input tokens leading up to the token being predicted at the moment. The
model has no knowledge of the tokens that come after this token. The model
then iterates over the input sequence one-by-one to predict the next token. Thus,
in contrast to autoencoding models, the model builds a unidirectional context
for each token.
                                        Causal Language Modeling (CLM)
                                           The   teacher
            Original text
           The teacher
            ceaches the
            scudent                             Decoder-onlyLLM
                                          Objective: Predict next token
                                           The   Leacher   Leaches
                                                                       Unidirectional context
By learning to predict the next token from a vast number of examples, the model
builds a statistical representation of the language. Predicting the next token is
sometimes called full language modeling by researchers.
These mode,ls are most suitable for text generation but large autoregressive
models also show strong zero-shot inference ability and can perform a variety of
tasks.
        Examples: GPT, BLOOM.

Encoder-Decoder Models (Sequence-to-Sequence Models)
The encoder-decoder variants of Transformers are also called sequence-to-
sequence models.
The exact details of pre-training objective vary from model to model. For example,
FLAN-T5 is trained using span corruption. In span corruption, a part of the
input sequence is masked and replaced by a sentinel token. These sentinel tokens
are special tokens added to the vocabulary that to do not correspond to any
actual word from the dataset. The decoder then has to reconstruct the sentence
autoregressively. The output is the sentinel token followed by the predicted
tokens.

                                                   4
                                               Span Corruption
                                  The   Eeacher    <HASK>    <MASK>    seudent
            Original text         The   ceacher                        scudent
           The teacher
            ceaches the                                            Sentinel token
            scudent                           Encoder-Decoder
                                                   LLM
                                          Objective: Reconstruct span
                                                 Eeaches     che
We can use such models for tasks such as translation, summarization and question
answering. They are most useful where the input and output both are bodies of
text.
        Examples: FLAN-T5, BART.

Computational Challenges in Training LLMs
LLMs have a lot of parameters and thus, require vast compute resources for
training.
One common issue is memory. Many models are too big to be loaded into a
single GPU’s memory. We can encounter an OutOfMemoryError when we try to
do so.
Consider an LLM with 1 billion parameters. All parameters are stored as floats
and using single-precision, each parameter occupies 32 bits or 4 bytes. 1 billion
parameters would require 4 × 109 bytes or ~4 GB. Thus, we need a GPU with
at least 4 GB of VRAM to even load the model.
On top of that, to train the model at 32-bit full precision, we need an additional
~80 GB of VRAM. 80 GB is the VRAM capacity of a single NVIDIA A100 GPU.
There is a popular technique to reduce memory usage.

Quantization
We reduce the amount of memory required to store and train the model by
reducing the precision from 32-bit floating point (FP32) numbers to 16-bit
floating point numbers (FP16) or 8-bit integers (INT8) for the model.
The range of values that can be represented by FP32 is from ~−3 × 10−38 to
~3 × 1038, and FP32 is the default representation for the model.
Quantization statistically projects the 32-bit numbers into a lower precision
space, using scaling factors calculated based on the range of the original 32-bit

                                                  5
floating point numbers.
        Example: Consider that we want to store π to 6 decimal places, that
        is we want to store π = 3.141592.

FP16 and BF16
This is stored as 0 10000000 10010010000111111011000 in 32-bit representation.
The first bit is the sign bit, the next 8 bits represent the exponent and the final
23 bits represent the fraction/mantissa/significand.
The last 23 bits decide the precision of the representation. If we convert this
to decimal and compare it to the real value of π, we’ll see that the converted
number has lost precision. But the number is accurate in the 6 decimal places
we require.
The same number is projected as 0 10000 1001001000 in 16-bit representation.
While there is a sign bit like 32-bit, there are only 5 bits for the exponent and
only 10 bits for the fraction. This makes the range of values much smaller
(~−65504 to ~65504). When we convert this, we lose even more precision, with
the result being 3.140625.
Another popular alternative in the AI field is BFLOAT16 or BF16, developed
by Google Brain. This has 1 sign bit, 8 exponent bits and 7 fraction bits. It’s a
hybrid between FP16 and FP32. It helps with training stability and is supported
by NVIDIA GPUs like the A100.
BF16 is also called the truncated FP32 since it captures the dynamic range of
FP32 but still uses 16 bits. It uses the full 8 bits for the exponent like FP32 but
truncates the fraction part to 7 bits. This saves memory and also increases model
performance by speeding up calculations. The disadvantage is that BF16 is not
well suited for integer calculations, which are anyways rare in deep learning.

INT8
If we project the 32-bit π representation to INT8, it will be stored as 0 0000011.
If we use one bit for the sign, there are 8 bits for the fraction. The range of
values is −128 to 127. π is simply projected to 3. The memory requirement is
halved even further, for a total reduction by a factor of 4 1.
But nowadays, its common for models to have 50 billion or over 100 billion
parameters, requiring thousands of gigabytes of GPU memory. In such cases,
we need to split the model across multiple GPUs for training (see Efficient
Multi-GPU Compute Strategies).

Scaling Choices
The goal during pre-training an LLM is to maximize the model’s performance
on its learning objective.

                                                   6
This is equivalent to minimizing the loss function.
There are two choices to achieve better performance:
   • Increasing the dataset size in terms of number of tokens.
   • Increasing the model size in terms of number of parameters.
These are the scaling choices available to us.

Compute Budget
In theory, we can scale either or both the dataset size and the model size, but we
are constrained by compute budget in terms of GPUs, training time, cost, etc.

Petaflops/s-day
There are some popular units used to measure compute budget.
One of them is the petaflops/s-day (PF-days). It is the number of floating
point operations performed at a rate of 1 petaflop (1 quadrillion floating point
operations) per second for one day.
With respect to training transformers, 1 petaflop/s-day is equivalent to about 8
NVIDIA V100 GPUs or 2 NVIDIA A100 GPUs operating at full efficiency for
24 hours.
The graph below shows the petaflops/s-day measurements for some popular
models. BERT/ROBERTA are encoder-only models, T5 is an encoder-decoder
model and GPT3 is a decoder-only model. The y-axis is logarithmic. The x-axis
varies in terms of the number of parameters trained.
 J 100001000100 ROBERTABERTI               TS                           GPT-3
 p         10
           BaseLarge-Base~LargeSmallBaseLarge3B      9    1    1
                                                               Large+        8 b:   1758
                                                7
Compute Budget, Dataset Size, Model Size vs Model Per-
formance
Researchers have explored the relationships between dataset size, model size and
compute budget.
In the paper Scaling Laws for Neural Language Models (OpenAI, 2020), we find
the following figure:
                                                      (0/5.4 10" 4=0C                 (N/8. 8 10"7,20 076
              (Cmin2,3 . 1081 Col05C
     To-, 10-7  10-3  10-}  10-1  10"
                 Compute                        Dataset Size                    Parameters
           PF-days, non-embedding                  iokens                      non-embedding
The graph shows a clear relationship between model performance and each of
the three factors, which can be approximated by a power-law relationship.
That is, one is proportional to the other raised to some power. When plotted on
a graph where both axes are logarithmic, such relationships appear as a straight
line. The relationship only holds when the training is not bottlenecked by the
other two factors.
More often than not, compute budget is a hard constraint, determined by:
    • Hardware Availability
    • Project Timeline
    • Financial Budget
Thus, most of the time, we end up increasing the model size or the dataset size
to increase performance.

Compute-Optimal Models
In the paper Training Compute-Optimal Large Language Models (DeepMind,
2022, popularly referred to as the Chinchilla paper), researchers tried to find the
optimal number of parameters and volume of training data for a given compute
budget. Such models are called compute-optimal models.
The paper found the following:
    • Very large models maybe be over-parameterized and under-trained. They
       have more parameters than they need to achieve a good understanding of
       language and they would benefit from seeing more training data.
    • Smaller models trained on more data could perform as well as large models.
    • Compute-optimal training dataset size is ~20 times the number of parame-
       ters.

                                                  8
The paper presented the Chinchilla model as a proof of their findings. The
Chinchilla model uses the same compute budget as another model called Gopher,
but has 4 times fewer parameters (70 billion vs 280 billion) and uses 4 times
more training data. It consistently outperforms Gopher while being significantly
smaller.
Due to the findings of the paper, research teams have started developing smaller
models that achieve similar if not better performance than larger models trained
in a non-optimal way.

Domain Adaptation
Introduction
Using a pre-trained LLM can help us save time and get to a working solution
much faster.
However, there is a situation where it may be necessary to pre-train our own
model. If the domain of the problem we are trying to solve uses vocabulary and
language structures that are not commonly used in day-to-day language, we
might need to train our own model.
This is called domain adaptation.

Examples
Legal Domain
Legal language is often very different from day-to-day language and usually
requires domain adaptation. For example, the usage of latin terms. Moreover, it
also uses everyday words in different contexts.

                                                  9
  The prosecutor            had difficulty
 proving        mens     rea      as   the    defendant
  seemed      unaware       that his actions
  were    illegal
  The    judge     dismissed         the    case
 citing       the    principle         of   res
  judicata        as   the    issue     had already
 been      decided       in   a previous          trial
 Despite        the    signed agreement ,              the
  contract        was    invalid as         there      was
  no   consideration            exchanged between
  the parties
Medical Domain
The medical domain also uses uncommon words to describe diseases. It also
involves the usage of language in an idiosyncratic way, as shown in the image
below. The line Sig: 1 tab po qid pc & hs does not make much sense to us but is
actually a shorthand used by doctors to write prescriptions. It makes a lot of
sense to a pharmacist.

                                10
 After       a   strenuous           workout ,         the
 patient experienced                     severe        myalqia
  that     lasted        for     several         days _
 After       the biopsy,             the     doctor
 confirmed           that      the     tumor       was
 maliqnant           and     recommended             immediate
  treatment
  Sig:      1  tab     po    qid pc         6  hs
Case Study - BloombergGPT - Domain Adaptation for
Finance
BloombergGPT is an LLM developed by Bloomberg for the finance domain.
It was trained on data consisting of both finance data (~51%) and general-
purpose text data (~49%). The model achieves best-in-class performance in
finance-related tasks while also maintaining competitive performance in general
language tasks.
The authors used the Chinchilla pre-scaling laws to guide their training.
See paper for more details.

                                   11
             Prompting and Prompt Engineering
                                      Malay Agarwal

Contents
Prompting                                                                           1

Prompt Engineering                                                                  1
    Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1
    In-Context Learning (ICL) . . . . . . . . . . . . . . . . . . . . . . . .       1
          Zero-Shot Inference . . . . . . . . . . . . . . . . . . . . . . . . . .   2
          Few-Shot Inference . . . . . . . . . . . . . . . . . . . . . . . . . .    2

Inference Configuration Parameters                                                  2
    Max New Tokens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      3
    Greedy vs Random Sampling . . . . . . . . . . . . . . . . . . . . . . .         3
          Sample Top-K and Sample Top-P . . . . . . . . . . . . . . . . . .         3
          Temperature . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     3

Prompting
The text that is fed to LLMs as input is called the prompt and the act of
providing the input is called prompting.

Prompt Engineering
Definition
The process of tweaking the prompt provided to an LLM so that it gives the
best possible result is called prompt engineering. Some common techniques are
given below.

In-Context Learning (ICL)
In ICL, we add examples of the task we are doing in the prompt. This adds
more context for the model in the prompt, allowing the model to “learn” more
about the task detailed in the prompt.

                                                1
Zero-Shot Inference
For example, we might be doing semantic classification using our LLM. In that
case, a prompt could be:
        Classify this review: I loved this movie!
        Sentiment:
This prompt works well with large LLMs but smaller LLMs might fail to follow
the instruction due to their size and fewer number of features. This is also called
zero-shot inference since our prompt has zero examples regarding what the
model is expected to output.

Few-Shot Inference
This is where ICL comes into play. By adding examples to the prompt, even a
smaller LLM might be able to follow the instruction and figure out the correct
output. An example of such a prompt is shown below. This is also called
one-shot inference since we are providing a single example in the prompt:
        Classify this review: I loved this movie!
        Sentiment: Positive
        Classify this review: I don’t like this chair.
        Sentiment:
Here, we first provide an example to the model and then ask it to figure out the
output for the I don’t like this chair review.
Sometimes, a single example won’t be enough for the model, for example when
the model is even smaller. We’d then add multiple examples in the prompt. This
is called few-shot inference.
In other words:
    • Larger models are good at zero-shot inference.
    • For smaller models, we might need to add examples to the prompt, for
        few-shot inference.

Inference Configuration Parameters
The size of the model we use for our tasks depends on the the actual tasks we
want to solve and the amount of compute resources available with us.
Once we have selected a model, there are some configurations that we can play
with to see if the model’s performance improves. Some of them are detailed
below.

                                                  2
Max New Tokens
This is used to limit the maximum number of new tokens that should be generated
by the model in its output. The model might output fewer tokens (for example,
it predicts <EOS> before reaching the limit) but not more than this number.

Greedy vs Random Sampling
Some models also give the user control over whether the model should use greedy
or random sampling.

Sample Top-K and Sample Top-P
Sample Top-K and Sample Top-P are used to limit the random sampling of a
model.
A top-K value instructs the model to only consider K words with the highest
probabilities in its random sampling. Consider the following softmax output:

                                     Probability       Word
                                     0.20              cake
                                     0.10              donut
                                     0.02              banana
                                     0.01              apple
                                     . . .             . . .

If K = 3, the model will select one of cake, donut or banana. This allows the
model to have variability while preventing the selection of some highly improbable
words in its output.
The top-P value instructs the model to only consider words with the highest
probabilities such that their cumulative probability, p1 + p2 + · · · + pK ≤ P . For
example, considering the above output, if we set P = 0.30, the model will only
consider the words cake and donut since 0.20 + 0.10 ≤ 0.30.

Temperature
Temperature is also another parameter used to control random sampling. It
determines the shape of the probability distribution that the model calculates
for the next word.
Intuitively, a higher temperature increases the randomness of the model while a
lower temperature decreases the randomness of the model. This temperature is
passed as a scaling factor to the final softmax layer of the decoder.
If we pick a cooler temperature (T < 1), the probability distribution is strongly
peaked. In other words, one (or a few more) words have very high probabilities
while the rest of the words have very low probabilities:

                                                  3
                                     Probability       Word
                                     0.001             apple
                                     0.002             banana
                                     0.400             cake
                                     0.012             donut
                                     . . .             . . .

Notice how cake has a 40% chance of being picked while other words have very
small chances of being picked. The resulting text will be less random.
On the other hand, if we pick a warmer temperature (T > 1), the probability
distribution is broader, flatter and more evenly spread over the tokens:

                                     Probability       Word
                                     0.040             apple
                                     0.080             banana
                                     0.150             cake
                                     0.120             donut
                                     . . .             . . .

Notice how none of the words have a clear advantage over the other words.
The model generates text with a higher amount of randomness and has more
variability in its output.
Clearly, when T = 1, the model uses the softmax output as is for random
sampling.

                                                  4
                                       Transformers
                                         Malay Agarwal

Contents
Before Transformers                                                                              2

Why Transformers?                                                                                2

Some Features of Transformers                                                                    2

Key Concepts                                                                                     2
     Self-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .              2
            Intuition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .            2
            Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                3
     Multi-Headed Attention . . . . . . . . . . . . . . . . . . . . . . . . . .                  6
            Intuition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .            6
            Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                6
            Code (PyTorch) . . . . . . . . . . . . . . . . . . . . . . . . . . . .               7
     Positional Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . .                 9
            Why? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .               9
            Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                9

Full Encoder-Decoder Architecture                                                              10
     Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .              11
            Residual Connections . . . . . . . . . . . . . . . . . . . . . . . . .              11
            Layer Normalization . . . . . . . . . . . . . . . . . . . . . . . . .               11
            Feed-Forward Neural Network (FFNN) . . . . . . . . . . . . . . .                    11
     Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .              12
            Masked Multi-Headed Attention . . . . . . . . . . . . . . . . . .                   12
            Cross-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . .             13
            Linear + Softmax - Prediction               . . . . . . . . . . . . . . . . . . .   14

Types of Configurations of Transformers                                                        15
     Encoder-only Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .                15
     Encoder-Decoder Models               . . . . . . . . . . . . . . . . . . . . . . . . .     15
     Decoder-only Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .                15

Useful Resources                                                                               15

                                                     1
Before Transformers
Text generation is not a new paradigm. Before Transformers, text generation
was carried out by Recurrent Neural Networks (RNNs).
RNNs were capable at their time, but were limited by the amount of compute
and memory needed to perform generative tasks.
For example, consider an RNN trained for next-token generation. By default, it
can only look at the previous word and as such the model is not very good. As
we scale the model to look at a greater number of previous words, we also need
to significantly scale the resources required to train the model.

Why Transformers?
To predict the next token, models need to see more than just the previous word.
Models need to have an understanding of the whole input prompt. Language is
complex and full of ambiguity. For example, consider the sentence:
       I took my money to the bank.
The word bank is a homonym which has multiple meanings. It is only with
the context of money that we understand that this bank refers to a financial
institution and not the bank of a river. Consider another example:
       The teacher taught the student with the book.
Does the book used belong to only the teacher, only the student or both of them
have a copy of the book with them?
These kind of problems are solved (to an extent, of course) by Transformers.

Some Features of Transformers
   • Can scale efficiently to use multi-core GPUs.
   • Can parallel-process input data, allowing the use of massive datasets
       efficiently.
   • Pay “attention” to the input meaning, allowing for better models which
       can generate more meaningful and relevant text.

Key Concepts
Self-Attention
Intuition
The power of the Transformer architecture lies in its ability to learn the relevance
and context of all of the words in the prompt with each other. The context is

                                                 2
learnt not just with their immediate neighbour, but with every other word.
   The teacher taught the student with the book:
For example, in the above image, the model learns how the word teacher is
associated with every other word - The, taught, the, student, etc.
The model applies “attention weights” to the relationships so that it learns
the relevance of each word to every other word. These “attention weights” are
learned during training.
This is called self-attention. The term originates from the fact that each
word in the prompt attends to other words in the same prompt, including itself.
This mechanism is what enables Transformers to capture relationships and
dependencies between words regardless of their distance from each other in the
prompt.

Computation
This self-attention is computed as follows for each word t in the prompt:

                    A(q<t>, K, V ) = ∑∑j exp(q<t>.K<j>) V<i>exp(q<t>.K<i>)
                                           i

This is essentially a softmax over the quantity q<t>.K<i>.
For each word t, we have three values:
    • q<t> - Key<t> - Query
    • k<t> - Value
    • v
Corresponding to the queries, keys and values, we have three weight matrices
that are learnt during training:
    • q<t> = WK .x<t>
    • k<t> = WV .x<t><t> = WQ.x<t>
    • v

                                                3
The terms query, key and value derived from DBMS. Intuitively, q<t> allows us
to ask a question for the word t and the product q<t>.k<j> tells us how good of
an answer is word j for the question.
Consider the sentence (in the context of machine translation):
        Jane visite l’Afrique en Septembre
The computation graph for the word l’Afrique is as shown:
                                                A(q    K,V)
       Sollmax             Sollmax              Sollmax             Sollmax       Sollmax
        Jane                visite             TAfrique                          Seplembre
       1*D                                      1*1>                 1*4>          1*6>
The steps are as follows:
    • The dot product of q<3> with each word’s k<t> is computed.

                                                     4
        • A softmax is taken over this dot product.
        • Each word’s v
        • The result is summed element-wise and gives the final A<t> is multiplied with the softmax output.<3> value.
                 Note: This shows that a word does not have a fixed representation
                 and can actually adapt to how it is used in the sentence.
Overall:
        • We feed X ∈ RL×d to the network, where L is the context window length
                 and d is the dimensions of the embedding.
        • We project X into three matrices Q, K and V :– Q = (WQXT )T = XW Q ∈ RL×dK , where WQ is a matrix of dimen-T
                        – K = (WK Xsion dK × d.T )T = XW K ∈ RL×dK , where WK is a matrix of dimen-T
                        – V = (WV Xsion dK × d.T T = XW V ∈ RL×dV , where WV is a matrix of dimen-)  T
                                sion dV × d.
        • We compute the attention using the following vectorized equation:
                                                                 A(Q, K, V ) = softmax( QKT)V ∈ RL×dV√dK
                 Note: WQ and WK need to have the same dimension since we take
                 a dot product between Q and K.
√dK Note: The output dimension depends on the dimension of WV .is used to prevent the dot product from becoming too large. This is called
the scaled dot-product attention.
See the image below for a full picture of the dimensions:

                                                                                                                    5
            Embeddingsize
 of tokensnumber                                                          QKT       Sottmax
               Inbulg                                                     Altenton
                                                                           matio
                                  WI
                                  Wulght
Multi-Headed Attention
Intuition
Multi-Headed Attention is essentially a for-loop over self-attention. Intuitively,
we have multiple questions we’d like to find the best answer for.
While the implementation differs for efficiency reasons, there are essentially h
number of WQ, WK and WV matrices, one for each question we’d like to answer.
h is called the number of heads.
Self-attention is computed with each of these matrices, to obtain an L × h × dV
matrix. The h and dV dimensions are concatenated to get an L × h.dV matrix.
This is finally multiplied with an dO × h.dV matrix WO to obtain the final output
of dimension L × dO .

Computation
The idea is to stack all the weight matrices required for computing the Q, K
and V matrices for each head into one single matrix. This ensures that we can
obtain the Q, K and V matrices using a single matrix multiplication instead of
multiple multiplications.
Consider that each Q, K and V matrix will have dQ = dK = dV = dh (say).
Suppose we have h number of heads. Thus, we need 3.h number of dh × d
matrices (3 for Q, K and V , and h for each head). In other words, we need a

                                                  6
3.h.dh × d matrix, where 3.h.dh represents the stacked matrix dimension. Let
this matrix be W .
We then multiply X ∈ RL×d with W as follows:

                        QKV = (W XT T = XW T ∈ RL×3.h.dh)
We then reshape this to obtain an L × h × 3.dh tensor. Finally, we can take
chunks of three from the last dimension to obtain 3 L × h × dh matrices, each
representing the Q, K and V matrices.
These three matrices are passed to the self-attention block to obtain an L×h×dh
output, which is concatenated along the last dimension to obtain the L × h.dh
output A. Finally, A is multiplied with an h.dh × h.dh (dO = h.dh) matrix WO
to obtain the final L × h.dh output as follows:

                         O = (WO AT ) = AW O ∈ RL×h.dh=dOT
In the actual implementation, we pass in three inputs:
   • d - Input embedding size.
   • h - Number of heads.
   • dO - Expected output dimension of multi-headed attention.
From this, dh is computed as dh = h since dO = h.dh. In other words, dOdO
should be such that dO mod h = 0. The rest is the same as above.

Code (PyTorch)
import math

import torch
import torch.nn as nn
import torch.nn.functional as F

def scaled_dot_product(q, k, v, mask=None):
      d_k = q.size()[-1]
      attn_logits = torch.matmul(q, k.transpose(-2, -1))
      attn_logits = attn_logits / math.sqrt(d_k)
      # Applying mask for masked multi-headed attention
      if mask is not None:
           attn_logits = attn_logits.masked_fill(mask == 0, -9e15)
      attention = F.softmax(attn_logits, dim=-1)
      values = torch.matmul(attention, v)
      return values, attention

                                              7
def expand_mask(mask):
      assert (
           mask.ndim > 2
      ), "Mask must be at least 2-dimensional with seq_length x seq_length"
      if mask.ndim == 3:
           mask = mask.unsqueeze(1)
      while mask.ndim < 4:
           mask = mask.unsqueeze(0)
      return mask

class MultiheadAttention(nn.Module):
      def __init__(self, d, dO, h):
           super().__init__()
           assert dO % h == 0, "Embedding dimension must be 0 modulo number of heads."

           self.dO = dO
           self.h = h
           # Compute dh
           self.dh = dO // h

           # Create the stacked weight matrix using a linear layer
           # It will receive a d-dim input
           # And produce a 3.h.dh = 3.dO dimensional output
           self.qkv_proj = nn.Linear(d, 3 * dO)

           # Create WO using a linear layer
           # It will receive an h.dh = dO-dim input and
           # Produce a dO-dim output
           self.o_proj = nn.Linear(dO, dO)

           self._reset_parameters()

      def _reset_parameters(self):
           # Original Transformer initialization, see PyTorch documentation
           nn.init.xavier_uniform_(self.qkv_proj.weight)
           self.qkv_proj.bias.data.fill_(0)
           nn.init.xavier_uniform_(self.o_proj.weight)
           self.o_proj.bias.data.fill_(0)

      def forward(self, x, mask=None, return_attention=False):
           batch_size, seq_length, _ = x.size()

           if mask is not None:
                mask = expand_mask(mask)

                                            8
               # [Batch, L, 3*h.dh] = [Batch, L, 3.dO]
               qkv = self.qkv_proj(x)

               # Reshape to [Batch, L, h, 3*dh]
               qkv = qkv.reshape(batch_size, seq_length, self.h, 3 * self.dh)
               # Permute as [Batch, h, L, 3*dh]
               qkv = qkv.permute(0, 2, 1, 3)
               # Take out [Batch, h, L, dh] chunks to obtain Q, K and V
               q, k, v = qkv.chunk(3, dim=-1)

               # Apply self-attention - [Batch, h, L, dh]
               values, attention = scaled_dot_product(q, k, v, mask=mask)
               # Permute to [Batch, L, h, dh]
               values = values.permute(0, 2, 1, 3)
               # Concatenate to [Batch, L, h.dh] = [Batch, L, dO]
               values = values.reshape(batch_size, seq_length, self.dO)
               # Multiply with WO for final output
               o = self.o_proj(values)

     return (o, attention) if return_attention else o

 Positional Encoding
 Why?
 Multi-headed attention has no information about the relative position of words
 in the input sequence. But, position can be extremely important in a sentence.
 Thus, transformers have a positional encoding step.

 Computation
 The position is encoded using the following function:
                                              {             t2k/d
                              PE(t, i) =         sin(10000t2k/d ), i = 2k
                                                 cos(10000         ), i = 2k + 1
 where t (1 ≤ t ≤ L) is the numerical position of the word being encoded and i
(0 ≤ i < d) is an index into the embedding for the word.
 Take the sentence:
         Jane visite l’Afrique en Septembre
 Consider the word Jane. Here, t = 1. Assuming 4-dimensional embedding
(0 ≤ i ≤ 3), the positional encoding of Jane would be:
                             [sin(1), cos(1), sin(√1000) ), cos(√10000 )]1       1
                                                           9
since PE(1, 0) = sin (1), PE(1, 1) = cos(1), PE(1, 2) = sin (√1000) ) and PE(1, 3) =1
cos( √10000 ) respectively.1
This creates a vector with alternating sine and cosine waves, which have different
frequencies. This vector is added to the original embedding for the word so that
the original embedding also has information about the word’s position.

Full Encoder-Decoder Architecture
                                                   Output
                                                 Probabilities
                                                   Softmax
                                                     Linear
                                                  Add & Norm
                                                     Feed
                                                    Forward
                                                  Add & Norm
                        Add & Norm                Multi-Head
                            Feed                   Attention
                          Forward                                        Nx
                                                  Add & Norm
                        Add & Norm                  Masked
                         Multi-Head               Multi-Head
                          Attention                 Attention
       Positional                                                  Positional
       Encoding                                                    Encoding
                        EmbeddingInput            EmbeddingOutput
                          Inputs                   Outputs
                                                (shifted right)
The Transformer is an encoder-decoder architecture. Both the encoder and

                                               10
decoder blocks are repeated N times. Typically, N = 6.
The working of the encoder and decoder is explained below (in the context of
machine translation).

Encoder
The parts (expect multi-headed attention) in the encoder are detailed below.

Residual Connections
The output matrix of the multi-headed attention is added to the original embed-
ded input using a residual connection.
       Note: This requires the output dimension of the multi-headed at-
       tention layer to match the original dimension of the input. In other
       words, dO = d so that the output is L × d.
This residual connection is important since:
   • It helps with the depth of the model by allowing information to be passed
       across greater depths.
   • Multi-headed attention does not have any information about the posi-
       tion of tokens in the input sequence. With the residual connection (and
       [[#Positional Encoding|positional encoding]]), it is possible to pass this
       information to the rest of the model instead of the information being lost
       after the first multi-headed attention pass. It gives the model a chance
       to distinguish which information came from which element of the input
       sequence.

Layer Normalization
A layer normalization is applied to the added output. This is preferred over
batch normalization since the batch size is often small and batch normalization
tends to perform poorly with text since words tend to have a high variance (due
to rare words being considered for a good distribution estimate).
Layer normalization:
   • Speeds up training.
   • Provides a small regularization.
   • Ensures features are in a similar magnitude among the elements in the
       input sequence.

Feed-Forward Neural Network (FFNN)
The normalized output is fed to an FFNN. It is applied to each element in the
input sequence separately and identically. The encoder uses a Linear → ReLU
→ Linear model. Usually, the inner dimension of the FFNN is 2-8 × larger than
d, the size of the input embedding.

                                                 11
The FFNN adds complexity to the model and can be thought of as an extra step
of “pre-processing” applied on the output of multi-headed attention.
There is also a residual connection between the output of the multi-headed
attention and the output of the FFNN, with layer normalization.

Decoder
The decoder is basically the same as the encoder but there are two things that
are important to note in the decoder.

Masked Multi-Headed Attention
The first multi-headed attention layer uses masking during training. This allows
parallel training.
During training, we have the entire expected output sequence. Thus, we do not
need to predict word by word. We can instead feed the entire output sequence
to the decoder.
To ensure that it still behaves as if its predicting one word at a time, a part of
the L × L matrix that is obtained after computing QKT√dK                is masked. In particular,
a word at index i should only attend to words from indices 1 to i. Thus, all
indices from i + 1 to L are set to −∞ so that they become 0 when the softmax
is applied. Consider the sentence:
        Je suis un étudiant
After computing QKT√dK        and applying softmax, the matrix might look like this:

                                        Je       suis     un       étudiant
                          Je            1        0        0        0
                          suis          0.02     0.98     0        0
                          un            0.05     0.20     0.75     0
                          étudiant      0.38     0.02     0.05     0.55

For suis, i = 2. Thus, the elements at indices (2, 3) and (2, 4) are 0 since suis
should attend only to Je and suis itself. On the other hand, the word étudiant
should attend to all the words since its the last word. Thus, no element in that
row is 0.
        Note: The output is shifted to the right during inference, where
        we do not have the entire output sequence and are actually predicting
        one word at a time. We start the decoder with the single token <SOS>
        as the input and then, as the decoder predicts the next word, we add
        this new word to the input. This is what’s referred to as “shifted
        right” in the diagram.

                                                  12
Cross-Attention
Cross-attention is a generalization of self-attention.
In self-attention, we are computing the attention matrix for a single sequence.
The same sequence is used to compute the Q, K and V matrices.
In cross-attention, we have two different sequences x1 and x2. x1 is used to
compute the Q matrix while x2 is used to compute the K and V matrices. When
x1 = x2, cross-attention reduces to self-attention.
In the decoder, cross-attention is used in the second multi-headed attention layer.
The Q matrix comes from the output of the first multi-headed attention layer,
which is turn uses the input to the decoder as its input. The K and V matrices
are computed from the output of the final encoder block.
In essence, the input to the decoder is acting as x1 and the output of the final
encoder block is acting as x2.
Cross-attention can work with sequences of different lengths. When computing
Q from x1, we get an L1 × dQ matrix. When computing K from x2, we get an
L2 × dQ matrix. When we take the dot product of Q and K, we get an L1 × L2
matrix. Since V is also computed from x2, its dimension is L2 × dV . Thus, the
overall result of cross-attention will be L1 × dV after softmax and multiplying
with V . See the image below (L1 = n and L2 = m):
            Embeddingsize
                                                                                     "new"
  number
 of tokens
               Inpuls
                                                                            QKT            Sottmax
  number                                                                     Aonhon
 of tokens                                    "new"
               Induls Z             w
    "new"                           Weights)
                                                                                               Ouiputs)
       Note: This technique of cross-attention is also used in diffusion

                                                  13
        models. See High-Resolution Image Synthesis with Latent Diffusion
        Models.

Linear + Softmax - Prediction
There is a final linear layer followed by a softmax activation. This converts the
output of the decoder to a probability distribution over all the words. In other
words, if the dictionary of words has N words, this layer has N units, for each
word.
The next word can be predicted from this probability distribution by either
taking the one with the maximum probability or using other techniques. These
other techniques can affect how creative the model is. For example, technique
might lead to the model not choosing the most “obvious” word every time and
going for slightly eccentric choices.

Greedy Sampling The technique of using the word with the maximum
probability is called greedy sampling. This is the most commonly used
technique for many models. Consider the following softmax output:

                                     Probability       Word
                                     0.20              cake
                                     0.10              donut
                                     0.02              banana
                                     0.01              apple
                                     . . .             . . .

The model would output the word cake since it has the highest probability.

Random Sampling Another approach is called random-weighted sam-
pling. It introduces some variability to the model’s output. Instead of taking
the word with the maximum probability, the probabilities are used as weights to
sample one word at random.
For example, consider the following softmax output:

                                     Probability       Word
                                     0.20              cake
                                     0.10              donut
                                     0.02              banana
                                     0.01              apple
                                     . . .             . . .

The word cake has a 20% chance of being selected while the word banana has a
2% chance of being selected. It might be that the model selects the word banana.

                                                 14
It is possible that this technique leads to the model becoming too creative, where
it generates words or wanders into topics that do not make sense with respect
to the prompt.

Types of Configurations of Transformers
Encoder-only Models
They only have encoders. Without some changes, these models always produce
an output which is of the same length as the input sequence.
It is possible to modify these so that they can be used for tasks such as semantic
classification.
       Example: BERT.

Encoder-Decoder Models
This is the model originally described in the Transformers paper and the one
detailed here. The output sequence and the input sequence can be of different
lengths.
It is useful for sequence-to-sequence tasks such as machine translation.
       Example: BART, FLAN-T5.

Decoder-only Models
These are some of the most commonly used models. As they have scaled, they
have gained the ability to generalize to pretty much any task.
       Example: GPT, BLOOM, LLaMA.

Useful Resources
   • Transformers paper - Attention Is All You Need.
   • Understanding and Coding the Self-Attention Mechanism of Large Lan-
       guage Models From Scratch.
   • Tutorial 6: Transformers and Multi-Head Attention.
   • Lecture on Transformers from the Course.
   • Lectures on Transformers from Deep Learning Specialization’s Sequence
       Model course on Coursera.
   • Layer Normalization.

                                              15
